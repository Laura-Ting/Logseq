- 特征提取：数据特征决定了特征的上限，预处理和特征提取才是核心，算法和参数只是以不同方式逼近这个上限
- 深度学习解决的问题是怎么样去提取特征
- 深度学习应用：无人驾驶，人脸识别，医学检测；最大的问题，移动端速度慢
- K近邻算法，KNN
  collapsed:: true
	- k=5和k=3时的结果是不同的
	- 流程：
	  collapsed:: true
		- 计算这个点和其他点的距离
		- 按照距离依次排序
		- 选取与当前点距离最小的k个点
		- 确定这k个点出现概率，取最高的作为这个点的类别
	- 分析：
	  collapsed:: true
		- 简单有效，是一种lazy-learning
		- 不需要训练集训练，训练时间复杂度为0
		- 计算复杂度和训练集数目成正比
		- k的选择，距离度量，分类决策->算法的基本要素
	- 不能用来图像分类：背景主导是问题但是我们关心的确实主导
- CIFAR-10：10类标签，50000个训练数据，10000个测试数据，32*32
- 距离的选择d1(I1, I2) = ∑|I1p - I2p|
- 神经网络基础：
  collapsed:: true
	- 线性函数：输入（图片）->输出的映射（每个类别的得分）
	  collapsed:: true
		- 每个像素点x，每个特征对应一个不同的权重参数W，f(x, W)，b是偏置参数
		- 比如说做一个十分类任务，f(x, W) = Wx + b，10 * 1 = 10 * 3072 * 3072 * 1 + 10 * 1
		- W起决定性作用，b起到微调作用（十个类别都要微调，是各自微调）
		- 权重大小越大的越重要，正值代表促进，负值代表抑制
		- 这个权重矩阵是优化得来的，比如梯度下降算法
		- 数据在处理之后x不会变了，而W会变，整个神经网络就是在改变W得到最好的，一开始是随机初始化，后来过程中不断改进
	- 损失函数：Li = ∑j!=yi max(0, sj-syi+1)
	  collapsed:: true
		- sj是其他类别，syi是当前正确类别，1是△容忍程度，损失函数越大代表错误越大，否则没有错是0
		- 改进：损失函数 = 数据损失 + 正则化惩罚项
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403101622220.png){:height 76, :width 730}
			- 正则化惩罚项R(W)是有关于W的损失，λ是惩罚系数，两个模型可能结果一样但是实际相差很多，比如[1,0,0,0]和[0.25,0.25,0.25,0.25]这种，而[1,0,0,0]就过于关注局部，出现过拟合状况
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403101627535.png)
	- Softmax分类器：
	  collapsed:: true
		- 目的：得到损失函数了，但是我们需要的是相应类别的概率
		- sigmod函数
		  collapsed:: true
			- 我们之前用过，把(-∞，∞)压缩到[-1, 1]
		- softmax
		  collapsed:: true
			- 先exp，这里是放大作用，把正数放的更大，负数几乎变为0
			- 再归一化，得到概率值
			- -log，通过概率值计算出损失，我们只关注正确的概率值
				- 使用log的原因：在0处log接近-∞，在1处log接近0，把[0,1]区间变为[-∞,0]，再添一个负号变为[0,∞]
			- 整体公式表示
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403101720032.png)
	- 目前整体流程：
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403101721873.png)
		- 回归任务是要得到一个值，用得分值计算损失
		- 分类任务由概率值计算一个损失
	- 前向传播：
	  collapsed:: true
		- 有了x和W之后怎么得出一个L损失
	- 反向传播：
	  collapsed:: true
		- 更新W，优化
		- 链式法则：计算梯度过程需要逐层计算，往回求偏导，这也就是叫做反向传播的原因
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403101729848.png)
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403101731567.png)
		- 可以一个步骤一个步骤的算，也可以框起来一大块一大块的算，eg一个sigmod function
		- 常见门单元（操作）
		  collapsed:: true
			- 加法门单元：均等分配x+y=a
			- MAX门单元：给最大的 max(0, x)，只会把梯度传给最大的，小的值就没有用了
			- 乘法门单元：互换的感觉x·y=q，对x求偏导是y，对y求偏导是x
	- 整体架构：
	  collapsed:: true
		- 层次结构：输入层，隐层1，隐层2，输出层
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403101744981.png)
		- 神经元：就是输入数据，输入层输入多少个就代表有多少特征输入
		- 全连接：输入层的1个特征对应隐层的全部特征
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403101748391.png){:height 257, :width 234}
			- 比如说原来一个特征转换成四个特征
-
- 注意力机制
	- 有效的从大量的输入信息或者外部信息中筛选出和任务相关的特定信息，减少冗余信息的干扰，大幅度降低模型的计算量
- 记忆增强网络
	- 为神经网络增加一块外部的记忆，增强记忆能力
	- 实现方式
		- 结构化：矩阵或者向量来存储
		- 基于神经动力学的联想记忆