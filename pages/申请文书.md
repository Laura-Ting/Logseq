- 个人陈述：
  collapsed:: true
	- 内容部分一定要说的3点：
		- Why this major? Why this university? 专业，学校
			- 项目或者实验室非常适合，感兴趣
			- 如果是授课硕：哪些课感兴趣，哪些课值得去学，帮助我实现职业规划，帮助我实现行业的了解
			- 如果是研究型硕士：哪些实验室，哪些项目，哪些课题感兴趣，不要假大空，一定要落实
			- 所以：个人兴趣，心路历程，行业发展和自己的见解
		- How you experience? personal or academic have shaped you? 个人经历
		  collapsed:: true
			- 突出优势，实习公司，干了什么什么样的项目，参与到了什么之中，发现自己在其中是非常享受的，感到自己的热忱和激情，并且觉得这个行业在未来的发展是非常有前途的，自己愿意选择这门学科
			- 或者说你曾经发表过一篇什么样的论文，跟着某一个教授参与了某一个科研项目当中，并且让你觉得这个科研的过程非常契合你，科研过程中感受到自己的热忱和享受
			- 如果没有科研和实习，两个角度可以去思考：
			  collapsed:: true
				- 平时上课过程中做过的一些大课题，大作业，不一定项目要高大上，但是要透露出自己的思考，自己的贡献，自己的创造力，作为优势去展现
				- graduation project毕业设计
				- GPA，托福，雅思，GRE，课外比赛获得的奖项
		- future career? long-term or short-term goals 未来职业规划
			- 短期目标：把书读完，拿下学位并不是重要的，重要的是一定要提升自己，希望在这几年内如何提升自己呢？
			- 长期目标：分清自己走那条道路，毕业之后进企业，还是毕业之后去当老师进高校，还是毕业之后打算自己去创业
	- 结构
	  collapsed:: true
		- 第一段：你的兴趣因何而起
			- 小故事，现场开一个情景，先描述一个人物，描述一件事情，这种慢慢进入的感觉
		- 第二段：你的本科知识储备
			- 本科阶段重要的基础课，重要的专业课，两三个，三四个体现专业特点的即可，变相证明你在这个专业是有继续发展的能力的
		- 第三段，第四段：讲你过去的科研，实习，比赛，大作业
			- 如果本专业科研和实习同等重要的话，一段科研，一段实习不错
			- 没有的话，一段比赛，一段过去的大作业
		- 第五段：长短期目标
		- 第六段：为什么选择这个学校，这个学校的这个项目
			- 怎么吸引你，怎么好，适合你
			- It provides a dynamic environment for great graduate education with challenging opportunities, world-renowned faculty, and state-of-the-art facilities.
	- 辞藻
	  collapsed:: true
		- grammerly
		- 谷歌翻译
		- 三稿法：
		  collapsed:: true
			- 初稿用中文(把内容和结构都搞清楚)
			- 第二稿：英文的准确性，不要有任何语法错误，长短句结合，自己的表达得体贴切
			- 第三稿：提升文采，形容词替换，important，interesting，excited大众化的词汇用高级词汇代替，在句式上提升，比如加入一些不定式，加入一些倒装等
	- 去官网上看，希望什么样的个人陈述
	- 不要踩坑
	  collapsed:: true
		- 名人名言开场，还不解释
		- 我小时候xxx dream job：from a young age, I have always been interested in, throughout my life I have always enjoyed而是what motivates you now
		- 不要盲目陈列，你从中收获了什么，这段经历如何帮助你成为一个ideal candidate，实习的时候遇到什么困难，怎么解决的，成长在哪里，收获在哪里
		- 夸夸其谈
			- More importantly, I now confront issues instead of avoiding them it is exciting to discover solutions to problems that affect others, as  was able to do as part of the 1st Place team for the 2010 United Nations Global Debates Program climate change and poverty take a natural interest in global issues, and plan to become a foreign affairs analyst or diplomat by studying international affairs with a focus on national identity.
- 如何写一个英文邮件
  collapsed:: true
	- 合适的邮箱，outlook，gmail
	- 构思
	  collapsed:: true
		- 对象
		- 主题：subject
		- 目的
	- 开头
		- Dear Professor xxx
		- 开头的第一句是客套话：新起一段，I hope this letter finds you well.
	- 主体：核心，开门见山，要说的话放到前面去说，总-分结构
		- My name is Li Ting. I am a xxx student at Universityxxx. I am writing this letter to ask whether it is possible for me to xxx. eg比如说问下一步应该如何去做，to ask what is my next stage of the research.之后再写我现在在做一个什么样的内容，数据非常不理想，我想知道下一步该怎么做
		- 分段很重要：一个语义分一段，每一段60-100词左右(学术可能150-200)
		- 长短句结合
		- 注意用词
			- 如果是偏学术：把句式拉长一点，用词变得高级一点，可以把形容词换成它的名词形式，但是不要用多，只是画龙点睛
	- 结尾：一段就好
	  collapsed:: true
		- 如果是期待对方给你一个回复：I'm looking forward to hearing from you. Should there be any questions or anything I can do to assist with you please do let me know/ please feel free to ask me. Thanks a lot for your help./ Thanks a lot for your kindness/patience.
	- 落款：重新另起一段，顶头不空格
	  collapsed:: true
		- Yours sincerely. Best regards. With kind regards.
	- 技巧：
	  collapsed:: true
		- 回复邮件时，可以看对方是怎么分段，分几段就有几个语义，你就对应回复
		- 巧用连接词：first of all, second of all, meanwhile, consequently
	- 工具：
	  collapsed:: true
		- 中式英语之鉴
		- grammerly
- 陶瓷信结构
  collapsed:: true
	- 介绍一下你自己：My name is xxx. I'm currently a fourth year student at xxx University, and my major is xxx.
	- 表达一下你想去他那读书的意愿，证明一下自己的能力表达一下做科研的兴趣和他的科研方向是相似的
		- 我本科的时候做过xxx的科研，跟哪个大牛做什么样的东西，这一段的意义就是研究背景，研究兴趣方向是什么
		- 他的实验室的项目非常吸引你，太好玩了，太有兴趣了，研究课题，研究方向
- 面试
  collapsed:: true
	- 回复答谢信：自己对这次面试机会的重视，如果没收到之后可以催一催
	- 常见的面试问题
	  collapsed:: true
		- 自我介绍 1min左右自述，自己是什么样的人，有什么样的过往经历，为什么要申请某个项目或者某个工作
		- 已修课程：这些课程的英文翻译，代表性
			- 基础课
			- 专业课
		- 科研经历/毕业设计/研究计划
		- 专业问题：看人，最好能捞到经验
		- 实验室项目：实验室感兴趣的方向，哪一个课题最感兴趣，去官网上看他们在做哪些研究，有哪些具体方向，对哪一个最感兴趣，其中论文有阅读过一两篇，对它有没有自己的想法
		- 长短期目标
		  collapsed:: true
			- 短期：毕业前，还有毕业前那么一两年打算干什么，继续做科研（对这个专业的热爱），做老师（喜欢当老师），去企业打工（更喜欢与人沟通嘛，喜欢人更多的环境嘛），要做好为什么的准备
			- 长期：自己对自己的人生有一个清晰的规划，终极目标，未来想成为什么样的人，明确的定位
		- 提问环节：被很多人忽略，但是也很重要
		  collapsed:: true
			- 假如我想来咱们实验室读博的话，您认为现阶段的我还需要做哪些方面的准备呢？
			- 刚刚和导师面试整个流程中，一些探讨过的话题进行反问
			- 导师发过的论文，其中有代表性的认真读，对其中不理解或者不会的点询问一下
	- tips：
	  collapsed:: true
		- 网络面试的话，写tips
		- 着装
- 项目经历如何回答-实例
  collapsed:: true
	- 我注意到你曾经有xxx的经历，能否为我们描述描述呢？300-400字 1min
		- 中文
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404012114612.png)
			- 我这个项目究竟做了什么？1-2句话
			- 设计内容包括哪几方面
			- 详细介绍项目一步一步如何做的，首先，然后，最后，总结  总-分-总
		- 英文
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404012118420.png)
			- 使用标志词，过去式或过去完成时，多使用名词书面化
		- 总-分-总，时间顺序
- 项目经历如何回答-理论
  collapsed:: true
	- 用一句话概括项目时间，参与人员，项目实现了什么
	  collapsed:: true
		- 我曾经在几几年大几的时候，作为牵头人，和其他两个小伙伴一起共同实现，搭建了一个自动垃圾分炼的小机器人
		- 我利用大四的时间开展了我的毕业设计研究，调查了什么，实现了什么，得出了什么结论
		- 针对我的RP，我计划在未来的多长时间内，涉及xxx的资源，与xxx一起共同实现什么目标
	- 详细展开研究内容
	  collapsed:: true
		- 时间顺序，一步一步介绍如何推进这一项目
		- 我先设计了调查问卷，确定了被访人群或购买了相应的实验原材料。第二步收集调查问卷的反馈，统计并进行数据分析或将实验原材料搭建成实验系统收集实验数据，自己是如何一步步开展实验并进行收集处理得出结论的
		- 标志词：initially，later on，finally，subsequently
		- 重要的核心结果或突破
			- 你根据你采集的数据发现了新的社会结构，再比如你运用前人已有的技术实现了新的应用，或者你用新材料搭建了新结构（创新点和买点）
			- 我这项科研的巨大突破是，我这项研究的巨大价值应用是
			- what's really fascinating about my work is , an essential breakthrough about my work is, my data shows promising results that
		- 最后一句话总结：发表了什么论文，申请了什么专利，有什么样的实物产出，精神上我收获了很大的成就感，获得了继续研究的动力
	- 导师的反馈
	  collapsed:: true
		- 你遇到什么困难，如何应对并解决，哪些工作是你独立完成的，你在这个项目中的工作量到底有多少，如何与其他成员配合完成这个项目的，你觉得这项研究有什么不足，自己做的有什么不好的地方，未来是否有进一步改善，有没有相关计划
- 没有科研经历怎么办
  collapsed:: true
	- 比赛+项目：通过文书包装转化成科研经历
	- 课堂大作业
	- 实习经历：参与过的项目，做过的课题，实质性的进展以及具体的产出，
- 属于自己学科的独立理解与认知
  collapsed:: true
	- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404020009378.png)
	- 扩充知识储备
	- 独特的思考习惯
- 查文献
  collapsed:: true
	- 硕博论文
- 读论文
  collapsed:: true
	- 带着问题去读，不要一行一行
	- 先看结论再看摘要
	- 重点段落的大意，边读边思考在空白出标注
- 如何写研究计划
	- 内容：你对这个专业，课题的认知，理解和想法
	- 结构：
		- 第一段：Abstract
			- 上来用一两句话去写research background，用一两句话去描述literature review里面的技术，用一句话去涵盖research method和research aim（我这篇研究计划打算用xxx方法达成xxx目的），再用一句话去涵盖expected results（我希望结果是什么样的）
		- 第二段：Research Background，在什么样的大环境下开展，你要研究的这个课题中的某个话题因何而起
			- 比如说是无人驾驶技术，无人驾驶技术可以解决“路怒症”，司机的违法行为；解放人的双手，能让驾驶员在驾驶过程中去干其他的事情
		- 第三段：Research Aim
			- 实现什么技术
		- 第四段：Literature Review，细化到了技术层面，站在什么样的技术成果和已经研究好的成果基础之上开展新的研究
			- 做好充分调研，如果有代表性的文章，展开写，每一篇文章展开成一个小段落，大家比较类似的话就是xxx在xxx条件下做了xxx事取得了xxx结果，也可以加一些图表，别人已经获取的实验数据
		- 第五段：Research Method
			- 做实验，设计实验，构建实验，做实验，记录数据，实验结果说明问题，不理想的话分析哪里出现了错误
		- 第六段：Expected Results
			- 研究目的具体落实成结果，和第三段交相呼应，xxx种类的激光雷达可以在无人驾驶中有应用
	- 格式
		- 全文Times New Roman
		- 字体大小12点
		- 行与行之间2倍行距
		- 每一大段标题加粗但字体不变，字号不变，行间距也不变
		- 每一段开头空四个英文字符
		- 每一段与另外一段之间不空行，回车另起一段就可以
		- 大段与大段之间空一行
		- 文章最后如果有参考文献的话，行距变为1倍，其他不变，注意规范写法
		- 如果有图标记得给图标加题注，图片下方图几 xxx，表格上一行表几 xxx
		- 表格里面一倍行距，字体大小可以根据表格内容做修改，字体不建议比12号正文大也不建议比9号字小
		- 在研究背景和研究综述，使用第三人称，客观性
		- 在目的，方法，预期结果，推荐第一人称，主观能动性，研究动向
- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404062318195.png)
- 项目经历
  background-color:: yellow
- 管道机器人缺陷检测项目
  collapsed:: true
	- 项目内容：机器人在管道上进行巡检，利用YOLO算法发现缺陷，及时上报
	  the robot conducts inspection on the pipeline, finds defects by using YOLO algorithm, and reports them in time
	- 创新点：
	  collapsed:: true
		- 我们对YOLO算法进行了改进，和Transformer结合
		  id:: 66123825-7126-413a-b47f-39a3b9c6e770
		  collapsed:: true
		  We have improved the YOLO algorithm and combined it with Transformer
			- 利用Transformer的注意力机制增强YOLO-v5的检测能力，聚焦关键，忽略冗余
			  Enhance the detection capabilities of YOLO-v5 with Transformer's attention mechanism, focusing on key points and ignoring redundancy
			- Transformer的自注意力机制可以捕捉图像中的长距离依赖关系，这意味着模型能够更好地理解图像的全局上下文信息
			  Transformer's self-attention mechanism can capture long-distance dependencies in an image, which means that the model can better understand the global context of the image
			- 在backbone的最后一块，将最后一个C3替换成C3TR
			  On the last piece of backbone, replace the last C3 with C3TR
	- 你的工作量
	  collapsed:: true
		- 参与数据重新标定，算法的选择、训练、调参、基于Attention机制改进和部署等任务
		  Participate in tasks such as data calibration, algorithm selection, training, tuning, attention-based mechanism improvement and deployment
		- 参与算法图形化界面编写，并将异常信息存入数据库，与甲方对接
		  Participate in the compilation of the graphical interface of the algorithm, save the abnormal information into the database, and connect with Party A
	- 基础问题
		- 介绍一下YOLO
		  collapsed:: true
			- 做什么的：一阶段目标检测，很快
			- yolov5网络结构
			  collapsed:: true
				- 图示
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404071426113.png){:height 555, :width 469}
				- 结构
				  collapsed:: true
					- backbone：用来提取最基本的特征信息
					  collapsed:: true
					                      It is used to extract the most basic feature information
						- conv+csp组合四次，最终再sppf(针对最后一个)
						- conv：Conv2d卷积+BN批量归一化+SILU激活函数
						- csp：残差中又残差
						  collapsed:: true
							- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404071436674.png){:height 109, :width 406}
							- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404071437392.png){:height 131, :width 410}
						- sppf：卷积和最大池化相结合，增强高级语义信息
						             Convolution and maximum pooling are combined to enhance high-level semantic information
							- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404071439341.png){:height 169, :width 654}
					- neck：对backbone来的特征信息进行增强
						- 双塔结构：先自底向上，再自顶向下，conv+csp+上采样
						- 自顶向下：将高级语义特征信息进行进一步加强
						- 自底向上：低级细节信息向上传递和加强，更有利于定位
					- head：任务导向，对他最后的输出进行损失函数约束，得到目标框输出
						- 得到三个不同尺度的特征图，80x80预测小目标，40x40预测中目标,20x20预测大目标
				- 代码
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404071453811.png){:height 500, :width 335}
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404071456977.png){:height 398, :width 391}
			- yolov8网络结构
			  collapsed:: true
				- 示意图
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404071504921.png){:height 605, :width 645}
				- 启发点
				  collapsed:: true
					- 使用conv中k=3，s=2，p=1，可以得到size/2, channel*2的结果
					- concat拼接操作要保证size相同，可以使用上采样；而下采样用的是conv
				- 和v5的区别：
				  collapsed:: true
					- v5中的C3被替换成C2f，C2f相当于多个残差
					  collapsed:: true
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404071512355.png){:height 410, :width 195}
					- 检测头解耦
					-
		- 介绍一下Transformer
			- 编码与解码
			  collapsed:: true
				- 每个Encoder和Decoder都是串联的组合
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404092031802.png){:height 519, :width 351}
					- 经常见到的6层结构，理解为一次完整的变形都需要2*6=12步操作
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041441008.png){:height 395, :width 544}
				- Encoder
				  collapsed:: true
					- 每个Encoder包含self-attention和前馈网络
					- 回顾attention：本质上就是通过加权求和获得对上下文的全局感知
					- self-attention就是变形金刚的拆解对照表，计算各个零部件的权重，标明互相间的关系
					  collapsed:: true
						- 每个self-attention会分解为8个部分：multi-head attention
					- 前馈网络：根据这些权重变一次形状
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404092032390.png){:height 375, :width 601}
				- Decoder
				  collapsed:: true
					- 除了self-attention和前馈网络，还多了一层encoder-decoder attention
					- encoder-decoder attention作用：在组装时不光只考虑自己，还要兼顾拆解时的整体信息
						- 比如机器翻译：解码时每个词不光要看已经翻译的内容，还要考虑encoder中上下文的信息
			- 数据的流动
			  collapsed:: true
				- 算法将单词向量化，嵌入位置信息，变成统一长度
				- encoder将其作为输入，通过self-attention和前馈网络发送到下一个编码器
					- self-attention就相当于一个零件自查表，通过权重标明相互关系，嵌入上下文信息
					  collapsed:: true
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041526287.png){:height 329, :width 625}
					- 每个输入向量先嵌入位置信息
					- 然后分别乘以3个训练好的向量Query，Key，Value
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041527045.png){:height 397, :width 638}
					- 再用每个单词的Q向量和所有单词的Key向量相乘，得到的权重就是attention
					- 然后归一化，用softmax函数过滤掉不相干的单词，乘以V向量后加权求和，得到输出向量Z
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041527161.png){:height 508, :width 622}
				- 矩阵表示
					- 总过程
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041537311.png){:height 376, :width 697}
					- 先得到QKV三个矩阵
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041529404.png){:height 426, :width 570}
					- 再计算self-attention，和X相比，Z的维度没有变，只是其中掺入了其他单词的上下文信息
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041532961.png){:height 187, :width 658}
					- 在multi-head attention中，使用了8个不同的权重矩阵，主要目的是为了消除QKV初始值的影响
			- 各个模块协同工作
			  collapsed:: true
				- 编码器：处理输入序列，将输出转化为attention向量K和V（零件拆解说明书）
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041543196.png)
				- 解码器：一边看自己的拆解说明书，一边考虑拆分时与其他零件的相互关系，每步组装输出一个零件，重复这个步骤就完成了变形
					- 解码器的输出是个向量->变为单词：
						- 线性层：简单的全连接网络，将解码器输出投影成一个一维向量，这个向量的维度很长，包含了所有可能出现的单词总和
						- softmax层：进一步归一化，其中概率最高的单词就是最后的输出
							- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041546906.png)
	- 模型局限性：
	  collapsed:: true
		- YOLO在大量数据下训练结果较好(例如说COCO有20万张图片，80个类别)，在模型训练时数据集中只有少量，同时缺陷的种类也是很固定，就那么几种；我们尝试过利用数据增强来产生一些新的数据进行训练
		  YOLO has good training results on a large number of data (for example, COCO has 200,000 pictures and 80 categories), but only a small amount of data set is available in model training, and the types of defects are also very fixed, just a few. We have tried using data enhancement to generate some new data for training
		- 这是根据特征进行学习，并没有语义判断，比如说在旁边的街道上有一个比较小的车而它会识别成生锈的缺陷
		  This is learning based on features, not semantic judgments, such as a small car on the street next to it and it recognizes a rusty defect
		- 还有一些典型特征，比如说有阀门泄露情况的发生，但是模型学习到的是阀门的特征，也就是说漏水或者漏燃气的学习权重较低，这个目前没有想到什么好的解决方法
		  There are also some typical characteristics, such as the occurrence of valve leakage, but what the model learns is the characteristics of the valve, that is, the learning weight of water leakage or gas leakage is low, and there is no good solution to this at present
		- 对于缺陷情况的判定，有一个类别是锈蚀，到什么样的程度才算锈蚀，有些情况有一个小点但是没有坏，有些情况坏了很大，这个度怎么判断
		  For the determination of defects, there is a category is rust, to what extent is rust, some cases have a small point but not bad, some cases are very bad, how to judge this degree
		- 而且对于分类也有有点模糊，因为锈蚀而裂缝是算锈蚀还是算开裂
		  And there is also a bit of ambiguity about the classification, because of rust and cracks are counted as rust or cracking
	- 当然也有一些改进思路：
	  collapsed:: true
		- 传统的多分类可能会取得良好的效果，但是这还需要考虑实时性的问题
		- 随着三维视觉的发展，对于重复的位置，机器人应该可以识别出来哪个地方它来过，我们可以利用slam技术对整个管道进行建模，每次对特征点增量式比较哪里有差别，有多大差别，就是它没有记忆，每次都重复的利用yolo进行目标检测会浪费很多不必要的GPU资源
		- 缺陷的变化是慢慢形成的，而且一个机器人只在一条管道上进行巡检，如果能结合一些时间序列，位置进行变化
		- 背景问题：在观察的时候，发现混淆矩阵中background占较大比例，影响了其他的类别
		- 分类问题：数据标注对模型训练起至关重要的作用，有时候有些分类可能不太合适，没有结合那个圆柱型的轨道的特征，还有比如说每次变形的特征就不太好提取
- CNVM
  collapsed:: true
	- 利用SVM来替换卷积网络中的全连接层，经实验验证，效果会提升2%-3%
	- CNN的最后一层卷积层的输出作为特征向量，然后使用SVM进行分类
		- CNN，它通过多层卷积和池化操作，能够有效地提取图像中的特征信息。CNN在图像分类任务中取得了巨大的成功，但是在处理大规模数据集时，其计算复杂度较高，训练时间较长。这就为我们引入SVM提供了契机。
		- SVM，是一种二分类模型，它通过在特征空间中构建一个最优超平面，将不同类别的数据点分开。SVM在处理高维数据时具有较好的泛化能力，并且能够有效地处理大规模数据集。然而，SVM本身不能直接处理图像数据，因为图像数据是高维的，且具有空间结构。因此，我们需要将CNN与SVM相结合，以充分利用两种模型的优势。
	- 模型局限性：
		- 和数据集有很大关系，泛化能力有待增强，有时候会发生过拟合
		- 模型有上限，到85%，如果说真正投入使用的话，把有病的人判断成没病的这个后果是很严重的
		- 我们这个模型只考虑了二分类问题，没有考虑多分类，但是实际上的肺炎应该有很多种异常
		- 当时没有使用预训练的CNN模型，如VGG16或ResNet，来提取图像的特征
		- CNN和SVM是两个不同的模型，它们的训练过程是分离的，因此需要额外的计算资源和时间
	- 改进：
		- 针对多分类：多个分类面的参数求解合并到一个最优化问题比较难，利用多个二分类器构建
		- 超参数优化：使用网格搜索、随机搜索或贝叶斯优化等方法来自动寻找最优的超参数组合