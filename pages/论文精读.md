### ResNet：
	- 标题：Deep Residual Learning for Image Recognition
	- 摘要：
	  collapsed:: true
		- 当前问题：深的神经网络非常难以训练。
		- 我们做了什么：使用一个残差学习框架，让训练非常深的网络比之前容易很多
		- 我们把这些层作为一个学习残差函数相对于层输入的一个方法，而不是说像之前一样学习unreferenced方式
		- 我们提供很多实验证据，来表示这些残差网络非常容易训练，而且能够得到很好的精度
		- 在ImageNet数据集上使用了152层，比VGG还要多了8倍，但是有更低的复杂度
		- 用了这些残差网络中的ensemble之后得到了3.57%的测试精度，赢得ImageNet2015年竞赛。我们还解释了怎么在CIFAR-10上面训练到1000层的网络
		- 深度对视觉很重要。我们仅仅把网络换成之前学习到的残差网络，得到了28%的相对改进 ，在coco这个目标检测数据集上，也在COCO上第一名。
	- 介绍：
	  collapsed:: true
		- 深度卷积网络很好，因为我们可以加很多层，把网络变得很深，不同的层会得到不同level的feature，
		- 随着网络越来越深，一个好的网络就是简单把所有的层堆在一起就行嘛？当网络变得很深的时候要么会出现梯度爆炸，要么会出现梯度消失。解决他的一个办法是在初始化的时候做的好一些，权重在随机初始化的时候不要太大也不要太小。在中间加入一些normalization，包括BN，可以使得校验每个层之间的那些输出和他的梯度的均值和方差，让比较深的网络是可以训练的，避免有一些层特别大有些层特别小。我们使用了这些技术之后是能够收敛的。
		- 当网络变得更深的时候，性能其实是变差的，也就是精度会变差。这不是一个层数变多了，导致模型变得复杂了的一个过拟合问题，是因为训练误差也变高了(overfitting是说训练误差变得低，但是测试误差变得高)，也就是说网络似乎是收敛的，但是没有到一个比较好的结果
		- 加了更多层之后，精度会变差。考虑一个比较浅一点的网络和他对应的比较深的版本，也就是一个浅层网络再多加一些层进去，如果浅层网络效果还不错的话深层网络的效果是不应该变差的。深的网络新加的那些层，我总是可以把这些层学成一个identity mapping，也就是输入是x，输出也是x，比如说把一些权重学成简单的1/n !使得你的输入和输出一一对应。但是实际情况是，虽然理论上权重是可以学成这个样子的，但实际上你做不到，假如说就让SDG去优化，存在一个比较好的结果是下面那些层学到和shallow一样的，上面多加的那些层变成identity，这样精度就不应该差，但是实际上SGD找不出来。
		- 这篇文章就是显式地构造出一个identity mapping使得深的网络不会比浅的网络更差。x是我们浅层网络已经学到的东西，f(x)=h(x)-x是我们新加的深的网络要去学习的东西输出是f(x)+x。浅的网络输出的是x，x进入一个层，之后relu激活，再进到一个新的层，结果是f(x)，之后f(x)+x再做一个relu激活
		- shortcut connection，不会增加任何要学习的参数，不会增加模型复杂度，也不会让计算变高，因为只是个加法而已，这个网络仍然是可以被训练的
		- 非常深的residual net非常容易去优化，如果不加这个残差连接，这个plain的版本效果就很差，深的网络越深精度越高
		- 越深的网络越训练不动，56层比20层在训练集和测试集上的误差都更大
	- 实现：
		- 残差连接的两种方式
		  collapsed:: true
			- 输入和输出都添加一些零，使得这两个形状能够对应起来做相加
			- 投影，全连接怎么做投影，如果做到卷积上就是1x1的卷积核，在空间维度上不做任何东西，主要是在通道维度上做改变，使得输出通道是输入通道的2倍，这样就能把输入和输出对上，1x1的卷积核，步长stride为2，这样高宽都能匹配上
		- 做法：
		  collapsed:: true
			- 把短边随机采样到[256,480]，这样在随机切成224*224的时候随机性会更多一点
			- 把每个pixel的均值都剪掉了，使用了一些颜色的增强
			- 权重，批大小256，学习率0.1，每次除10，当错误率比较平的时候就会÷10，但是这个要人看着
			- 训练了60*10^4一个小批量
			- 权重衰减率0.0001，momentum0.9
			- 没有用dropout，因为没有全连接层了
			- 测试，10crop，随机10张图片做测试，结果取平均
	- 实验结果：
	  collapsed:: true
		- 有时候训练集的误差会比测试集还高，可能是因为用了数据增强，相当于训练集上噪音较大，测试集上误差较小
		- 为什么会突然下降，学习率在这时*0.1，跳太早后期收敛会无力
		- 有了残差连接，误差会减小，收敛会快很多
		- 在实验时测试了三种残差连接方式的结果：添加0，需要的时候投影，全部都投影
			- 第一种最差，第二种和第三种差不多，但是第三种比较昂贵，计算复杂度较高
	- 怎样构建更深的resnet
		- 到50或者往上，会引入bottlenet design。当我变深的时候，可以学到更多的模式，通道数也就变大一点，从64变到256，计算复杂度扩大16倍，于是我们通过一个1x1的卷积把它映射投影回64位，再做3x3通道数不变的一个卷积，再投影回256。这种设计就会让虽然通道数变成了原来的4倍，但是复杂度是差不多的
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404191959131.png)
		- 如果后面新加上那些层不能让模型变好的时候，因为有残差连接的存在那些层应该不会学到任何东西，应该都是靠近0。没有加的时候实际上是没有收敛好的，加了之后能够fall back，变回一个简单模型
		- 还有从梯度的角度来理解：让梯度不会消失，误差反串的时候训练比较快
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202404192032252.png){:height 198, :width 346}