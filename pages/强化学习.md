- Chapter 0 概述
	- 基本工具
	  collapsed:: true
		- Chapter 1 基本概念
		  collapsed:: true
			- 概念: state, action, reward, return, episode, policy
			- e.g. Grid-world examples：机器人在网格世界里面去找目标区域
			- MDP: Markov decision process 框架
		- Chapter 2 贝尔曼公式(Bellman Equation)
		  collapsed:: true
			- 一个概念：状态值(state value)
			  collapsed:: true
				- vπ(s) = E[Gt|St = s]
				- 从一个状态出发，沿着一个策略所得到的奖励回报的平均值，能够评价一个策略好or坏
				- 状态值越高就说明策略越好，状态值越低说明了策略越差
			- 一个工具：贝尔曼公式
			  collapsed:: true
				- vπ=rπ + γPπvπ
				- 用来分析状态值，描述了所有状态 状态值之间的关系
			- 策略评价(Policy Evaluation)：求解贝尔曼公式进而得到一个策略所对应的状态值
			  collapsed:: true
				- 重要的思想：评价一个策略，得到他的值，基于它的值再改进策略，循环下去，从而得到最优策略
		- Chapter 3 贝尔曼最优公式(Bellman Optimality Equation)
		  collapsed:: true
			- 贝尔曼公式的特殊情况：
			  collapsed:: true
				- 每个贝尔曼公式都对应了一个策略
				- 贝尔曼最优公式就对应了最优策略，强化学习的目标就是在求解最优策略
			- 两个概念：
			  collapsed:: true
				- 最优策略(optimal policy π*)：沿着这个策略能够得到最大的状态值
				- 最优状态值(optimal state value)
			- 一个工具：贝尔曼最优公式
			  collapsed:: true
				- v=maxπ(rπ + γPπv) = f(v) <- 矩阵向量的形式
				- 不动点原理(Fixed-point theorem)
				- 基础的问题：这个最优策略一定存在，最优策略不一定唯一(deterministic确定性的 or stochastic 随机性的)，但最优状态值一定唯一
				- 一个能求解贝尔曼最优公式的算法
	- 算法
	  collapsed:: true
		- Chapter 4 值迭代(Value Iteration) & 策略迭代(Policy Iteration)
		  collapsed:: true
			- 三个算法：
			  collapsed:: true
				- 值迭代(Value Iteration, VI)：上一章中能求解贝尔曼公式的就是值迭代算法
				- 策略迭代(Policy Iteration, PI)：在下一章不需要模型的蒙特卡洛方法中重要应用
				- 截断策略迭代(Truncated policy iteration)
					- 前两个都是他的特殊情况
			- 特点：
			  collapsed:: true
				- 都是迭代算法
				- 每步中都有值更新(value update)和策略更新(policy update)，比如当前策略不太好，做一个策略评价，得到值之后根据这个值改进这个策略，得到新的策略，再估计它的值，再改进策略
				- 需要模型
		- Chapter 5 Monte Carlo Learning
		  collapsed:: true
			- 最简单，第一个不需要模型就能够找到最优策略的方法
			- 没有模型那学习什么
			  collapsed:: true
				- 随机变量的期望值expectation: E[x]≈xbar = 1/n ∑xi
				- 为什么：之前的状态值都是期望
				- 有什么：我们有对x的采样，xi
				- 得到平均值可以很好的近似期望
			- 算法
			  collapsed:: true
				- MC Basic：
					- 实际上就是策略迭代中把依赖于模型的那部分拿掉，换成依赖于数据的，就得到MC Basic
					- 但是实际中算法不能用，效率很低
					- 把复杂的东西和最核心的东西剥离开，最核心很简单，复杂的东西进来是因为要更好地利用数据提高效率
				- MC Exploring Starts
				- MC ε-greedy
				- 特点：
					- 三个算法难度依次增加
		- Chapter 7 时序差分方法(Temporal-Difference Learning) <- Chapter 6 随机近似理论(Stochastic Approximation)
		  collapsed:: true
			- 先学习第六章：
			  collapsed:: true
				- 第五章的算法都是non-incremental, 第七章都是incremental
				- 均值估计(Mean estimation) E[x]
				  collapsed:: true
					- non-incremental: 有1万个采样，全部采到后一次性求平均，得到E[x]的近似值
					- incremental：增量式算法，开始的时候有个估计，这个估计可能不准但没关系，得到一个采样就用这个采样更新我的估计，所以我的估计会越来越准。不用等很久，进来一个用一个，等待过程中就有信息可以去用
				- 算法
				  collapsed:: true
					- Robbins-Monro (RM) algorithm
					  collapsed:: true
						- 实际上在求解一个非常简单的函数等于0的方程，g(w) = 0
						- 但是不需要知道g(w)长什么样子，他的梯度导数全都不需要知道就可以求出来
					- SGD 随机梯度下降(Stochastic gradient descent)
						- 需要特殊的RM算法
					- SGD，BGD(batch)，MBGD(mini-batch)比较
			- 再学习第七章：
			  collapsed:: true
				- 时序差分方法是个经典RL算法
				- 算法
				  collapsed:: true
					- TD计算状态值
					- 思想推广到Sarsa：用TD来学习action value，以此为据更新策略，再计算再循环知道得到最优策略
					- Q-learning：特殊的是TD直接计算optimal action value
					  collapsed:: true
						- 是个off-policy算法：
							- on-policy 与 off-policy
							  collapsed:: true
								- behavior policy：生成经验数据
								- target policy：目标策略，希望这个tartet policy能够收敛到最优的策略
								- 如果behavior policy和tartet policy相同就是on-policy
								- 如果可以不同，那就是off-policy
							- off-policy好处
							  collapsed:: true
								- 之前别的策略所生成的数据为我所用，学习来得到最优策略
					- 统一化视角：这些算法都是类似的
		- Chapter 8 Value Function Approximation
		  collapsed:: true
			- 跨越：从表格表示(tabular representation)到函数表示(function representation)
			  collapsed:: true
				- 表格表示：每个状态都对应一个状态值，这些状态值存在一个表格或者向量当中，访问和修改都很容易，但是如果状态很多or状态连续那么表格形式就不再适用了
				- 用函数表示来近似
			- 算法
			  collapsed:: true
				- State value estimation with value function approximation(VFA):
				  collapsed:: true
					- minJ(w) = E[vπ(S)-^v(S, w)]
					- 过程
					  collapsed:: true
						- 明确目标函数
						- 求目标函数梯度
						- 用梯度上升or梯度下降对这个目标函数进行优化
					- 此处目标函数是J(w)是真实值与函数值之差的平均值，我希望找到一个最优的w使我这个函数更好地区表发vπ(s)
				- Sarsa with VFA
				- Q-learning with VFA
				- Deep Q-learning: DQN
			- 神经网络进入强化学习
		- Chapter 9 Policy Gradient Mathods
		  collapsed:: true
			- 跨越：之前全部都是value-based的，之后两章是policy-based
			  collapsed:: true
				- 基于值：J(w)，w是值函数的参数，要更新这个值函数的参数使得这个值函数能够很好地近似或者估计出来一个策略所对应的值，在这个基础上更新策略，不断循环直到得到最优策略
				- 基于策略：J(theta)，theta是策略的参数，此时策略也由表格行书转换为函数形式，直接去优化theta也就是直接改变策略
			- 内容
			  collapsed:: true
				- 找目标函数：J(theta) = vbarπ，rbarπ
				- 目标函数的梯度：Policy gradient，直接给出了他的梯度
				- 梯度上升(gradient-ascent)or梯度下降去优化目标函数
					- REINFORCE
		- Chapter 10 Actor-Critic Methods
			- 跨越：将policy-based与value-based相结合
			  collapsed:: true
				- 实际上actor对应的就是policy update，critic就是评论一个策略好还是不好（即求他的值，对应的就是value）
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202312181056607.png)
			- 算法
			  collapsed:: true
				- 最简单的actor-critic(QAC)：实际上就是上面那个式子
				- advantage actor-critic(A2C)：引入了一个baseline来减小估计的方差
				- Off-policy actor-critic：因为actor-critic本质上还是policy gradient，而policy gradient自然是一种on-policy算法，将他变成off-policy，使用重要性采样(Importance sampling)
				- Deterministic actor-critic(DPG)：前面的策略都是要求随机的，在每一个状态他有概率选择到所有的action，我们也可以用确定性的策略deterministic
	- 历史
	  collapsed:: true
		- 范式
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202312181123866.png)