- 数据的种类
	- 有序数据
	  collapsed:: true
		- 时序数据：事务涉及时间序
			- eg同一个用户不同时间购买的商品
		- 基因序列数据
		- 时空数据：时间属性+控件属性
			- eg海陆月平均温度
- 数据的质量
  collapsed:: true
	- 数据质量的重要性
		- 低质量数据，eg造成营业额损失，医疗模型，银行信贷业务判断是否贷款
	- 数据质量的问题
		- 噪声，离群点
		  collapsed:: true
			- 噪声
			  collapsed:: true
				- 对于物体：不相关
				- 对于属性：对原始值的修改
					- eg打电话有背景音噪声
			- 离群点Outliers
			  collapsed:: true
				- 特点
				  collapsed:: true
					- 是一个数据对象
					- 但是和绝大多数其他数据对象有很大差距
				- 情况：
				  collapsed:: true
					- 混入噪声产生
					- 这个离群点就是我们要找的，eg信用卡诈骗，非法入侵检测
		- 缺失值/遗漏值 Missing Values
		  collapsed:: true
			- 原因
				- 当时就没有收集，eg人们拒绝给出身高体重
				- 这个属性本身就不应该有值，eg孩子的年收入
			- 处理
				- 丢弃
				- 填充：估计
					- eg：时间序列上的温度监控认为是平滑的
					- eg：年收入，所有人的平均工资代替
			- （人们要做的是
				- 发现模式，发现源数据（主键，依赖约束））
		- 重复值Duplicate Data
			- 出现原因
				- 异构数据融合，数据整合
					- eg一些人有多个邮件地址
			- 数据清理Data cleaning
				- 数据库有一个专门的去重操作，eg哈希
				- 对于完全相同的值，排序也是一个方法
- 相似性和距离
  collapsed:: true
	- 相似性
		- 两个对象有多相似在数值上的表示
		- 通常在[0,1]，越高越像
	- 相异性Dissimilarity
		- 越小两个对象越像
		- [0,上界不定]，完全相同则相异性为0
	- 临近度?
	- 对于简单属性?
	- 欧几里得距离
		- 每个点有n个属性，d(x, y)
		- 实际上是闵克福斯基距离（r维）的特例，欧几里得只是2维，
	- r=1，曼哈顿距离，L1范式，从(0,0)到(10,20)是30
		- eg海明距离?
	- r=2
	- r=∞，就是两个元组对应属性最大的查，上确界距离，r无穷范式
	- 注意：r是参数，n是属性的数量
	- 马氏距离：欧式距离的改进
		- 欧氏距离受变量标度的影响
			- eg现在有年龄和收入，收入的差会很大，年龄怎么也不会差到100岁，这样收入完全支配结果
			- 归一化处理，eg正常的正态分布->标准正态分布
		- 公式：m(x, y) = (x - y)T ∑-1(x-y)
			- 协方差矩阵如果是单位向量，即各维度独立同分布，马氏距离就变成了欧式距离
			- 如果不是，把方差大的减小，方差小的增大，比如说现在是一个椭圆，就把这个椭圆捏成一个单位元
			- 马氏距离不太关注最大方差方向的差异
	- 距离的属性
		- 非负性：d(x, y)
		- 对称性
		- 三角不等式：两点距离小于第三方中转的距离
		- 满足上面三个性质，叫它测度
	- 相似性性质
		- 只有xy相等时，才=1
		- 对称性
	- 二元属性相似性计算
		- p, q两个对象，每个属性只能取0,1
		- f01，f10，f00，f11
		- 简单匹配系数SMC = (f00+f11) / (f00+f11+f01+f10 )
		- Jaccard系数，非对称二元属性，只有1时有意义：J = (f11) / (f01 + f10 + f11)
	- 余弦相似性
		- 忽略0-0匹配，也用于解决非二维数组
		- cos(d1, d2) = <d1, d2> / ||d1||*||d2||，内积，对应属性的乘积加和（0-0匹配没有增加值）
		- 0的时候最相似
	- 相关系数Corr
	  collapsed:: true
		- 两个向量的变化趋势是否一致
		- cov(x, y)：两个变量对均值的变化趋势，正数相同，负数相反
			- 为什么1/n-1，均值作为参数去算，最后一个值是不变的，自由度是n-1？
		- 归一化，
		- 缺点：
			- 只能度量线性关系
		- 互信息：Mutual information，度量非线性关系
			- 给定一个变量的值，就能增强对另一个变量的了解程度
			- 信息和概率的关系：这个事情发生的可能性越高，带给我们的信息就越少
			  collapsed:: true
				- 信息就是熵：事情越无序熵越高
				- 对于变量X，可能有x1,x2,...xn，每个结果有一个概率，p1,p2,...pn
				- X的熵H(X) = -∑pilog2pi，概率乘上标识结果用的比特求和
				- 其中一个结果是（-log2pi），如果pi = 1/32, 则-log2pi = 5，有32个结果，pi是其中的一个，需要5个比特来标识
				- 最低为0，熵最低概率是1时，必然发生则最有序
				- 最高为log2n，n个结果出现的可能性是均等的
				- 有时pi不是给的，而是通过观测得到的
			- I(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X,Y)
	- 相似性的组合
		- kth属性，sk(x, y),定义一个标记变量deltak = 0则把kth属性从整体中拿掉，0-0or0-n；否则=1
		- 公式=？
		- 加权重，对于某些
	- 密度
		- 在特定区域里面彼此的接近程度
		- 密度和临近度相关，聚簇和异常点检测
		- 欧几里得密度最常见
		- 网格，落在里面有多少个，计数
		- 以某个点为中心的密度，每个对象为单位，圆里有多少点
			- 一般来说，里面少于10个点，就认为是离群点
	-
- 数据预处理
  collapsed:: true
	- Aggregation聚集
	  collapsed:: true
		- 两个对象变为1个对象，less is more
		- eg求平均值，合并成绩
		- 目的
		  collapsed:: true
			- 数据压缩
			- 尺度变换（分辨率的变换）
				- eg城市，洲，国家；周，月，年
				- 分辨率越高，会降低数据的方差
	- Sampling采样
	  collapsed:: true
		- 处理所有的数据太难了
		- 应用：
		  collapsed:: true
			- 统计学家，选举
			- 数据科学家处理大量数据100T
		- 原则：
			- 采样需要有代表性，在各方面属性应该和原始数据集类似
		- 方式：
		  collapsed:: true
			- 简单随机采样
				- 无放回，有放回
			- 分层采样
				- 每个阶层都采样
	- 降维
	  collapsed:: true
		- 原因：维度诅咒
		  collapsed:: true
			- 随着维度增加，数据空间变得越来越稀疏，eg2维->3,4,5,6维
			- 各个点的差异变得没有那么大，各个点之间的距离越来越近
		- 方法
			- 维度消减/维归约：
			  collapsed:: true
				- 把高维映射为低维
				- 效率提升，可视化效果变好，把不相关特征拿掉
				- 技术：
					- PCA主成分分析，方差最大的作为第一个轴，第二大的作为第二个轴，...，后面几个都很小了
					- 奇异值分解，监督学习等
			- 特征子集的选择：选择一些
			  collapsed:: true
				- 冗余特征（重复信息）
				- 不相关特征：和挖掘结果相关联的特征才是有用的特征
			- 特征生成：在数据集中从原来属性生成新的属性，帮助更好的挖掘
			  collapsed:: true
				- 特征抽取：eg几何抽出边
				- 特征构建：eg密度 = 重量 / 体积
				- 映射：eg傅里叶变换，小波变换
			- 离散化：
			  collapsed:: true
				- 连续属性变为序数属性 eg小杯，中杯，大杯（有次序关系）
				- 在分类当中用的很多
				- eg鸢尾花，划分点的确定
					- 根据无监督的方式划分，找出划分点（柱状图分布找差距最大的）
					- 监督：通过标签的学习
			- 二元化：离散化的极端，0和1
			  collapsed:: true
				- 一般是先划分成多个，
				- 多用于关联分析
			- 属性转换：利用一个函数把旧值变成更好的新值
			  collapsed:: true
				- 规范化or标准化，有可能会出现规范化后的相关性减弱
-
- ### 第三章 分类：基本概念和技巧
  collapsed:: true
	- 分类
		- 定义：训练集，看做传统的关系数据，只不过多了一个类标号
		- 目标：从已有训练集中获得一个模型
		- 任务：垃圾邮件分类，肿瘤细胞判断，星系判断
		- 基本过程：归纳Induction（从特殊到一般，），Deduction推理（）
	- 基本方法
		- 决策树
			- 构成
				- 内节点：对原有数据集做划分（包括根）
				- 叶节点：标签
			- 特点
			  collapsed:: true
				- 同一个训练集得到的决策树可能不一样，即不唯一（因为划分属性不一样）
				- 分类的过程就是从根移到叶子
			- 决策树归纳算法
			  collapsed:: true
				- Hunt
				  collapsed:: true
					- 基本思路：
						- Dt训练元组的集合：到达节点的记录
						- 如果Dt包含的记录都属于同一个类yt，就认为当前节点是叶节点
						- 如果Dt包含的记录属于更多类别，则需要对Dt进行划分
						- 递归地划分
					- 几个问题
						- 怎么划分：选择哪个属性划分？怎么找划分点？
							- 测试条件：
							  collapsed:: true
								- 和属性类型相关，eg二值，
									- 标称属性：多值划分（有几个分几个）or二值划分（每次分成两个非空子集即可）
									- 序数属性O：多值，二值要求在原来的划分中连续（eg小和大不能放一类），要求整体可比较
									- 连续属性Continuous：多值（找多个划分点），二值（找一个划分点）
										- 离散化：连续属性变成序数属性，等宽，聚类or直接全部划分，静态，动态
										- 二值：找到最佳划分点，可能是计算密集型
								- 和划分数量相关
							- 好不好
							  collapsed:: true
								- 如何定义好不好：
									- 贪心：倾向于更纯净的类分布
									- 需要测量节点的不纯净度，eg5-5分不纯性高，9-1不纯性低
									- Gini系数，熵，误分类误差
									  collapsed:: true
										- 在t这个节点上类别j的比例
										- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403131713259.jpg)
										- Gini：如果当前节点有nc个类别，0<= p(j|t) <= 1，∑p(j|t)^2最大值是所有都在同一个类，是1，Gini最小0纯净度最高；最小是均匀分布(1/nc)^2*nc，Gini最大1-1/nc纯净度最低
										  collapsed:: true
											- 一组子节点：加权求和∑ni/nGINI(i)，最小化，一般用于CART
											- 不同同类型的基尼系数：
												- 对于二元属性；
												- 对于类别属性：多路or二路，比较之后选择小的方法
												- 连续属性：二路划分（找划分点），不用考虑“连续”，eg10-11中间有无数个点，但是划分效果等价于用10or11
												  collapsed:: true
													- 先把所有属性排序，取两个相邻值的平均值，初始一个矩阵，移动一个就可以得到所有的Gini
										- 熵：最大lognc平均分布，最小0只有1个类别，用于ID3和C4.5
										- 误分类：最大1-1/nc均匀分布，最小0只分一类
									- Gain = P - M，P未划分前不纯度，M划分后不纯度，要Gainmax，则最小化M
									- 问题：刚才说的都倾向于生成更多数量的分片
									  collapsed:: true
										- 不能单纯追寻收益，要找增益率
										- GainRATIO = GAIN/SplitINFO，对于上述的ID情况要增加一个惩罚量
						- 树怎么停止扩展？可解释性
							- 如果某个节点所有节点属于同一个类别，或者属性值都相等；分不开就不分，选取较多类别作为当前节点
				- CART，ID3/C4.5
			- 优点
			  collapsed:: true
				- 方便构建
				- 在分未知类别的时候划分很快
				- 小规模树可解释性好
				- 噪声鲁棒性
				- 冗余或不相关可以不选
			- 缺点
			  collapsed:: true
				- 指数级，贪心不能找到最优
				- 基于一个属性划分，难以处理多属性
				- 划分线认为只和坐标轴平行
			- 分类错误
			  collapsed:: true
				- 无法斜线，只能平行坐标轴
				- 希望得到泛化误差较低的决策树，训练误差，测试误差
				- 欠拟合：模型太简单，决策树没有真正抓住模型特点，还应该继续训练
				- 过拟合：模型太复杂，捕捉到的是数据集的细节(可能是噪声)，测试误差变大
				  collapsed:: true
					- 原因：
					  collapsed:: true
						- 数据集小
						- 模型复杂(多重比较问题：有足够多的人瞎猜会有0.9+的概率猜对，初始模型M，划分下一层，变成M‘，△(M, M')>α，即使所有的属性都是不相关的划分但是也有能蒙对的概率)
						- 训练误差不能很好反应泛化误差
				- 增加训练数据的规模
				- 估计泛化误差
					- 验证集：从原来的训练集中划分出，用于寻找模型的超参数，缺点是训练数据变少
					- 测试集：估计泛化误差
				- 模型的选择：应该简单，但不应该过分简单
					- 泛化误差 = 训练误差+a*复杂度
					- 悲观误差分析，树T，k叶子节点数量，err(T)训练误差，a超参数增加，Ntrain训练总记录数
						- 认为泛化误差永远小于训练误差
						- 对模型的训练误差进行统计修正
			-
			- 先剪枝：完全生成之前结束
			  collapsed:: true
				- 同类别同属性值就不要再划
				- 到达当前节点的实例数量太少就不用再划分
				- 当前节点的属性和类属性独立，eg用id预测分数
				- 扩展当前节点没有改进误差
				- 估计的泛化误差低于阈值
			- 后剪枝：先生成之后再做处理
			  collapsed:: true
				- 子树替换：从叶子节点自底向上，如果用一个叶节点替换子树效果更好则代替整个子树
				- 子树提升：一颗子树的某一棵子树来替换这个子树
-
- ### 第四章 分类：替换性
  collapsed:: true
	- 基于规则的分类器
		- IF THEN
		  collapsed:: true
			- LHS左部：条件的合取，RHS右部：结果
		- 应用
		  collapsed:: true
			- 规则覆盖这个实例：这个实例满足这个规则
		- 覆盖率
		  collapsed:: true
			- 给定一个数据集，满足前提条件的记录比例
		- 准确率
		  collapsed:: true
			- 满足前件，也满足后件
		- 如何工作
		  collapsed:: true
			- 问题：有分类，有的没有分类，有的有等多个分类
			- 互斥：每个对象最多只能被一个规则覆盖
			- 穷举性：任何一个值的组合都能找到分类，每个对象至少被一个规则覆盖
			- 同时满足互斥性和穷举性：每个对象只被一个规则覆盖
			- 有些规则本身就不是互斥的：对规则进行排序或者投票
			- 规则不是穷举的：有对象没有触发任何规则，搞一个默认类
			- 规则是有序的：得到决策列表
			- 排序模式：基于规则or基于类别
			- 如何构建规则
				- 直接方式：直接从数据中抽取规则
				  collapsed:: true
					- 序列覆盖：
					  collapsed:: true
						- 从空规则开始执行
						- 每次学习一个规则函数生成规则☆
							- 从一般到特殊：先建立初始的空的规则，加入新的合取项(属性的一些条件)提高规则质量。加了合取项是降低规则的覆盖率但是提高规则的准确性
							- 特殊到一般：随机选一个正例对象作为增长的初始种子，删掉合取项(减掉一些属性的限制)
							- 如何确定应该添加或删除哪个合取项
								- 信息增益：Gain
									- t，pi正例数量，ni负例数量， pi/(pi+ni)正例和准确率都不太低
						- 从训练集中删除已经被规则覆盖的对象
						- 重复2,3步直到完成
					- eg对于二分类：正负例
					- 对于多分类：根据出现频率升序排序
					- RIPPER：
						- 规则增长：一般到特殊，增加合取项减少覆盖范围，直到剩下的全是
						- 规则剪枝：可能会发生过拟合，v=(p-n)/(p+n),p是验证集正例，n是验证集负例，依次从最后添加的合取项开始拿掉，选择v值最大
						- 构建规则集：序列覆盖，选择当前最佳规则，丢弃被覆盖实例。一组数据的描述长度是数据压缩编码后的长度加上模型的长度
						- 优化：替换规则r*，修改规则r'，选择一个可以最小化规则长度的集合(类似于遗传算法)
				- 间接方式：从其他分类模型中抽取规则
					- eg决策树变成规则
		- 优势
	- 基于实例的学习
	  collapsed:: true
		- eg；所有数据的都存储下来，“死记硬背”，很难找到完全匹配的->所以去找近似匹配的对象，最近邻
		- 需要
			- 需要一组标定好类别的数据
			- 给定距离函数
			- 确定k值，前k个
		-
		- 最笨的方法
			- 扫描一遍，用一个最大堆，维护前k个距离最小的；计算量O(n)
		- 加快计算
			- 1近邻特殊索引结果：沃罗诺伊图，相邻的点连线，画连线的垂直线，将空间分块，任何出现在某个区域内的最近邻都是这个点
			- 两个点之间的距离一般还是欧式距离，选择多数类，或者越近权重越大，距离越远权重越小
		- k值选择
			- k值越小，受到噪声点的影响
			- k值太大，受到距离很远的点的影响
		- 需要考虑的问题
			- 缩放问题：防止单个属性影响太大，将属性值规范化
			- 距离函数的规定：欧式距离有缺陷，相同的欧式距离但是差别很大。eg非对称
			- knn本身是消极学习器，eg决策树是生成模型，对这个模型来做处理，之后可以不看这个数据；但是knn不构建模型，分类成本很高，同样它决策边界可以任意形状
			- 基于局部信息
			- 冗余属性导致问题
			- 缺失值导致问题
		- 改善
			- 降低计算费用：k-d trees, LSH
			- 压缩：相同性能的更小数据集
	- 贝叶斯分类器
	  collapsed:: true
		- 基于概率的分类器：基于贝叶斯定理
		- 贝叶斯定理：
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403201624317.jpg){:height 292, :width 426}
			- 某一个条件概率好算，某一些不好算，可以用好算的算不好算的
		- 在分类器里的应用：所有的属性值以及类标号看做随机变量，X1，X2...Xd
			- 目的是找到一个Y，使得Y属于哪个类别的条件概率最大
			- 贝叶斯定理计算后验概率：P(Y|X1X2...Xd)=P(X1X2...Xd|Y)P(Y)/P(X1X2...Xd)想要左侧最大，即右侧最大，给定一个值分母的值不变，分子越大越好，P(Y)可以认为是Y在数据集占比，比较好算，主要是P(X1X2...Xd|Y)，很难找到和X1X2...Xd一样的一个元组
		- 朴素贝叶斯：认为属性之间是独立分布的
		  collapsed:: true
			- P(X1X2...Xd|Y)=P(X1|Yj)P(X2|Yj)...P(Xd|Yj)
			- 如何计算给定Yj看Xi的概率，也是从训练集中看
			- P(Yk)=Nk/N，这个很好算，数数就行
			- 给定一个类别，求属性值P(Xi|Yk)=|Xik|/Nk，Xik既具有Xi的属性，有属于Y类
			- 对于连续属性：
				- 需要进行离散化操作，（连续分布具体值的概率是0），分成很多个桶，连续属性变成序数属性
				- 概率密度估计：假设说服从正太分布，每个方程两个参数，期望和方差，用分布参数计算条件概率
				- 问题：两边有可能都是0->无论怎么样，都让某个条件概率不能是0
				- 拉普拉斯估计：分子+1/分母+c，c是类别的数量，即使原来的分子分母都是0，我也认为是均匀分布
				- m：分母+先验概率？
			- 总结：
				- 对于噪声点比较健壮
				- 缺失值的处理：通过忽略一些值来处理
				- 不相关属性值健壮：均匀分布不影响
				- 独立性假设不一定成立：bigggg problem，贝叶斯信度网络？
			-
			-
	- 人工神经网络 ANN
	  collapsed:: true
		- 输入节点和输出节点全连接，互联的节点和加权有向边
		- 感知器，最简单的神经网络，单层的网络
			- f=sign(wx)，大于0分为1，小于0分为-1
			- 调整：预测错了，猜大了往上调，猜小了往下调
			- 感知器是一个线性划分的面，非线性可划分的情况感知器做不好，eg异或
			- 隐藏层的引入：中间的每一个点和初始输入都构成一个感知器，可以解决任何一个具有非线性决策边界的问题
		- 神经网络是一个多层的，在感知器基础上进行扩展，学习神经元的权重
		- 前馈网络：数据只能往前传
		- 递归网络
		- 需要考虑的问题
			- 选择几个输入节点
			- 输出节点数量
				- 二元分类只需要一个输出，多元多个
			- 多层神经网络会出现过拟合问题
			- 构建神经网络的时间很长
		- 未来发展趋势
			- 需要获取大量数据：得不到高质量的大量数据，如何利用不标注数据学习，谷歌大脑
	- 支持向量机
	  collapsed:: true
		- 不同的划分，哪种划分更好：中间的距离叫做间隔，希望得到间隔大的决策边界
		- 线性SVM
			- 只有和上下两个边界相交的两个点才有 大于等于0，所以叫支持向量
		- 非线性SVM
			- 属性变化，把低维空间变到高维空间的，在高维空间可以进行线性划分
	- 组合方法
	  collapsed:: true
		- 构建一组分类器：预测数量最多的类标号就是预测的标号
		- 根据抽样分布操作
		- 通过输入特征操作，随机森林
		- 通过类标号，基于抽样分布，装包，eg有放回1-(1-1/n)^n，问题：提升，重视之前分类错误的记录（一开始均匀采样，每一轮对选择概率进行调整，如果上次被分错了，那么采样概率应该提高，上次被分对的采样概率可以降低）
-
- ### 第五章：关联分析
	- Data：set of sets，
	- 关联规则的挖掘
	  collapsed:: true
		- 在某些元素出现的取情况下，某些元素更容易出现，通过规则预测  {diaper}->{beer}逻辑蕴涵
		- 频繁项目集
		  collapsed:: true
			- 项目集：一个或者多个项组成的集合
			- k项目集：包含了k个项目集
			- 频繁：这个项目集被很多个事物包含 ->统计
			- 支持度计数 support count 但但是单纯看这个值不太准，需要看占比
			- 支持度：支持度计数/总数
			- 把支持度作为阈值minsup，超过minsup的被叫做频繁项目集
		- 关联规则
		  collapsed:: true
			- 蕴含表达式：X->Y，X项目集在事务中出现时Y更倾向于出现
			- 支持度support(s)：X和Y并同时出现，（但是X出现而Y不出现的事物更多，需要看占比）
			- 置信度confidence(c)：包含X所有的事务中Y也出现
		- 形式化任务
		  collapsed:: true
			- 输入：事务集T，想要找到support>=minsup, confidence>=minconf
			- 暴力法：列出所有，但是计算开销太大，总数=∑Ckd * ∑C(d-k)j，左部从k个中选d个，右部从剩下的选j个
				- 10,20,30...100：K，M，G，T，P，E，Z，Y，B，N
			- 利用剪枝：
			  collapsed:: true
				- 用相同的项目集做的支持度是相同的，但是置信度不同
				- 首先找支持度满足minsup的项目集(频繁项目集)
				  collapsed:: true
					- 暴力：太多了 ，形成lattice
					- 扫描，满足+1，前提是必须有足够大的内存 O(NMw)item总共有N个，候选总共有M个，item长度为w  M是2的d次方，太大了
					- 策略：
						- 减少候选的数量M：先验定律帮助减少
						  collapsed:: true
							- Apriori principle：一个项目集如果不频繁，那么他的超集一定不频繁，反单调性(添加是的支持度不会上升，而是不变或下降)
							- Fk是频繁k项目集，Lk是k项目集候选
							- 先生成候选，再进行候选剪枝，支持度计数
								- 生成候选：
								  collapsed:: true
									- 完备性
									- 冗余？ 一个候选生成一次就够了，生成多了还要去重；固定一种顺序
									- Fk-1 * Fk-1：只有一个不同，凑成k个，这样也可以保证完备和冗余
								- 候选剪枝：只有F1-F2没有prune，其他都有
								  collapsed:: true
									- 使用第一个的后k-2个和第二个的前k-2个
									- 把需要匹配的候选放到一个数据结构里，只匹配哈希结构里面的某些桶，
							- itemset：{ti1，ti2...tik+1}，如果他们全是频繁的就没有理由把他们排除掉
							- 可以用Fk-1和F1去生成Lk
						- 减少事务数量N
						- 减少要匹配的对MN
				- 利用频繁项目集生成关联规则
				  collapsed:: true
					- 频繁项目集L，任意一个f当成左部，L-f当成右部
					- 一般置信度不再满足反单调，但是也可以满足，比如(ABC->D),(AB->CD),(A->BCD)，分母不变，分子减小
		- 影响apriori的复杂度因素
		  collapsed:: true
			- minsup的阈值
			- 数据维度增加，频繁一项目集变多了
			- 数据库的大小
			- 事务的平均长度：导致一些不频繁的项变成频繁的项
		- 实验结果
		  collapsed:: true
			- 频繁一项目集受制于元素数量，频繁二项目集(一项目集平方一定在)，到3的时候剪枝，之后再组合增加
		- 优化
		  collapsed:: true
			- 支持度计数扫描一遍数据库，跳过
			- 分区  min_sup_count，原本需要多轮扫描，现在扫描两边，把整个数据库分块，D->D1,D2...Dn在每个上面分别跑aporori
			  collapsed:: true
				- 第一遍：在D1上面，|D1|/D * min_sup_count，如果都没出现加在一起也达不到总共的。得到一个候选集
				- 第二遍：扫描候选集真正的结果
			- 近似解，抽样，修改采样概率得到样本数量，小样本上做apriori，原本支持度计数100，现在10%从D里面得到S，min_sup_count=10
		- 压缩存储频繁项目集
		  collapsed:: true
			- 极大频繁项目集Maximal F I：本身是频繁，任何一个直接超集是不频繁的；子集都是频繁，所有组合成集合，
				- 如果不是子集但存在：不是频繁，那么就有一个比他还大的，最后发现有个无穷大的，不成立
				- 只保留了是否频繁，没保留支持度计数
			- 闭项目集Closed F I：它所有的直接超集都不能和他拥有相同的支持度，只能比他小
				- 极大一定是闭
				- 但是闭不一定是极大
				- 闭的所有子集放在一起能恢复频繁项目集
				- 支持度和子集中某个最小的支持度相同，再往下找一定能找到边界，在把他们合并
	- Aporiri Rule
	- FP-growth algorithm：不去生成候选，直接找出频繁项目集，把计算都放到内存里面做
	  collapsed:: true
		- FP树：比整个数据库要小很多，重叠，空间上效率高
		- 从短模式逐步生成长模式：使用条件数据库，eg“abc”是一个频繁模式，给DB|abc，所有包含abc的项目集，找到d，abcd就一定是频繁的；只要是频繁的就能找到，分治
		- 过程：
		  collapsed:: true
			- 降序排序的频繁单项
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403271634424.jpg){:height 576, :width 748}
		- 特例：
		  collapsed:: true
			- 单前缀路径，拆成两部分，单前缀看成一个点，和下面频繁项目集拼接
		- 思路：
		  collapsed:: true
			- 递归生成数据库分片
			- 方法：构造条件模式基，条件FP树，新产生的FP树中找
		- 如果内存不够大：投影数据库，建立自己的FP树
	- 模式评估
	  collapsed:: true
		- 光是用conf的坏处：把有关联的看做没有关联的
		- 想要conf(X->Y)，不妨设conf(X->Y)
	- 翻转不变性：0和1同等重要，01变10是相同的
	- 0加性：和某些东西无关加上不会对结果有影响  里面就没有其中一项，比如f00
	- 行列扩展：采样，代理工作能力可能不一样，但是访问比例应应该影响结果，比如男*2， 女*3，这个指标不应该跟着变
	- Simpson悖论：有些从数据中观察的很有道理，但是实际上不对，一般有隐层变量，数据进行合适的分层，细粒度