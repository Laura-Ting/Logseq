- 线性回归
  collapsed:: true
	- 特点：有监督
	  collapsed:: true
		- 建立回归方程，参数
	- 公式
		- 尽可能多的去拟合：htheta(x) = theta0 + theta1x1 + theta2x2
			- theta0是偏置项，结果微调
		- 矩阵表达：如果添加上x0与theta0组合，所有的x0都是1，htheta(x) = ∑thetaixi = theta^Tx
	- 误差
		- 真实值 = 预测值 + 误差项
		- y(i) = theta^Tx(i) + ε(i)
		- 希望误差项越小越好
		- ε(i)是独立并且具有相同的分布，并且服从均值为0方差为theta^2的高斯分布
			- 独立：每个样本的误差项，之间相互没有影响
			- 同分布：来自同一个模型，因为不同模型的参数不同，评估标准也不一样
			- 高斯分布：有时多，有时少，但结果中体趋于平均，绝大多数情况下不会浮动太大，极少情况下会浮动比较大，符合正常情况
		- 求解theta^T
			- 已知误差项服从高斯分布，但是我们要求的是theta^T
			- 将所有的ε(i)用y(i) - theta^Tx(i)来代替
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202312261606992.png)
		- 似然函数
			- 连乘：因为是独立同分布，联合概率等于边缘概率的乘积
			- 似然函数：期望x(i)和theta的乘积和y(i)越接近越好，概率越大越好
			- 如果项数太多，乘法不好计算，用log转换为加法，会改变极值，但是极值点没有变
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202312261614920.png)
		- 化简
			- 常数全部提出来，不用管
			- J(theta)：目标函数，损失函数，越小越好
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202312261625724.png)
		- 求解
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202312261625112.png)
	- 梯度下降
		- 理解
			- 梯度：下山越快越好
			- 下降：梯度方向正常是上升的，沿着梯度的反方向
			- 更新：每走一步之后梯度需要重新计算
			- 步长：小步长多次迭代，学习率，一般0.01,0.001
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202312261714232.png)
		- 过程
			- 分别求解theta0和theta1，因为两者并不相关
			- 注意点
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202312261656503.png)
			- 公式表示
			  collapsed:: true
				- 目标函数：m个样本的平均
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202312261708925.png)
				- 批量梯度下降，对于第i个样本的第j列，每个j需要单独看，参数更新，但是计算次数多导致速度很慢->随机梯度下降
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202312261708755.png)
				- 随机梯度下降：随机找一个样本，速度虽然快，但是不一定是收敛方向
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202312261711054.png)
				- 小批量梯度下降：选择一小部分样本，实际使用，所以batch越大越精准，但是速度慢，在设备允许的情况下，batch越大越好
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202312261712597.png)