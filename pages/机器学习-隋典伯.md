- SVM
- 前馈神经网络
  collapsed:: true
	- 常见
	  collapsed:: true
		- 前馈
		- 记忆
	- 结构
	- 消息传递
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403140801009.jpg){:height 420, :width 627}
		-
	- 输出单元
	  collapsed:: true
		- 回归-线性和输出单元
		- 二分类-线性+sigmoid，交叉熵损失
		- 多分类-softmax，m维向量概率
	- 通用近似定理
	  collapsed:: true
		- 前馈神经网络可以拟合任何一个连续函数
		- “挤压”
	- 前馈神经网络的学习参数
	  collapsed:: true
		- 准则
			- 交叉熵损失函数：
				- one-hot独热向量：一个向量只有一个是1，其余全是0，eg[0,1,2]->[1,0,0]T[0,1,0]T[0,0,1]T
		- 梯度下降
		- 反向传播
		  collapsed:: true
			- 矩阵微分
			- 链式法则
			  collapsed:: true
				- 标量
				- 向量
					- entry xij
			- 算法
			  collapsed:: true
				- 对w和b求偏导，引入z
				  collapsed:: true
					- ①j->i 只会影响到i
					- ②z对b求偏导
					- ③第l层神经元的误差项，引入zl+1，但是没有联系，引入al
				- 三个数据集：
					- Training set：确定W和b
					- Dev set/Validation set：验证W和b（参数）有多好，确定超参数
					- Test set：不能假定有标签，模型的泛化能力{x}，事后评估
					- random-sample采样，一般8:1:1 or 7:2:1
					-
			- 缺陷：
			  collapsed:: true
				- 过程，结果不可解释
				- 学习速度慢，需要多轮迭代
				- 梯度消失，爆炸
				- 局部搜索
- 卷积神经网络
  collapsed:: true
	- 为什么
	  collapsed:: true
		- 基于前馈神经网络的图像分类：张量就是矩阵的高维表示100 * 100 * 3
		- 感受野Receptive Field：只接受刺激范围内的信号
		- CNN：
		- 特性：局部连接，权重共享，池化
	- 卷积运算
	  collapsed:: true
		- 卷积的定义，f，g
		- 一维卷积：累计延迟；不同的卷积核提取的特征不同
		- 二维卷积：原始图像->feature map（经过卷积）
		  collapsed:: true
			- 高斯：平滑去噪
			- 提取边缘
		- 步长stride，填充padding
		- 转置卷积/微步卷积：低维->高维
		- 空洞卷积dilation：增加输出单元的感受野，卷积核中插入空洞（分散）
		- inchannel和input相关，是输入通道数
		- outchannel是卷积核个数相关，输出通道数
		- 动机：参数非常多，效率很低Ml * Ml-1 变为Ml * k
		- 权重共享：只选取一个卷积核的话，提取特征单调
	- 组成结构
	  collapsed:: true
		- 卷积层：提取一个局部区域的特征
			- 卷积核始终覆盖输入数据的各个切片
			- feature map：经过卷积处理后得到的图
			- 卷积层输出的维度：
				- 无填充：(N-F)/stride+1
				- 加上填充：(N+2P-F)/stride+1 eg:(32+4-5) / 1+1=32，输出维度32 * 32 * 10，参数量5 * 5 *3+1=76 76 * 10=760个参数
		- 池化层：子采样层
			- 最大化池化
			- 平均汇聚
		- 全连接层
	- 特性
		- 平移不变性：卷积+池化共同实现，相同的特征经过平移输出相同的结果
			- 卷积一定能够保持，池化尽可能的保持
		- 浅层学到的特征为简单的边缘、角点、纹理、几何形状；深层学到的特征更深
	- 特征图可视化
		- 每层权重是有数据驱动学习得来的
		- 边缘->纹理->几何
	- 经典架构
	  collapsed:: true
		- LeNet-5手写字符识别，共7层
		  collapsed:: true
			- 32x32->6x28x28con->6x14x14pool16x10x10
		- ImageNet Project
		- AlexNet
		  collapsed:: true
			- 深度卷积网络：GPU，ReLU，Dropout
		- Inception，GoogLeNet
		  collapsed:: true
			- 参数量少
			- 多层小卷积核代替大卷积核以减少计算量和参数量
			- 1x1卷积核：（会考）
				- 升维/降维：不会改变h和w，可以改变输入特征的通道数
				- 增加非线性：比如说后面直接价格ReLU
				- 跨通道信息交互：通道之间信息的线性组合
		- ResNet
		  collapsed:: true
			- 越深网络准确率应该越高，但是实际并不是，存在梯度爆炸，网络
			- 网络应该先能记住数据，才能泛化数据
			- 给非线性的卷积层增加直连边来提高信息的传播效率
			- 之前用f(x, θ)逼近h(x)，h(x)=x+(h(x)-x)，现在用x+f(x, θ)
			- 短路连接
			- 前向传播：输入信号从任意底层直接传播到高层而前馈神经网络逐层传递
			- 反向传播：错误信号合一不经过任何中间权重矩阵变换直接到底层
			- 可视化层面：带短路连接的旺火损失曲面更光滑
		- U-Net：分割segmentation(不同于检测detection)  U型，5层encoder，4层decoder
		  collapsed:: true
			- input feature：572x572x1黑白图像
			- output feature：388x388x2
			- 正卷积->反卷积
		- Ngram：类似于文本的卷积
		- 文本序列的卷积
			- 一般做宽卷积(全部特征而非局部特征)
	- 应用
	  collapsed:: true
		- 翻译
		- AlphaGo
		- 图像信息处理：目标检测，分割，OCR(光学字符识别)
		- 生成汉字
		- 画风迁移
		- 对抗样本
- 序列建模
  collapsed:: true
	- 语言模型概述
	  collapsed:: true
		- 计算一段文字的概率
		- 如何统计：使用句子不太合理；使用句子构成单位的联合概率也有问题，没有考虑顺序
		- p(s)=p(w1)xp(w2|w1)xp(w3|w1w2)x...xp(wm|w1...wm-1)
		- wi可以是字，词，短语，成为统计基元
		- token：词牌 ≈word  My name is dian## ## bo    apple ## ee # mm
		- L基元数，历史情况L^(i-1)
	- N-gram语言模型
	  collapsed:: true
		- n元文法模型：减少历史基元的个数
			- Unigram，Bigram，Trigram...  <BOS> 。。。<EOS>
			- 最大似然估计：这个概率/和其他？任何一个组合的概率的加和
			- 缺陷：
				- 数据匮乏
					- n太短
					- 问题：分子为0；分子分母为0
				- 参数随着模型增加而增加
				- 没有考虑词和词之间的相关性
	- 神经语言模型：根据历史单词对下一时刻词进行预测
	  collapsed:: true
		- 前馈神经网络语言模型
			- 沿用马尔可夫假设，能够很好的捕捉词与词之间的相关性
		- RNN
		- LSTM
- 模型xx
  collapsed:: true
	- 集成学习
		- Bagging
	- 自训练：标注数据训练一个模型+未标注数据预测伪标签，加入数据集
	- 深度训练
	- 多任务学习：利用多个任务的相关性改进每个模型，迁移归纳学习
	  collapsed:: true
		- 归纳偏置(归纳偏好)：根据人为偏好将某种方案优先与其他；先前已经基于模型
		  collapsed:: true
			- 深度学习中：
			  collapsed:: true
				- CNN：假设特征具有局部性，当我们把相邻的特征放在一起更容易得到解
				- RNN：假设每一时刻的计算依赖于历史计算结果
				- word2vec：基于分布式假设，一个词的意思是由经常出现在附近的词给出的
		- 推理：
		  collapsed:: true
			- 归纳induction(个别推普遍)，问题是黑天鹅事件只需要一个特例就可以打破认知
			- 演绎deduction(普遍推个别)，大前提，小前提，结论
		- 特点：
		  collapsed:: true
			- 通过包含在相关任务中的信息作为归纳偏置
		- 共享机制：深度学习很方便共享
		  collapsed:: true
			- 硬共享：不同任务的神经网络共享一些底层模块提取一些通用特征，在针对不同任务设置一些高层私有模块提取特定特征
			- 软共享：不现实的设置共享模块，但每个人物都可以从其他任务中窃取信息提高自己的能力，音状态的复制or
			- 层次共享：不同层网络抽取的特征不一样，底层一般低级局部特征，高层高级抽象语义信息，那么低级任务从低层输出，高级任务从高层输出
			- 共享-私有模式：更加分工明确，共享模块负责跨任务的共享特征，私有特定局部
				- 怎么判断共享的学习到了共享的：对抗训练，判别器，保证共享部分没有偏差
	- 迁移学习：
	  collapsed:: true
		- 比较像pre-training，在其他地方学到的知识用于这个
		- 元学习：根据任务集合得到元学习算法，给定新任务用学习算法生成模型
		  collapsed:: true
			- MAML：模型无关的元学习
			  collapsed:: true
				- 需要一大堆任务（构造：）
				- 参数随机初始化
				- 循环：
					- 散布一些task bsz=3
					- 对全部Ti计算梯度，theta'计算，有了thetai
					- 对θ做一些copyθ1,θ2,θ3...
				- 不再评估θi，而是fθi'
		- 小样本学习：小样本上的快速学习能力
	- 终身学习：
		- 灾难性遗忘：fθ做一个猫狗判别，再做猫和狮子就忘记了猫狗
- PyTorch
  collapsed:: true
	- Tensor：张量，和数组很像，但是支持GPU加速
		- torch.LingTensor([2])
		- torch.BoolTensor([0])
	- Autograd：自动求导，根据输入和前向传播过程自动计算
		- torch.Tensor  .requires_grad设置为True
		- 用户自己创建的grad_fn是None，而y计算结果的不是None
		- .detach() 阻止一直被跟踪
		- 结果保存在.grad属性中
	- nn.Module
		- 可以表示某个层，也可以包含很多层，最常见的是继承两层人后边写自己的
		- 自动检测parameter将其作为可学习参数(权重,bias,BN中的r,B)，递归查找子Module的parameter
		  id:: 661494b9-39c5-4393-ae5b-136d82fe62ec
		  collapsed:: true
			- 学习参数(权重,bias,BN中的r,B)
			- 不可学习参数
		- 实现一个自定义
		  collapsed:: true
			- 继承Module类，实现自定义init函数和forward函数
			- init
				- 设置Linear层的权重和bias
				- Conv2设置in_channels, out_channels,kernal_size,stride,padding
			- forward
				- torch.nn.functional实现，或自定义方式nn.Parameter权重
	- 优化器 torch.optim
	  collapsed:: true
		- 所有方法都要继承基类optim.Optimizer
		- 调整学习率：
			- 修改optimizer.param_groups
	- nn.funtional
	  collapsed:: true
		- 都是函数，小写
		- output1=model(input_1)就相当于model.forward
		- model.bias
		- b = nn.functional.relu(input1)和b2 = nn.ReLU()(input1)是一样的
		- tips
		  collapsed:: true
			- 如果模型有可学习参数是，建议使用nn.Module
			- 激活函数中没有可学习参数，可以使用对应的functional，eg激活，池化
			- dropout建议使用nn.Dropout
	- 数据处理
		- dataloader，dataset
		- 需要实现getitem()，len()
		- num_works必须要设成0，不使用多进程，debug
		- collate_fn，一个batch加速
	- 使用GPU加速
		- Tensor
		- nn.Modules
		- .cuda转换成GPU，tensor.cuda(0)，如果使用第二块GPU，手动指定tensor.cuda(1)
		- 或者改变环境变量：export CUDA_VISIBLE_DEVICE=1
		- load和save可能有的在gpu上，有的在cpu上