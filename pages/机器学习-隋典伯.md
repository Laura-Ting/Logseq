- SVM
- 前馈神经网络
  collapsed:: true
	- 常见
	  collapsed:: true
		- 前馈
		- 记忆
	- 结构
	- 消息传递
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403140801009.jpg){:height 420, :width 627}
		-
	- 输出单元
	  collapsed:: true
		- 回归-线性和输出单元
		- 二分类-线性+sigmoid，交叉熵损失
		- 多分类-softmax，m维向量概率
	- 通用近似定理
	  collapsed:: true
		- 前馈神经网络可以拟合任何一个连续函数
		- “挤压”
	- 前馈神经网络的学习参数
	  collapsed:: true
		- 准则
			- 交叉熵损失函数：
				- one-hot独热向量：一个向量只有一个是1，其余全是0，eg[0,1,2]->[1,0,0]T[0,1,0]T[0,0,1]T
		- 梯度下降
		- 反向传播
		  collapsed:: true
			- 矩阵微分
			- 链式法则
			  collapsed:: true
				- 标量
				- 向量
					- entry xij
			- 算法
			  collapsed:: true
				- 对w和b求偏导，引入z
				  collapsed:: true
					- ①j->i 只会影响到i
					- ②z对b求偏导
					- ③第l层神经元的误差项，引入zl+1，但是没有联系，引入al
				- 三个数据集：
					- Training set：确定W和b
					- Dev set/Validation set：验证W和b（参数）有多好，确定超参数
					- Test set：不能假定有标签，模型的泛化能力{x}，事后评估
					- random-sample采样，一般8:1:1 or 7:2:1
					-
			- 缺陷：
			  collapsed:: true
				- 过程，结果不可解释
				- 学习速度慢，需要多轮迭代
				- 梯度消失，爆炸
				- 局部搜索
- 卷积神经网络
  collapsed:: true
	- 为什么
	  collapsed:: true
		- 基于前馈神经网络的图像分类：张量就是矩阵的高维表示100 * 100 * 3
		- 感受野Receptive Field：只接受刺激范围内的信号
		- CNN：
		- 特性：局部连接，权重共享，池化
	- 卷积运算
	  collapsed:: true
		- 卷积的定义，f，g
		- 一维卷积：累计延迟；不同的卷积核提取的特征不同
		- 二维卷积：原始图像->feature map（经过卷积）
		  collapsed:: true
			- 高斯：平滑去噪
			- 提取边缘
		- 步长stride，填充padding
		- 转置卷积/微步卷积：低维->高维
		- 空洞卷积dilation：增加输出单元的感受野，卷积核中插入空洞（分散）
		- inchannel和input相关，是输入通道数
		- outchannel是卷积核个数相关，输出通道数
		- 动机：参数非常多，效率很低Ml * Ml-1 变为Ml * k
		- 权重共享：只选取一个卷积核的话，提取特征单调
	- 组成结构
	  collapsed:: true
		- 卷积层：提取一个局部区域的特征
			- 卷积核始终覆盖输入数据的各个切片
			- feature map：经过卷积处理后得到的图
			- 卷积层输出的维度：
				- 无填充：(N-F)/stride+1
				- 加上填充：(N+2P-F)/stride+1 eg:(32+4-5) / 1+1=32，输出维度32 * 32 * 10，参数量5 * 5 *3+1=76 76 * 10=760个参数
		- 池化层：子采样层
			- 最大化池化
			- 平均汇聚
		- 全连接层
	- 特性
		- 平移不变性：卷积+池化共同实现，相同的特征经过平移输出相同的结果
			- 卷积一定能够保持，池化尽可能的保持
		- 浅层学到的特征为简单的边缘、角点、纹理、几何形状；深层学到的特征更深
	- 特征图可视化
		- 每层权重是有数据驱动学习得来的
		- 边缘->纹理->几何
	- 经典架构
	  collapsed:: true
		- LeNet-5手写字符识别，共7层
		  collapsed:: true
			- 32x32->6x28x28con->6x14x14pool16x10x10
		- ImageNet Project
		- AlexNet
		  collapsed:: true
			- 深度卷积网络：GPU，ReLU，Dropout
		- Inception，GoogLeNet
		  collapsed:: true
			- 参数量少
			- 多层小卷积核代替大卷积核以减少计算量和参数量
			- 1x1卷积核：（会考）
				- 升维/降维：不会改变h和w，可以改变输入特征的通道数
				- 增加非线性：比如说后面直接价格ReLU
				- 跨通道信息交互：通道之间信息的线性组合
		- ResNet
		  collapsed:: true
			- 越深网络准确率应该越高，但是实际并不是，存在梯度爆炸，网络
			- 网络应该先能记住数据，才能泛化数据
			- 给非线性的卷积层增加直连边来提高信息的传播效率
			- 之前用f(x, θ)逼近h(x)，h(x)=x+(h(x)-x)，现在用x+f(x, θ)
			- 短路连接
			- 前向传播：输入信号从任意底层直接传播到高层而前馈神经网络逐层传递
			- 反向传播：错误信号合一不经过任何中间权重矩阵变换直接到底层
			- 可视化层面：带短路连接的旺火损失曲面更光滑
		- U-Net：分割segmentation(不同于检测detection)  U型，5层encoder，4层decoder
		  collapsed:: true
			- input feature：572x572x1黑白图像
			- output feature：388x388x2
			- 正卷积->反卷积
		- Ngram：类似于文本的卷积
		- 文本序列的卷积
			- 一般做宽卷积(全部特征而非局部特征)
	- 应用
	  collapsed:: true
		- 翻译
		- AlphaGo
		- 图像信息处理：目标检测，分割，OCR(光学字符识别)
		- 生成汉字
		- 画风迁移
		- 对抗样本
- 序列建模
  collapsed:: true
	- 语言模型概述
	  collapsed:: true
		- 计算一段文字的概率
		- 如何统计：使用句子不太合理；使用句子构成单位的联合概率也有问题，没有考虑顺序
		- p(s)=p(w1)xp(w2|w1)xp(w3|w1w2)x...xp(wm|w1...wm-1)
		- wi可以是字，词，短语，成为统计基元
		- token：词牌 ≈word  My name is dian## ## bo    apple ## ee # mm
		- L基元数，历史情况L^(i-1)
	- N-gram语言模型
	  collapsed:: true
		- n元文法模型：减少历史基元的个数
			- Unigram，Bigram，Trigram...  <BOS> 。。。<EOS>
			- 最大似然估计：这个概率/和其他？任何一个组合的概率的加和
			- 缺陷：
				- 数据匮乏
					- n太短
					- 问题：分子为0；分子分母为0
				- 参数随着模型增加而增加
				- 没有考虑词和词之间的相关性
	- 神经语言模型：根据历史单词对下一时刻词进行预测
	  collapsed:: true
		- 前馈神经网络语言模型
			- 沿用马尔可夫假设，能够很好的捕捉词与词之间的相关性
		- RNN
		- LSTM