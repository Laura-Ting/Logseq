- 神经网络
  collapsed:: true
	- 神经元模型（感知机）
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041032222.png)
		- 常见激活函数
			- 最常用效果：ReLU>Tanh?Sigmoid
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041033391.png)
	- 通过神经元升维，增加线性转换能力
	- 通过多个隐藏层，增加非线性转换次数
	- 过于复杂的设计会带来效率低下->过拟合
		- 卷积神经网络CNN
		- 循环神经网络RNN
- 卷积神经网络CNN
  collapsed:: true
	- 卷积（convolution）=卷（vol）+积（con）
	  collapsed:: true
		- 数学形式：f(t)*g(t) = ∫f(τ)g(t-τ)dτ
		  collapsed:: true
			- 极简的数学形式描述了一个漂亮的数学过程，用于两种运动的动态融合，类比火车进入山洞的过程，f是山洞，g是火车
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041050150.png)
			- g(-τ)：火车先旋转位置以和山洞相对
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041053291.png)
			- g(t-τ)：配上全局时间t，火车就可以运动起来了
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041052725.png)
			- f(τ)g(t-τ)：相乘，火车进入山洞后每一时刻t两者相对位置
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041054376.png)
			- 积分：两者不断重叠，相互作用的过程
			  collapsed:: true
				- ![](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041054376.png)
			- 实际形式
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041056293.png)
			- 注意
			  collapsed:: true
				- 可以是火车进山洞，也可以是山洞套火车->卷积交换性质
				- 卷积函数不仅可以是连续曲线，也可以是离散形式，本质上是一样的（选取采样点）
		- 应用
		  collapsed:: true
			- 处理图像
				- 二维离散卷积
				  collapsed:: true
					- 卷积核
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041100981.png)
				- 添加边界
				  collapsed:: true
					- 一般补一圈0来保证输入输出维度一致
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041101443.png)
			- 处理音频，NLP
				- 一般卷积核是一维
	- 卷积+神经网络
	  collapsed:: true
		- 模型图
		  collapsed:: true
			- 特点：卷积+池化层层连接
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041113750.png)
		- 传统神经网络
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041115862.png)
			- 构成全连接网络层：每个节点都和上一层的全部节点相连，但这样冗余参数太多，参数众多，难以训练，产生过拟合
		- 卷积
		  collapsed:: true
			- 使用卷积改进传统神经网络，小小的卷积核可以起到强大的特征提取作用
			  collapsed:: true
				- 将全连接变成二维局部连接，卷积核具备空间平移不变特性
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041119956.png)
				- 如果使用3*3卷积核，那么只需要9个参数
			- 特征图：feature map，卷积后得到的结果
			  collapsed:: true
				- 特征图上的每一个点都是由上一张图上若干点决定的
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041121266.png)
				- 上一层这9个点成为感受野
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041122086.png)
				- 网络层数越多，每一点的感受野就越大，就能够捕捉更大尺寸的特征，对复杂特征的表达能力就越强
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041123771.png)
			- 多层卷积神经网络
			  collapsed:: true
				- 第一层：表示特定的位置和角度，是否出现边缘
				- 第二层：将这些边缘组合成有趣的模式，比如花纹
				- 第三层：上一层的花纹能够汇合成物体部位特征的模式
				- ...逐级表示下去就能学到图像的各种特征
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041126764.png)
			- 卷积过后加入激活函数
				- 最常用：ReLU，线性整流单元，就是把特征图上的所有负数变成0
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041128604.png)
		- 池化 pooling
		  collapsed:: true
			- 抓住主要矛盾，忽略次要因素
			- 把局部多个神经元的输出组合成下层单个神经元来减少数据维度
			- 局部池化一般选取2*2，常用最大池化，平均池化
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041132711.png)
		- 全连接层
		  collapsed:: true
			- 最后一层：把相邻两层神经元全部交叉相连，与传统神经网络相同
			- 从全局出发，做最终结论，与前面的卷积层形成“先局部，后整体”
			- 卷积加池化多层级联，对输入数据进行多尺度特征提取和深度学习
	- 发展历程
	  collapsed:: true
		- 1980：层级化人工神经网络，神经认知模型处理手写字符识别问题，卷积神经网络的前身
		- 1998 LeNet：基于梯度学习的卷积神经网络算法，美国邮政系统手写数字，字符识别
		- 2012 AlexNet：结构和LeNet相似，但更深更大，使用层叠的卷积层获取特征，识别率超高
		- 之后：
		  collapsed:: true
			- ZFNet：可视化展示卷积神经网络各层功能和作用
			- 牛津大学VGGNet：堆积的小卷积核代替大卷积核，增加决策函数的判别性，减少参数量
			- GoogleNet：增加卷积神经网络的宽度，1*1卷积降维，减少参数量，多个不同尺度的卷积核上卷积后再聚合
			- ResNet：残差网络，解决网络模型的退化问题，使得神经网络可以更深
	- 改进方向
	  collapsed:: true
		- 输入形式，卷积核，级联和初始化方法都没有严格要求，很大改进空间
		  collapsed:: true
			- 显著性检测：先把图像进行超像素分割，分割后的超像素作为新的网络输入
		- 传统CNN特征提取时好比用筛子筛谷子，中间筛出去的东西也可以好好利用
		  collapsed:: true
			- 人脸识别
		- 传统CNN就是卷积核，我们可以用其他类型的矩阵，比如Gabor滤波器，小波核等
		- 训练算法上，例如对非线性激活函数改进：Sigmoid->双曲正切->ReLU
- 循环神经网络RNN
  collapsed:: true
	- RNN
	  collapsed:: true
		- 关注隐层每个神经元在时间维度上不断地成长和进步
		- 表示隐藏层在不同时刻的状态
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041319637.png)
		- 隐层的连接既可以是全连接（所有线都相连），也可以是自己对自己
		  collapsed:: true
			- 全连接
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041320223.png)
			- 自己对自己
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041322635.png)
		- 用Ws表示层级间的权重矩阵（假定不同的层就是不同的时刻），所有的层共享同一个Ws，有效减少训练参数
		- RNN输出：St = f(Win·X + Ws·St-1 + b)
		  collapsed:: true
			- 建立了隐层t-1时刻和t时刻的关联，让神经网络有了某种记忆的能力
		- 应用
		  collapsed:: true
			- 对于前后无关的图片，可能用处不大
			- 但是语言类的问题，上下文语境就很重要，NLP中巨大应用
		- 看似级联，但实际上是沿着时序反复迭代
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041330080.png)
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041331751.png)
			-
		- 使用
		  collapsed:: true
			- 和传统神经网络一样，也使用误差反向传播+梯度下降，只不过在计算隐层时要引入之前不同时刻的数据
			- 但是这种依赖一般超过10步就不行了->长短期记忆网络LSTM(long short-term memory)
		- 输入输出关系更加灵活
		  collapsed:: true
			- 一对多：输入是一张图片，输出是一段话or音乐
			- 多对一：输入是一段话，输出是单个的，可以是情感正负向判断，或一张图片
			- 多对多（n to n）：输入和输出等长，生成文章、诗歌，甚至代码
			- 多对多（n to m）：输入和输出不等长，Encoder-Decoder模型/ Seq2Seq模型
				- 将输入数据编码成一个上下文向量，通过他输出预测的序列，应用于机器翻译、文本摘要、阅读理解、对话生成等
	- LSTM
	  collapsed:: true
		- 之前的RNN属于短期记忆
		- 增加一条长期记忆链C，之前的S是短期记忆链，两者相互更新
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041346484.png)
		- 更新过程
		  collapsed:: true
			- 计算St时，除了输入Xt和前一时刻St-1，还要包含当前时刻的日记信息Ct
			- 删除旧日记
			  collapsed:: true
				- f1类似于橡皮擦，根据昨天的记忆St-1和输入Xt，决定要修改日记中哪些记录
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041351127.png)
				- 选择性遗忘部分记忆，因此也被称为forget gate，过滤重要特征，忽略无关信息
			- 增添新日记
			  collapsed:: true
				- f2就像一个铅笔，同样是根据昨天的记忆St-1和输入Xt，决定增加日记中哪些记录
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041353571.png){:height 600, :width 688}
				- tanh相当于对这两天发生的事情进行梳理和归纳
				- f2也被称为input gate
			- Ct：先相乘再相加
			  collapsed:: true
				- Ct除了向下传递，还被用来更新短期记忆St
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041355326.png)
- Attention机制
  collapsed:: true
	- Attention就是权重
	- 核心思想：通过加权求和解决context的理解，本质上就是在不同的上下文下专注不同的信息
	- 两个RNN组合形成Decoder-Encoder
	  collapsed:: true
		- 先对一句话编码，再解码就能实现机器翻译
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041409888.png)
		- 但是这种情况不管输入多长都统一压缩成相同长度编码c，导致翻译精度下降
	- Attention机制
	  collapsed:: true
		- 通过每个时间输入不同的c来解决上述问题
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041417717.png)
		- αti标明了在t时刻所有输入的权重，以ct的视角看过去就是不同输入的注意力，因此也被称为Attention分布
		  collapsed:: true
			- ![image.png](../assets/image_1704349216970_0.png)
		- 可以通过神经网络数据训练得到最好的attention权重矩阵
		- Attention机制的引入，打破了只能利用Encoder形成单一向量的限制，让每一时刻模型都能动态的看到全局信息，将注意力集中到对当前单词翻译最重要的信息上
	- self-attention
	  collapsed:: true
		- 发现RNN的顺序结构很不方便，难以并行运算，效率太低
		- 既然Attention模型本身就已经对全部输入打了分，RNN中的顺序实际上没什么用，干脆简化调->self-attention
		- 去掉了输入的箭头，Encoder编码阶段利用attention机制计算每个单词与其他所有单词之间的关联（相关联的词有更高的attention score）
		- 利用这些权重加权表示，在放到一个前馈神经网络中得到新的表示，就很好的嵌入了上下文信息，这样的步骤重复几次效果会更好
		  collapsed:: true
			- ![image.png](../assets/image_1704349771922_0.png)
		- Decoder时也是类似，不仅要看之前产生的输出，还要看Encoder得到的输出
	- 应用
	  collapsed:: true
		- 谷歌Transformer
		- GPT
		- Bert
	- 优点
	  collapsed:: true
		- 参数更少
		- 速度更快
		- 效果更好
- Transformer
  collapsed:: true
	- 编码与解码
	  collapsed:: true
		- 每个Encoder和Decoder都是串联的组合
		  collapsed:: true
			- 经常见到的6层结构，理解为一次完整的变形都需要2*6=12步操作
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041441008.png)
		- Encoder
		  collapsed:: true
			- 每个Encoder包含self-attention和前馈网络
			- 回顾attention：本质上就是通过加权求和获得对上下文的全局感知
			- self-attention就是变形金刚的拆解对照表，计算各个零部件的权重，标明互相间的关系
				- 每个self-attention会分解为多个部分：multi-head attention
			- 前馈网络：根据这些权重变一次形状
		- Decoder
		  collapsed:: true
			- 除了self-attention和前馈网络，还多了一层encoder-decoder attention
			  id:: 65965a47-5047-4b62-9d04-bd2ed73cc723
			- encoder-decoder attention作用：在组装时不光只考虑自己，还要兼顾拆解时的整体信息
				- 比如机器翻译：解码时每个词不光要看已经翻译的内容，还要考虑encoder中上下文的信息
			-
	- 数据的流动
	  collapsed:: true
		- 算法将单词向量化，嵌入位置信息，变成统一长度
		- encoder将其作为输入，通过self-attention和前馈网络发送到下一个编码器
		  collapsed:: true
			- self-attention就相当于一个零件自查表，通过权重标明相互关系，嵌入上下文信息
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041526287.png)
			- 每个输入向量先嵌入位置信息
			- 然后分别乘以3个训练好的向量Query，Key，Value
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041527045.png)
			- 再用每个单词的Q向量和所有单词的Key向量相乘，得到的权重就是attention
			- 然后归一化，用softmax函数过滤掉不相干的单词，乘以V向量后加权求和，得到输出向量Z
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041527161.png)
		- 矩阵表示
		  collapsed:: true
			- 总过程
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041537311.png)
			- 先得到QKV三个矩阵
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041529404.png)
			- 再计算self-attention，和X相比，Z的维度没有变，只是其中掺入了其他单词的上下文信息
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041532961.png)
			- 在multi-head attention中，使用了8个不同的权重矩阵，主要目的是为了消除QKV初始值的影响
	- 各个模块协同工作
		- 编码器：处理输入序列，将输出转化为attention向量K和V（零件拆解说明书）
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041543196.png)
		- 解码器：一边看自己的拆解说明书，一边考虑拆分时与其他零件的相互关系，每步组装输出一个零件，重复这个步骤就完成了变形
			- 解码器的输出是个向量->变为单词：
				- 线性层：简单的全连接网络，将解码器输出投影成一个一维向量，这个向量的维度很长，包含了所有可能出现的单词总和
				- softmax层：进一步归一化，其中概率最高的单词就是最后的输出
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202401041546906.png)
- 图神经网络GNN
	-