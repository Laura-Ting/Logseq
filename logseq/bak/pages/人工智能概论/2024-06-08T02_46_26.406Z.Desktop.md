- ![1-第1章 绪论（导论5）.pdf](../assets/1-第1章_绪论（导论5）_1717297539102_0.pdf)
- 一，绪论
  collapsed:: true
	- 人工智能的基本概念
	  collapsed:: true
		- 智能的概念
		- 智能的特征
		- 人工智能
	- 人工智能的发展简史
	  collapsed:: true
		- 孕育
		- 形成
		- 发展
		- 大数据驱动发展期
	- 人工智能研究的基本内容
	  collapsed:: true
		- 知识表示
		- 机器感知
		- 机器思维
		- 机器学习
		- 机器行为
	- 人工智能的主要研究领域
	  collapsed:: true
		- 自动定理证明
		- 博弈
			- 棋类游戏的计算复杂性
			- 国际象棋比赛
			- 围棋比赛
		- 模式识别
		- 机器视觉
		- 智慧医疗
		- 自然语言理解
		- 机器听觉
		- 机器翻译
		- 智能信息检索
		- 数据挖掘与知识发现
		- 专家系统
		- 自动程序设计
		- 机器人
		- 组合优化问题
		- 智慧物流
		- 人工神经网络
		- 分布式人工智能与多智能体
		- 智能控制
		- 智能仿真
		- 智能CAD
		- 智能CAI
		- 智能管理与智能决策
		- 智能多媒体系统
		- 智能操作系统
		- 智能计算机系统
		- 智能通信
		- 智能网络系统
		- 人工生命
- ![2-第2章 知识表示与知识图谱（导论5）.pdf](../assets/2-第2章_知识表示与知识图谱（导论5）_1717297552405_0.pdf)
- 二，知识表示与知识图谱
  collapsed:: true
	- 知识与知识表示的概念
	  collapsed:: true
		- 知识的概念
		  collapsed:: true
			- 定义：把有关信息关联在一起所形成的信息结构，反应不同关系
			- 分为事实和规则
		- 知识的特性
		  collapsed:: true
			- 相对正确性：在一定情况和条件下知识才是正确的
			- 不确定性：可能由随机性，模糊性，经验，不完全性引起
			- 可表示性和可利用性
		- 知识的表示
		  collapsed:: true
			- 将人类知识形式化，模型化
			- 选择表示方法的原则
				- 充分表示领域知识
				- 有利于对知识的利用
				- 便于对知识组织，维护与管理
				- 便于理解与实现
	- 一阶逻辑谓词表示法
	  collapsed:: true
		- 命题
		  collapsed:: true
			- 真命题，假命题
			- 命题逻辑：研究命题及命题之间关系的符号逻辑系统
			- 命题逻辑表示方法：无法反映事物的结构，无法表述不同事物间的共同特征
		- 谓词：P(x1, x2, x3..., xn)
		  collapsed:: true
			- 个体x1...xn，谓词名P
			- 个体可以是常量，变元，函数，谓词
		- 谓词公式
		  collapsed:: true
			- 连接词：否定/非，析取/或，合取/与，蕴含/条件，等价/双条件
			- 量词：全称量词(任意x)，存在量词(存在x)
			- 谓词公式
			  collapsed:: true
				- 单个谓词是谓词公式，称为原子谓词公式
				- 若A是谓词公式，﹁A，A∧B，A∨B，A→B， A⟷B，任意A，存在A也都是谓词公式
				- 连接词的优先级别从高到低排列：﹁， ∧， ∨， →， <->
			- 量词的辖域
			  collapsed:: true
				- 量词的辖域：这个量词所管辖的地方，比如(存在x)(P(x, y) → Q (x, y))∨R(x, y)中(P(x, y) → Q (x, y))是(存在x)的辖域
				- 约束变元与自由变元：辖域内与量词中同名的变元称为约束变元，不同名的变元称为自由变元，比如(P(x, y) → Q (x, y))中的x是约束变元，R(x, y)是自由变元
		- 谓词公式的性质
		  collapsed:: true
			- 谓词公式的解释
			  collapsed:: true
				- 谓词公式在个体域上的解释，比如Friends (george, x)，可以有Friends (george, susie)=T，Friends (george, kate)=F的解释
				- 对于每一个解释，谓词公式都可求出一个真值（T或F）
			- 谓词公式的永真性，可满足性，不可满足性
			  collapsed:: true
				- 永真性：谓词公式P在所有D上都取得真值T
				- 永假性：谓词公式P在所有D上都取得真值F
				- 可满足性：如果至少存在一个解释使得P在此解释下的真值为T，则称P是可满足的，否则，则称P是不可满足的
			- 谓词公式的等价性
			  collapsed:: true
				- P和Q对于共同的个体域D上任意解释都有相同的真值，P<=>Q
			- 谓词公式的永真蕴含
			  collapsed:: true
				- 如果P→Q永真，则称公式P永真蕴含Q，且称Q为P的逻辑结论，称P为Q的前提，记为P=>Q
			- 谓词逻辑的其他推理规则
			  collapsed:: true
				- P规则：前提引入
				- T规则：使用永真蕴含公式S，推理规则
				- CP规则：如果命题R和前提能推出S，则使用R->S，附加前提引入
				- 反证法：P=>Q，当且仅当P∧﹁Q<=>F（P∧﹁Q不可满足）
		- 一阶逻辑知识表示方法
		  collapsed:: true
			- 谓词公式表示知识的步骤
			  collapsed:: true
				- 定义谓词及个体
				- 变元赋值
				- 用连接词连接各个谓词，形成谓词公式
		- 一阶逻辑表示法的特点
		  collapsed:: true
			- 优点：自然性，精确性，严密性，容易实现
			- 局限性：不能表示不确定的知识，组合爆炸，效率低
			- 应用：自动问答系统，机器人行动规划系统，机器博弈系统，问题求解系统
	- 产生式表示法
	  collapsed:: true
		- 产生式
		  collapsed:: true
			- 适合：表示事实性知识和规则性知识
			- 确定性规则知识的产生式表示
			  collapsed:: true
				- IF P THEN Q，或者P->Q
			- 不确定性规则知识的产生式表示
			  collapsed:: true
				- IF P THEN Q （置信度），或者P->Q（置信度）
			- 确定性事实性知识的产生式表示
			  collapsed:: true
				- 三元组表示（对象，属性，值）
			- 不确定性事实性知识的产生式表示
			  collapsed:: true
				- 四元组表示（对象，属性，值，置信度）
			- 产生式与谓词逻辑中的蕴含式的区别
			  collapsed:: true
				- 除逻辑蕴含外，产生式还包括各种操作、规则、变换、算子、函数等
				- 蕴含式只能表示精确知识，产生式还可以表示不精确知识
			- 产生式的形式描述及语义：
			  collapsed:: true
				- 符号表示：“::=”表示“定义为”；符号“|”表示“或者是”；符号“[ ]”表示“可缺省”
				- BNF
				  collapsed:: true
					- <产生式>::=<前提>-><结论>
					- <前 提>::=<简单条件>|<复合条件>
					- <结 论>::=<事实>|<操作>
					- <复合条件>::=<简单条件>AND<简单条件>[AND<简单条件>…|<简单条件>OR<简单条件>[OR<简单条件>…
					- <操 作>::=<操作名>[(<变元>，…)]
		- 产生式系统
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406030903629.png){:height 199, :width 447}
			- 规则库: 用于描述相应领域内知识的产生式集合
			- 综合数据库(事实库、上下文、黑板等)：一个用于存放问题求解过程中各种当前信息的数据结构
			- 控制系统（推理机构）：由一组程序组成，负责整个产生式系统的运行，实现对问题的求解
			  collapsed:: true
				- 从规则库中选择与综合数据库中的已知事实进行匹配
				- 匹配成功的规则可能不止一条，进行冲突消解
				- 执行某一规则时，如果其右部是一个或多个结论，则把这些结论加入到综合数据库中：如果其右部是一个或多个操作，则执行这些操作
				- 对于不确定性知识，在执行每一条规则时还要按一定的算法计算结论的不确定性
				- 检查综合数据库中是否包含了最终结论，决定是否停止系统的运行
		- 产生式系统——动物识别系统
		- 产生式表示法的特点
		  collapsed:: true
			- 优点：自然性，模块性，有效性，清晰性
			- 缺点：效率不高，不能表达结构性知识
		- 适合产生式表示的知识
		  collapsed:: true
			- 领域知识间关系不密切，不存在结构关系
			- 经验性及不确定性的知识，且相关领域中对这些知识没有严格、统一的理论
			- 领域问题的求解过程可被表示为一系列相对独立的操作，且每个操作可被表示为一条或多条产生式规则
	- 框架表示法
	  collapsed:: true
		- 概要：一种结构化的知识表示方法
		- 框架的一般结构
		  collapsed:: true
			- 框架：一种描述所论对象（一个事物、事件或概念）属性的数据结构
			  collapsed:: true
				- 一个框架由若干个被称为“槽”（slot）的结构组成，每一个槽又可根据实际情况划分为若干个“侧面”（faced）
				- 槽相当于属性，侧面相当于属性的一个方面
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406030913773.png){:height 233, :width 278}
			- 事例框架：把具体的信息填入槽或侧面后
		- 特点：结构性，继承性，自然性
	- 知识图谱
	  collapsed:: true
		- 概要：知识图谱是一种互联网环境下的知识表示方法，2012年谷歌提出
		- 定义：用各种不同的图形等可视化技术描述知识资源及其载体，挖掘、分析、构建、绘制和显示知识及它们之间的相互联系
		  collapsed:: true
			- 由一些相互连接的实体及其属性构成
			- 三元组：(实体1-关系-实体2)，(实体-属性-属性值)
		- 表示：图，节点表示实体/概念，边表示关系/属性
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406030923978.png){:height 250, :width 367}
		- 架构：
		  collapsed:: true
			- 逻辑结构：模式层和数据层
				- 数据层主要是由一系列的事实组成，而知识以事实为单位进行存储
				- 模式层构建在数据层之上，是知识图谱的核心
			- 体系架构
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406030921858.png){:height 186, :width 443}
			- 资源对象：结构化数据，半结构化数据，非结构化数据
		- 构建：
		  collapsed:: true
			- 自顶向下：先为知识图谱定义好本体与数据模式，再将实体加入到知识库
			- 自底向上：从一些开放链接数据中提取出实体，选择其中置信度较高的加入到知识库，再构建顶层的本体模式
		- 典型应用：维基百科，DBpedia，YAGO，XLORE
- ![3-第3章 确定性推理方法（导论5）.pdf](../assets/3-第3章_确定性推理方法（导论5）_1717297562681_0.pdf)
- 三，确定性推理方法
  collapsed:: true
	- 推理的基本概念
	  collapsed:: true
		- 推理的定义：根据数据库中的已知事实(证据)和知识库中的知识，基于某种策略得出结论
		- 推理方式及其分类
		  collapsed:: true
			- 演绎推理、归纳推理、默认推理
			  collapsed:: true
				- 演绎推理：三段论式（大前提，小前提，结论），一般 → 个别
				- 归纳推理：个别 → 一般，分为完全归纳推理（必然性推理），不完全归纳推理（非必然性推理）
				- 默认推理/缺省推理：知识不完全的情况下假设某些条件已经具备所进行的推理
			- 确定性推理、不确定性推理
			  collapsed:: true
				- 确定性推理：推理时所用的知识与证据都是确定的，推出的结论也是确定的，其真值或者为真或者为假
				- 不确定性推理：推理时所用的知识与证据不都是确定的，推出的结论也是不确定的
				  collapsed:: true
					- 似然推理（概率论）
					- 近似推理或模糊推理（模糊逻辑）
			- 单调推理、非单调推理
			  collapsed:: true
				- 单调推理：随着推理向前推进及新知识的加入，推出的结论越来越接近最终目标
				- 非单调推理：由于新知识的加入，不仅没有加强已推出的结论，反而要否定它，使推理退回到前面的某一步，重新开始
				- 默认推理是非单调推理
			- 启发式推理、非启发式推理
			  collapsed:: true
				- 启发性知识：与问题有关且能加快推理过程、提高搜索效率的知识
		- 推理的方向
		  collapsed:: true
			- 正向推理（事实驱动推理）: 已知事实 → 结论
			  collapsed:: true
				- 基本思想
				  collapsed:: true
					- 从初始已知事实出发，在知识库KB中找出当前可适用的知识，构成可适用知识集KS
					- 按某种冲突消解策略从KS中选出一条知识进行推理，并将推出的新事实加入到数据库DB中作为下一步推理的已知事实，再在KB中选取可适用知识构成KS
					- 重复（2），直到求得问题的解或KB中再无可适用的知识
				- 实现正向推理需要解决的问题：
				  collapsed:: true
					- 确定匹配（知识与已知事实）的方法
					- 按什么策略搜索知识库
					- 冲突消解策略
				- 正向推理简单，易实现，但目的性不强，效率低
			- 逆向推理（目标驱动推理）：以某个假设目标作为出发点
			  collapsed:: true
				- 基本思想：
				  collapsed:: true
					- 选定一个假设目标
					- 寻找支持该假设的证据，若所需的证据都能找到，则原假设成立；若无论如何都找不到所需要的证据，说明原假设不成立的；为此需要另作新的假设
				- 主要优点：不必使用与目标无关的知识，目的性强，同时它还有利于向用户提供解释
				- 主要缺点：起始目标的选择有盲目性
				- 逆向推理需要解决的问题：
				  collapsed:: true
					- 如何判断一个假设是否是证据？
					- 当导出假设的知识有多条时，如何确定先选哪一条？
					- 一条知识的运用条件一般都有多个，当其中的一个经验证成立后，如何自动地换为对另一个的验证？
				- 目的性强，利于向用户提供解释，但选择初始目标时具有盲目性，比正向推理复杂
			- 混合推理
			  collapsed:: true
				- 正向推理: 盲目、效率低
				- 逆向推理: 若提出的假设目标不符合实际，会降低效率
				- 正反向混合推理：
				  collapsed:: true
					- 先正向后逆向：先进行正向推理，帮助选择某个目标，即从已知事实演绎出部分结果，然后再用逆向推理证实该目标或提高其可信度
					- 先逆向后正向：先假设一个目标进行逆向推理，然后再利用逆向推理中得到的信息进行正向推理，以推出更多的结论
			- 双向推理
			  collapsed:: true
				- 正向推理与逆向推理同时进行，且在推理过程中的某一步骤上“碰头”的一种推理
		- 冲突消解策略
		  collapsed:: true
			- 事实与知识的三种匹配情况：
			  collapsed:: true
				- 恰好匹配成功（一对一）
				- 不能匹配成功
				- 多种匹配成功（一对多、多对一、多对多）->冲突消解
			- 多种冲突消解策略：
			  collapsed:: true
				- 按针对性排序
				- 按已知事实的新鲜性排序
				- 按匹配度排序
				- 按条件个数排序
	- 自然演绎推理
	  collapsed:: true
		- 从一组已知为真的事实出发，运用经典逻辑的推理规则推出结论的过程
		- 推理规则：P规则、T规则、假言推理、拒取式推理
		  collapsed:: true
			- P规则：在推理的任何步骤都可以引入前提。即推理中使用的前提
			- T规则：推理时，如果前面步骤中有一个或者多个公式永真蕴含公式S，则可把S引入推理过程中
			- 假言推理： P, P→Q=>Q
			- 拒取式推理： P→Q, ﹁Q=>﹁P
		- 错误：
		  collapsed:: true
			- 否定前件：~~P→Q, ﹁P=>﹁Q~~
			- 肯定后件：~~P→Q, Q=>P~~
		- 优点：
		  collapsed:: true
			- 表达定理证明过程自然，易理解
			- 拥有丰富的推理规则，推理过程灵活
			- 便于嵌入领域启发式知识
		- 缺点：易产生组合爆炸，得到的中间结论一般呈指数形式递增
- ![4-第4章 不确定性推理方法（导论5）.pdf](../assets/4-第4章_不确定性推理方法（导论5）_1717297572678_0.pdf)
- 四，不确定性推理方法
  collapsed:: true
	- 不确定性推理的基本问题
	  collapsed:: true
		- 基本概念
		  collapsed:: true
			- 推理：从已知事实（证据）出发，通过运用相关知识逐步推出结论或者证明某个假设成立或不成立的思维过程
			- 不确定性推理：从不确定性的初始证据出发，通过运用不确定性的知识，最终推出具有一定程度的不确定性但却是合理或者近乎合理的结论的思维过程
		- 不确定性的表示与量度
		  collapsed:: true
			- 知识不确定性的表示
			  collapsed:: true
				- 在专家系统中知识的不确定性一般是由领域专家给出的，通常是一个数值——知识的静态强度
			- 证据不确定性的表示——证据的动态强度
			  collapsed:: true
				- 用户在求解问题时提供的初始证据
				- 在推理中用前面推出的结论作为当前推理的证据
			- 不确定性的量度
			  collapsed:: true
				- ① 能充分表达相应知识及证据不确定性的程度
				- ② 度量范围的指定便于领域专家及用户对不确定性的估计
				- ③ 便于对不确定性的传递进行计算，而且对结论算出的不确定性量度不能超出量度规定的范围
				- ④ 度量的确定应当是直观的，同时应有相应的理论依据
		- 不确定性匹配算法及阈值的选择
		  collapsed:: true
			- 不确定性匹配算法：用来计算匹配双方相似程度的算法
			- 阈值：用来指出相似的“限度”
		- 组合证据不确定性的算法
		  collapsed:: true
			- 最大最小方法、Hamacher方法、概率方法、有界方法、Einstein方法等
		- 不确定性的传递算法
		  collapsed:: true
			- 在每一步推理中，如何把证据及知识的不确定性传递给结论
			- 在多步推理中，如何把初始证据的不确定性传递给最终结论
		- 结论不确定性的合成
	- 可信度方法
	  collapsed:: true
		- 可信度：根据经验对一个事物或现象为真的相信程度。可信度带有较大的主观性和经验性，其准确性难以把握
		- C－F模型：基于可信度表示的不确定性推理的基本方法
		  collapsed:: true
			- 知识不确定性的表示
			  collapsed:: true
				- IF E THEN H (CF(H,E))
				- CF(H,E)：可信度因子（certainty factor），反映前提条件与结论的联系强度，取值范围: [-1,1]，>0越支持为真，<0越支持为假，无关为0
			- 证据不确定性的表示
			  collapsed:: true
				- CF(E)，取值范围：[-1，1]，1为肯定真，-1为肯定假，0 < CF(E) < 1为某种程度真，－1 < CF(E) < 0为某种程度假，0为无关
				- 静态强度CF（H，E）：知识的强度，即当 E 所对应的证据为真时对 H 的影响程度
				- 动态强度 CF（E）：证据 E 当前的不确定性程度
			- 组合证据不确定性的算法
			  collapsed:: true
				- E=E1 AND E2 AND … AND En则CF(E)=min{CF(E1), CF(E2), CF(E3), ...CF(En)}
				- E=E1 OR E2 OR … OR En则CF(E)=max{CF(E1), CF(E2), CF(E3), ...CF(En)}
			- 不确定性的传递算法
			  collapsed:: true
				- 结论H的可信度：CF(H)=CF(H, E)x max{0, CF(E)}
				- CF(E)<0时，CF(H)=0
				- CF(E)=1时，CF(H)=CF(H, E)
			- 结论不确定性的合成算法
			  collapsed:: true
				- 分别对每一条知识求出CF（H）
				- 求出 与 对H的综合影响所形成的可信度CF1,2(H)
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032113790.png)
		- 优点：直观、简单，且效果好
	- 证据理论：D－S理论
	  collapsed:: true
		- 概率分配函数
		  collapsed:: true
			- 设函数 M： （对任何一个属于D的子集A，命它对应一个数M [0，1]）
				- M(∅) = 0
				- ∪M(A) = 1，则 M是 上的基本概率分配函数，M(A)是 A的基本概率数
			- 设样本空间D中有n个元素，则D中子集的个数为2^n个，2^D：D的所有子集
			- 概率分配函数：把D的任意一个子集A都映射为[0，1]上的一个数M(A)
			- 概率分配函数与概率不同
		- 信任函数Bel
		  collapsed:: true
			- 2^D->[0, 1]且Bel(A)=∑M(B)，B是A的子集，任意A是D的子集
			- Bel(A)是对命题A为真的总的信任程度
		- 似然函数：不可驳斥函数或上限函数
		  collapsed:: true
			- Pl：2^D->[0, 1]且Pl(A)=1-Bel(﹁A)
		- 概率分配函数的正交和（证据的组合）
		  collapsed:: true
			- 设M1和M2是两个概率分配函数，则其正交和M=M1⊕M2
			  collapsed:: true
				- M(∅) = 0
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032157690.png){:height 119, :width 352}
				- 如果K≠0，则正交和 M也是一个概率分配函数
				- 如果K=0 ，则不存在正交和 M，即没有可能存在概率函数，称M1与M2矛盾
		- 基于证据理论的不确定性推理
		  collapsed:: true
			- 建立问题的样本空间D
			- 由经验给出，或者由随机性规则和事实的信度度量算基本概率分配函数
			- 计算所关心的子集的信任函数值、似然函数值
			- 由信任函数值、似然函数值得出结论
			- 特定概率分配函数：简化不确定性的推理模型
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032207520.png){:height 185, :width 319}
	- 模糊推理方法
	  collapsed:: true
		- 模糊逻辑的提出与发展
		  collapsed:: true
			- 1965年，美国L. A. Zadeh发表了“fuzzy set”的论文，首先提出了模糊理论
			- ...
		- 模糊集合
		  collapsed:: true
			- 定义：
			  collapsed:: true
				- 论域：所讨论的全体对象，用 U 等表示
				- 元素：论域中的每个对象，常用a,b,c,x,y,z表示
				- 集合：论域中具有某种相同属性的确定的、可以彼此区别的元素的全体，常用A，B等表示
				- 模糊逻辑给集合中每一个元素赋予一个介于0和1之间的实数，描述其属于一个集合的强度，该实数称为元素属于一个集合的隶属度。集合中所有元素的隶属度全体构成集合的隶属函数
			- 表示方法
			  collapsed:: true
				- 当论域中元素数目有限时，模糊集合A的数学描述为：
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032238545.png){:height 40, :width 315}
					- μA(x)：元素x属于模糊集A的隶属度， X是元素x的论域
				- Zadeh表示法
				  collapsed:: true
					- 论域是离散且元素数目有限
					  collapsed:: true
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032240727.png)
					- 论域是连续的，或者元素数目无限
					  collapsed:: true
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032240026.png){:height 57, :width 111}
				- 序偶表示法
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032242360.png){:height 35, :width 415}
				- 向量表示法
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032242837.png){:height 37, :width 343}
			- 隶属函数
			  collapsed:: true
				- 常见的隶属函数有正态分布、三角分布、梯形分布等
				- 隶属函数确定方法：模糊统计法，专家经验法，二元对比排序法，基本概念扩充法
		- 模糊集合的运算
		  collapsed:: true
			- 包含关系：若 μA(x)>=μB(x)，则A包含B
			- 相等关系：若 μA(x)=μB(x)，则A=B
			- 交并补运算
			  collapsed:: true
				- 交取最小：
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032248673.png){:height 33, :width 342}
				- 并取最大：
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032248846.png){:height 34, :width 390}
				- 补：
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032249017.png){:height 38, :width 223}
			- 代数运算
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032250795.png)
		- 模糊关系与模糊关系的合成
		  collapsed:: true
			- 模糊关系
			  collapsed:: true
				- 普通关系:两个集合中的元素之间是否有关联
				- 模糊关系:两个模糊集合中的元素之间关联程度的多少
				- A、B：模糊集合模糊关系用叉积表示，叉积常用最小算子运算
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032255915.png){:height 35, :width 196}
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032256834.png){:height 43, :width 366}
				- A、B：离散模糊集，其隶属函数分别为：
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032257078.png)
			- 模糊关系的合成：矩阵相乘
		- 模糊推理
		  collapsed:: true
			- 模糊知识表示
			  collapsed:: true
				- 从条件论域到结论论域的模糊关系矩阵 R。通过条件模糊向量与模糊关系 R 的合成进行模糊推理，得到结论的模糊向量，然后采用“清晰化”方法将模糊结论转换为精确量
			- 对 IF A THEN B 类型的模糊规则的推理
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406032313678.png){:height 270, :width 499}
		- 模糊决策
		  collapsed:: true
			- 由模糊推理得到的结论或者操作是一个模糊向量，转化为确定值的过程
			- 最大隶属度法
			- 加权平均判决法
			- 中位数法
		- 模糊推理的应用
		  collapsed:: true
			- 温度和风门开度
- ![5-第5章 搜索求解策略（导论5）.pdf](../assets/5-第5章_搜索求解策略（导论5）_1717297581943_0.pdf)
- 五，搜索求解策略
  collapsed:: true
	- 搜索的概念
	  collapsed:: true
		- 搜索的基本问题与主要过程
		  collapsed:: true
			- 基本问题：是否一定能找到一个解，找到的解是否是最佳解，时间与空间复杂性如何，是否终止运行或是否会陷入一个死循环
			- 主要过程：
			  collapsed:: true
				- 从初始或目的状态出发，并将它作为当前状态
				- 扫描操作算子集，将适用当前状态的一些操作算子作用于当前状态而得到新的状态，并建立指向其父结点的指针
				- 检查所生成的新状态是否满足结束状态，如果满足，则得到问题的一个解，并可沿着有关指针从结束状态反向到达开始状态，给出一解答路径；否则，将新状态作为当前状态，返回第(2)步再进行搜索
		- 搜索策略
		  collapsed:: true
			- 搜索方向：
			  collapsed:: true
				- 数据驱动：从初始状态出发的正向搜索
				- 目的驱动：从目的状态出发的逆向搜索
				- 双向搜索：从开始状态出发作正向搜索，同时又从目的状态出发作逆向搜索，直到两条路径在中间的某处汇合为止
			- 盲目搜索与启发式搜索:
			  collapsed:: true
				- 盲目搜索：在不具有对特定问题的任何有关信息的条件下，按固定的步骤（依次或随机调用操作算子）进行的搜索
				- 启发式搜索：考虑特定问题领域可应用的知识，动态地确定调用操作算子的步骤，优先选择较适合的操作算子，尽量减少不必要的搜索，以求尽快地到达结束状态
	- 状态空间的搜索策略
	  collapsed:: true
		- 状态空间表示法
		  collapsed:: true
			- 状态：表示系统状态、事实等叙述型知识的一组变量或数组 Q=[q1, q2, ..., qn]T
			- 操作：表示引起状态变化的过程型知识的一组关系或函数 F={f1, f2, ..., fm}T
			- 状态空间：利用状态变量和操作符号，表示系统或问题的有关知识的符号体系，状态空间是一个四元组 (S, O, S0, G)
			  collapsed:: true
				- S是状态集合，O是操作算子集合，S0包含问题的初始状态是S的非空子集，G是若干具体状态或满足某些性质的路径信息描述
			- 求解路径：从S0结点到G结点的路径
			- 状态空间解：一个有限的操作算子序列
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406040901240.png){:height 44, :width 361}
			- 八数码问题，旅行商问题
		- 状态空间的图描述
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406040903262.png){:height 257, :width 369}
	- 盲目的图搜索策略
	  collapsed:: true
		- 回溯策略
		  collapsed:: true
			- 带回溯策略的搜索
			  collapsed:: true
				- 从初始状态出发，不停地、试探性地寻找路径，直到它到达目的或“不可解结点”，即“死胡同”为止
				- 若它遇到不可解结点就回溯到路径中最近的父结点上，查看该结点是否还有其他的子结点未被扩展。若有，则沿这些子结点继续搜索；如果找到目标，就成功退出搜索，返回解题路径
			- 回溯搜索的算法
			  collapsed:: true
				- PS（path states）表：保存当前搜索路径上的状态。如果找到了目的，PS就是解路径上的状态有序集
				- NPS（new path states）表：新的路径状态表。它包含了等待搜索的状态，其后裔状态还未被搜索到，即未被生成扩展
				- NSS（no solvable states）表：不可解状态集，列出了找不到解题路径的状态。如果在搜索中扩展出的状态是它的元素，则可立即将之排除，不必沿该状态继续搜索
			- 回溯思想
			  collapsed:: true
				- 用未处理状态表（NPS）使算法能返回（回溯）到其中任一状态
				- 用一张“死胡同”状态表（NSS）来避免算法重新搜索无解的路径
				- 在PS 表中记录当前搜索路径的状态，当满足目的时可以将它作为结果返回
				- 为避免陷入死循环必须对新生成的子状态进行检查，看它是否在该三张表中
		- 宽度优先搜索策略
		  collapsed:: true
			- 以接近起始节点的程度（深度）为依据，进行逐层扩展的节点搜索方法
			- 特点:
			  collapsed:: true
				- 每次选择深度最浅的节点首先扩展，搜索是逐层进行的
				- 一种高代价搜索，但若有解存在，则必能找到它
			- 表
			  collapsed:: true
				- open表（NPS表）：已经生成出来但其子状态未被搜索的状态，特点：先进先出
				- closed表（ PS表和 NSS表的合并）：记录了已被生成扩展过的状态
			- 图示
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041031303.png){:height 283, :width 331}
		- 深度优先搜索策略
		  collapsed:: true
			- 首先扩展最新产生的节点, 深度相等的节点按生成次序的盲目搜索
			- 特点：扩展最深的节点的结果使得搜索沿着状态空间某条单一的路径从起始节点向下进行下去；仅当搜索到达一个没有后裔的状态时，才考虑另一条替代的路径
			- 算法：
			  collapsed:: true
				- 防止搜索过程沿着无益的路径扩展下去，往往给出一个节点扩展的最大深度——深度界限
				- 与宽度优先搜索算法最根本的不同：将扩展的后继节点放在OPEN表的前端
				- 深度优先搜索算法的OPEN表后进先出
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041040055.png){:height 260, :width 186}
				- 在深度优先搜索中，当搜索到某一个状态时，它所有的子状态以及子状态的后裔状态都必须先于该状态的兄弟状态被搜索
				- 为了保证找到解，应选择合适的深度限制值，或采取不断加大深度限制值的办法，反复搜索，直到找到解
				- 深度优先搜索并不能保证第一次搜索到的某个状态时的路径是到这个状态的最短路径
				- 对任何状态而言，以后的搜索有可能找到另一条通向它的路径。如果路径的长度对解题很关键的话，当算法多次搜索到同一个状态时，它应该保留最短路径
	- 启发式图搜索策略
	  collapsed:: true
		- 启发式策略
		  collapsed:: true
			- 启发式信息：用来简化搜索过程有关具体问题领域的特性的信息
			- 启发式图搜索策略（利用启发信息的搜索方法）的特点：重排OPEN表，选择最有希望的节点加以扩展
			- 种类：A、A*算法
			- 运用启发式策略的两种基本情况
			  collapsed:: true
				- 一个问题由于存在问题陈述和数据获取的模糊性，可能会使它没有一个确定的解
				- 虽然一个问题可能有确定解，但是其状态空间特别大，搜索中生成扩展的状态数会随着搜索的深度呈指数级增长
		- 启发信息和估价函数
		  collapsed:: true
			- 求解问题中能利用的大多是非完备的启发信息：
			  collapsed:: true
				- 求解问题系统不可能知道与实际问题有关的全部信息，因而无法知道该问题的全部状态空间，也不可能用一套算法来求解所有的问题
				- 有些问题在理论上虽然存在着求解算法，但是在工程实践中，这些算法不是效率太低，就是根本无法实现
			- 按运用的方法分类：
			  collapsed:: true
				- 陈述性启发信息：用于更准确、更精炼地描述状态
				- 过程性启发信息：用于构造操作算子
				- 控制性启发信息：表示控制策略的知识
			- 按作用分类：
			  collapsed:: true
				- 用于扩展节点的选择，即用于决定应先扩展哪一个节点，以免盲目扩展
				- 用于生成节点的选择，即用于决定要生成哪些后继节点，以免盲目生成过多无用的节点
				- 用于删除节点的选择，即用于决定删除哪些无用节点，以免造成进一步的时空浪费
			- 估价函数（evaluation function）：估算节点“希望”程度的量度
			  collapsed:: true
				- 估价函数值 f(n) ：从初始节点经过 n节点到达目标节点的路径的最小代价估计值，其一般形式是f(n)=g(n)+h(n)
				- g(n)：从初始节点 S0 到节点 n 的实际代价
				- h(n)：从节点 n 到目标节点 Sg 的最优路径的估计代价，称为启发函数
				  collapsed:: true
					- h(n) 比重大：降低搜索工作量，但可能导致找不到最优解；
					- h(n) 比重小：一般导致工作量加大，极限情况下变为盲目搜索，但可能可以找到最优解
			- 八数码问题
			  collapsed:: true
				- 启发函数1: 取一棋局与目标棋局相比，其位置不符的数码数目
				- 启发函数2: 各数码移到目标位置所需移动的距离的总和
				- 启发函数3：对每一对逆转数码乘以一个倍数, 例如3倍
				- 启发函数4：将位置不符数码数目的总和与3倍数码逆转数目相加
		- A搜索算法
		  collapsed:: true
			- A 搜索算法：使用了估价函数 f 的最佳优先搜索
			- 估价函数 f(n) = g(n) +h(n)
			  collapsed:: true
				- g(n)：状态n的实际代价，例如搜索的深度
				- h(n)：对状态n与目标“接近程度”的某种启发式估计
			- 如何寻找并设计启发函数 h(n) ，然后以 f(n) 的大小来排列OPEN表中待扩展状态的次序，每次选择 f(n) 值最小者进行扩展
			- 算法流程图
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041108934.png){:height 302, :width 356}
			- 问题：A搜索算法能不能保证找到最优解（路径最短的解）
		- A*搜索算法及其特性分析
		  collapsed:: true
			- 定义h* (n)为状态n到目的状态的最优路径的代价，则当A搜索算法的启发函数h(n)小于等于 h *(n)时，即h(n)≤h* (n)，则被称为A *算法
			- 如果某一问题有解，那么利用A*搜索算法对该问题进行搜索则一定能搜索到解，并且一定能搜索到最优的解而结束
			- A*搜索算法及其特性分析
			  collapsed:: true
				- 可采纳性
				  collapsed:: true
					- 当一个搜索算法在最短路径存在时能保证找到它，就称该算法是可采纳的
					- A*搜索算法是可采纳的
					- A*搜索算法( h(n)=0 )<=>宽度优先搜索算法
				- 单调性
				  collapsed:: true
					- 如果某一启发函数h(n)满足：1）对所有状态ni 和nj ，其中nj 是ni 的后裔，满足h(ni )h(nj)<=cost(ni, nj)，其中cost(ni, nj)是从ni到nj的实际代价。2）目的状态的启发函数值为0。则称h(n)是单调的
					- A*搜索算法中采用单调性启发函数，可以减少比较代价和调整路径的工作量，从而减少搜索代价
				- 信息性
				  collapsed:: true
					- 在两个A*启发策略的h1和h2中，如果对搜索空间中的任一状态n都有h1(n) ≤ h2(n)，就称策略h2比h1具有更多的信息性
					- 如果某一搜索策略的h(n)越大，则A*算法搜索的信息性越多，所搜索的状态越少
					- 但更多的信息性需要更多的计算时间，可能抵消减少搜索空间所带来的益处
			- A*搜索算法实例
			  collapsed:: true
				- A*算法的核心步骤：
				  collapsed:: true
					- 构造启发式函数h(n)，并满足h(n)≤h*(n)
					- 对当前状态计算f(n)=g(n)+h(n)，从候选项中取最小值的选项继续迭代
					- 若有相同的最小值选项，取h(n)较小者。若h(n)也相等，则随机选择一个
				- 关键问题：如何构造满足要求的h(n)62
- ![6-第6章 智能计算及其应用（导论5）.pdf](../assets/6-第6章_智能计算及其应用（导论5）_1717297589978_0.pdf)
- 六，智能计算及其应用
  collapsed:: true
	- 进化算法的产生与发展
	  collapsed:: true
		- 进化算法的概念：是基于自然选择和自然遗传等生物进化机制的一种搜索算法
		- 进化算法的生物学背景：适者生存
		- 设计原则：适用性原则，可靠性原则，收敛性原则，稳定性原则，生物类比原则
	- 基本遗传算法
	  collapsed:: true
		- 遗传算法：一类借鉴生物界自然选择和自然遗传机制的随机搜索算法,非常适用于处理传统搜索方法难以解决的复杂和非线性优化问题
		- 遗传算法的基本思想
		  collapsed:: true
			- 在求解问题时从多个解开始，然后通过一定的法则进行逐步迭代以产生新的解
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041337750.png){:height 234, :width 336}
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041337144.png){:height 162, :width 226}
		- 发展历史
		- 编码
		  collapsed:: true
			- 位串编码，一维染色体编码方法：将问题空间的参数编码为一维排列的染色体的方法
			  collapsed:: true
				- 二进制编码：
				  collapsed:: true
					- 用若干二进制数表示一个个体，将原问题的解空间映射到位串空间 B={0，1}上，然后在位串空间上进行遗传操作
					- 优点：类似于生物染色体的组成，算法易于用生物遗传理论解释，遗传操作如交叉、变异等易实现；算法处理的模式数最多
					- 缺点：① 相邻整数的二进制编码可能具有较大的Hamming距离，降低了遗传算子的搜索效率。15：01111 16： 10000② 要先给出求解的精度，即编码长度。优化中难以调整。③ 求解高维问题时，二进制编码串很长，算法的搜索效率低。
				- Gray 编码
				  collapsed:: true
					- 将二进制编码通过一个变换进行转换得到的编码
			- 实数编码
			  collapsed:: true
				- 采用实数表达法不必进行数制转换，可直接在解的表现型上进行遗传操作
			- 多参数级联编码
			  collapsed:: true
				- 基本思想：把每个参数先进行二进制编码得到子串，再把这些子串连成一个完整的染色体
				- 多参数映射编码中的每个子串对应各自的编码参数，所以，可以有不同的串长度和参数的取值范围
		- 群体设定
		  collapsed:: true
			- 初始种群的产生
			  collapsed:: true
				- 随机产生群体规模数目的个体作为初始群体
				- 随机产生一定数目的个体，从中挑选最好的个体加到初始群体中。这种过程不断迭代，直到初始群体中个体数目达到了预先确定的规模
				- 根据问题固有知识，把握最优解所占空间在整个问题空间中的分布范围，然后，在此分布范围内设定初始群体
			- 种群规模的确定
			  collapsed:: true
				- 群体规模太小，遗传算法的优化性能不太好，易陷入局部最优解
				- 群体规模太大，计算复杂
				- 模式定理表明：若群体规模为M，则遗传操作可从这M^3个个体中生成和检测 个模式，并在此基础上能够不断形成和优化积木块，直到找到最优解
		- 适应度函数
		  collapsed:: true
			- 将目标函数映射成适应度函数的方法
			  collapsed:: true
				- 若目标函数为最大化问题，则Fit(f(x))=f(x)
				- 若目标函数为最小化问题，则Fit(f(x))=1/f(x)
				- 将目标函数转换为求最大值的形式,且保证函数值非负
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041353013.png)
			- 适应度函数的尺度变换
			  collapsed:: true
				- 在遗传算法中，将所有妨碍适应度值高的个体产生，从 而 影 响 遗 传 算 法 正 常 工 作 的 问 题 统 称 为 欺 骗 问 题
				  collapsed:: true
					- 过早收敛：缩小这些个体的适应度，以降低这些超级个体的竞争力
					- 停滞现象：改变原始适应值的比例关系，以提高个体之间的竞争力
				- 尺度变换（fitness scaling）或定标：对适应度函数值域的某种映射变换
				  collapsed:: true
					- 线性变换
					- 幂函数变换法
					- 指数变换法
		- 选择
		  collapsed:: true
			- 选择操作也称为复制（reproduction）操作：从当前群体中按照一定概率选出优良的个体，使它们有机会作为父代繁殖下一代子孙
			- 判断个体优良与否的准则是各个个体的适应度值：个体适应度越高，其被选择的机会就越多。
			- 个体选择概率分配方法
			  collapsed:: true
				- 适应度比例方法：各个个体被选择的概率和其适应度值成比例
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041401138.png){:height 102, :width 129}
				- 排序方法
				  collapsed:: true
					- 线性排序
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041402460.png)
					- 非线性排序
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041402577.png)
			- 选择个体方法
			  collapsed:: true
				- 转盘赌选择：
				  collapsed:: true
					- 按个体的选择概率产生一个轮盘，轮盘每个区的角度与个体的选择概率成比例。
					- 产生一个随机数，它落入转盘的哪个区域就选择相应的个体交叉
				- 锦标赛选择方法：
				  collapsed:: true
					- 从群体中随机选择一定数量个体（有放回抽样），将其中适应度最高的个体保存到下一代
					- 随机竞争方法：每次按轮盘赌选择方法选取一对个体，然后让这两个个体进行竞争，适应度高者获胜。如此反复，直到选满为止。
				- 最佳个体保存方法：
				  collapsed:: true
					- 把群体中适应度最高的个体不进行交叉而直接复制到下一代中，保证遗传算法终止时得到的最后结果一定是历代出现过的最高适应度的个体
					- 让每轮迭代的最优解一定是单调变化的。其他的方法并不保护本轮最优解，可能会出现震荡现象。
		- 交叉
		  collapsed:: true
			- 基本的交叉算子
			  collapsed:: true
				- 一点交叉：在个体串中随机设定一个交叉点，实行交叉时，该点前或后的两个个体的部分结构进行互换，并生成两个新的个体
				- 二点交叉：随机设置两个交叉点，将两个交叉点之间的码串相互交换
			- 修正的交叉方法
			  collapsed:: true
				- 部分匹配交叉PMX
		- 变异
		  collapsed:: true
			- 位点变异：群体中的个体码串，随机挑选一个或多个基因座，并对这些基因座的基因值以变异概率作变动
			- 逆转变异：在个体码串中随机选择两点（逆转点），然后将两点之间的基因值以逆向排序插入到原位置中
			- 插入变异：在个体码串中随机选择一个码，然后将此码插入随机选择的插入点中间
			- 互换变异：随机选取染色体的两个基因进行简单互换
			- 移动变异：随机选取一个基因，向左或者向右移动一个随机位数
		- 一般步骤
		  collapsed:: true
			- 使用随机方法或者其它方法，产生一个有N个染色 体的初始群体 pop(1)， t=1
			- 对群体中的每一个染色体popi(t)，计算其适应值fi=fitniess(popi(t))
			- 若满足停止条件，则算法停止；否则，以概率pi=fi/∑fi从pop(t)中随机选择一些染色体构成一个新种群newpop(t+1) = {popj(t)|j=1,2,...,N}
			- 以概率 pc进行交叉产生一些新的染色体，得到一个新的群体crosspop(t+1)
			- 以一个较小的概率pm使染色体的一个基因发生变异，形成mutpop(t+1)；t=t+1 ，成为一个新的群体返回 pop(t)=mutpop(t+1)（2）。
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041436757.png){:height 389, :width 415}
		- 特点：全局优化概率算法
		  collapsed:: true
			- 遗传算法对所求解的优化问题没有太多的数学要求，由于进化特性，搜素过程中不需要问题的内在性质，可直接对结构对象进行操作
			- 利用随机技术指导对一个被编码的参数空间进行高效率搜索
			- 采用群体搜索策略，易于并行化
			- 仅用适应度函数值来评估个体，并在此基础上进行遗传操作，使种群中个体之间进行信息交换
			- 遗传算法能够非常有效地进行概率意义的全局搜素
	- 遗传算法的改进算法
		- 双倍体遗传算法
			- 基本思想
			  collapsed:: true
				- 双倍体遗传算法采用显性和隐性两个染色体同时进行进化，提供了一种记忆以前有用的基因块的功能
				- 双倍体遗传算法采用显性遗传
				- 双倍体遗传延长了有用基因块的寿命，提高了算法的收敛能力，在变异概率低的情况下能保持一定水平的多样性
			- 双倍体遗传算法的设计
				- 编码/解码：两个染色体（显性、隐性）
				-
		- 双种群遗传算法
		- 自适应遗传算法
	- 遗传算法的应用：流水车间调度问题
	-
- ![7-第7章 专家系统（导论5）.pdf](../assets/7-第7章_专家系统（导论5）_1717297597460_0.pdf)
- 七，专家系统
  collapsed:: true
	- 专家系统的产生和发展
	- 专家系统的概念
	  collapsed:: true
		- 定义
		  collapsed:: true
			- 专家系统：一类包含知识和推理的智能计算机程序
			- 基本组成：
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406042218043.png){:height 238, :width 360}
		- 特点
		  collapsed:: true
			- 特点：具有专家水平的专业知识，能进行有效的推理，具有启发性，具有灵活性，具有透明性，具有交互性
			- 专家系统与传统程序的比较：
			  collapsed:: true
				- 编程思想：传统程序 = 数据结构+算法，专家系统 = 知识+推理
				- 求解：传统程序关于问题求解的知识隐含于程序中，专家系统知识单独组成知识库，与推理机分离
				- 处理对象：传统程序数值计算和数据处理，专家系统符号处理
				- 解释功能：传统程序不具有解释功能，专家系统具有解释功能
				- 答案：传统程序产生正确的答案，专家系统通常产生正确的答案，有时产生错误的答案
				- 系统的体系结构不同
		- 类型
		- 应用
	- 专家系统的工作原理
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406042226119.png){:height 194, :width 334}
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406042226878.png){:height 203, :width 346}
	- 知识获取的主要过程与模式
	  collapsed:: true
		- 知识获取的过程：抽取知识、知识的转换、知识的输入、知识的检测
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406042228062.png){:height 138, :width 431}
		- 知识获取的模式：非自动知识获取、自动知识获取、半自动知识获取
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406042228000.png){:height 205, :width 431}
-
- ![8-补充1-模式识别系统.pdf](../assets/8-补充1-模式识别系统_1717297604972_0.pdf)
- 八，模式识别系统
  collapsed:: true
	- 基本概念
	  collapsed:: true
		- 模式：通过对具体的个别事物进行观测所得到的具有一定时间或空间分布的信息
		  collapsed:: true
			- 模式不等于事物本身，而是从事物中获得的某些信息，同一事物可以产生不同的模式
			- 模式的表示：向量、矩阵、几何表示、链码、树、时间序列、图等
		- 模式识别：是指计算机将某一模式进行分类、聚类或回归分析；是研究人类识别能力的数学模型，并借助于计算机技术实现对其模拟的科学
		- 模式类：模式所属的类别或者同一类中模式的总体
		- 模式识别系统
			- 传感器：将图像声音等物理输入转换为输入信号
			- 分割器：将物体与背景及其他物体分开
			- 特征提取器：测量用于分类的物体属性
			- 分类器：根据特征给物体赋予类别标记
			- 后处理器：主要进行上下文信息处理等
	- 模式识别系统组成
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041726975.png){:height 221, :width 392}
		- 信息获取：从具体事物获取原始信息。通常输入计算机的有三类信息：(1) 二维图像：如人脸图像；(2) 一维波形：如语音信号；(3) 物理参量和逻辑值：如体温值，疼/不疼(1/0)。
		- 预处理：对信息进行分割、增强、复原、去噪等操作。
		- 特征提取与选择：提取最能反映分类本质的特征，同时压缩数据量。(1) 测量空间：原始数据所组成的空间；(2) 特征空间：特征数据所组成的空间
		- 分类器的设计(训练)：根据训练样本集，在特征空间中确定某种判决规则，通常，该规则的确定是一个反复调整的过程，最后所得到的规则即为分类器
		- 分类决策(识别)：在特征空间中，用所设计的分类器将待识别模式归为某一模式类
		  collapsed:: true
			- 统计模式识别 vs. 句法模式识别
			- 统计模式识别：指用统计学的方法来分析所输入的模式，进而进行识别的方法
			- 句法模式识别：指通过分析模式的结构来进行识别的方法
			- 上述分类方法并不是绝对的。由于数据缺失、模式的结构规律的不确定性等问题往往不可避免，即使对结构数据，目前主流方法仍然是统计模式识别方法
			- 对于图像、时间序列等模式数据，综合考虑数据和问题的结构，同样有助于设计更好的的统计模式识别方法，如马尔科夫随机场、隐马尔科夫模型
		- 设计循环：设计一个模式识别系统通常涉及如下几个不同步骤的重复：数据采集、特征选择、模型选择、训练和评估
		- 学习和适应：
		  collapsed:: true
			- 有监督学习：存在一个教师信号，对每个输入样本能提供类别标记和分类代价
			- 无监督学习：没有显式的教师，系统对输入样本自动形成聚类或自然的组织
			- 强化学习智能体（Agent）以“试错”的方式进行学习，通过与环境进行交互获得的奖赏指导行为，目标是使智能体获得最大的奖赏。最终结果会有标签，但单次分类无标签
	- 困难与挑战
	  collapsed:: true
		- （单一特征难以达到理想的分类效果）
		- （过分复杂的模型导致复杂的判决曲线，从而影响泛化能力）
		- 模式类的类内变化
		- 模式类的类间变化
	- 主要研究方向
	  collapsed:: true
		- 特征选择与表示
		- 监督学习（分类器设计、回归分析）
		- 无监督学习（聚类）
		- 弱监督与半监督学习
		- 强化学习
	- 应用
	  collapsed:: true
		- 生物信息学，医学成像，视觉信息获取与编辑，图像分割，图像理解，目标检测与图像检索，视觉监控，生物特征识别
-
- ![9-补充2-模型评估方法.pdf](../assets/9-补充2-模型评估方法_1717297613425_0.pdf)
- 九，模型评估方法
  collapsed:: true
	- 机器学习定理
	  collapsed:: true
		- 没有免费的午餐：没有天生优越的分类器
		- 丑小鸭定理：没有天生优越的特征
		- 奥卡姆剃刀原理：不要选用比“必要”更复杂的模型
	- 经验误差与过拟合
	  collapsed:: true
		- 经验误差：模型在训练集上的误差
		- 泛化误差：模型在测试集上的误差
	- 模型评估方法
	  collapsed:: true
		- 测试集和训练集的拆分：留出法，交叉验证法，自助法
		- 留出法：
		  collapsed:: true
			- 直接将数据集划分为两个互斥集合
			- 训练/测试集划分要尽可能保持数据分布的一致性
			- 一般若干次随机划分、重复实验取平均值
			- 训练/测试样本比例通常为2:1~4:1
		- 交叉验证法：
		  collapsed:: true
			- 将数据集分层采样划分为k个大小相似的互斥子集，每次用k-1个子集的并集作为训练集，余下的子集作为测试集，最终返回k个测试结果的均值，k最常用的取值是10
			- 与留出法类似，将数据集D划分为k个子集同样存在多种划分方式，为了减小因样本划分不同而引入的差别，k折交叉验证通常随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值，例如常见的“10次10折交叉验证”。
			- 假设数据集D包含m个样本，若令k=m，则得到留一法:
				- 不受随机样本划分方式的影响
				- 结果往往比较准确
				- 当数据集比较大时，计算开销难以忍受
		- 自助法:
		  collapsed:: true
			- 以自助采样法为基础，对数据集D有放回采样m次得到训练集D,D \D'用做测试集
			- 实际模型与预期模型都使用m个训练样本
			- 约有1/3的样本没在训练集中出现，用作测试集
			- 从初始数据集中产生多个不同的训练集，对集成学习有很大的好处
			- 自助法在数据集较小、难以有效划分训练/测试集时很有用;由于改变了数据集分布可能引入估计偏差，在数据量足够时，留出法和交叉验证法更常用。
		- 模型评估三大原则
		  collapsed:: true
			- 奥卡姆剃刀：在性能得到满足的情况下，模型越简单越好
			- 数据集划分时的样本采样原则：训练集、测试集和验证集的分布应尽量一致
			- 测试集使用原则
			  collapsed:: true
				- 训练阶段不要以任何理由偷看测试集
				- 对测试集的反复评估也是一种隐蔽地偷看行为
	- 模型性能度量
	  collapsed:: true
		- 准确率与错误率
		  collapsed:: true
			- 准确率：分类结果正确的样本数量占总样本数量的比例
			- 错误率：分类结果错误的样本数量占总样本数量的比例
			- 特点
			  collapsed:: true
				- 准确率 + 错误率 = 1
				- 每个样本在统计时的权重相同
				- 适用于类样本平衡的数据集
		- 查准率与查全率
		  collapsed:: true
			- 类样本不平衡：稀有样本类别为正类，其余样本为负类
			- 查准率：
			  collapsed:: true
				- 分类结果为正类的样本中，实际结果为正类的比例
				- P=TP / (TP + FP)
			- 查全率：
			  collapsed:: true
				- 实际结果为正类的样本中，分类结果为正类的比例
				- R=TP / (TP +FN)
			- 平衡点：P与R往往是矛盾的，P = R
			- F1度量： 比平衡点更高效，F1=2*P*R / (P + R)
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041631364.png){:height 172, :width 233}
-
- ![10-补充3-机器学习模型类型.pdf](../assets/10-补充3-机器学习模型类型_1717297620758_0.pdf)
- 十，机器学习模型类型
  collapsed:: true
	- 有监督学习KNN
	  collapsed:: true
		- K最近邻(k-Nearest Neighbor，KNN)，是一种常用于分类的算法，是有成熟理论支撑的、较为简单的经典机器学习算法之一
		- 基本思路：
		  collapsed:: true
			- 如果一个待分类样本在特征空间中的k个最相似(即特征空间中K近邻)的样本中的大多数属于某一个类别，则该样本也属于这个类别
		- 需要样本标签的支持，是一种有监督学习算法
		- 数据集：
		  collapsed:: true
			- 即必须存在一个样本数据集，也称作训练集，样本数据集中每个样本是有标签的，即我们知道样本数据集中每一个样本的标签
		- 样本的向量表示：
		  collapsed:: true
			- 即不管是当前已知的样本数据集，还是将来可能出现的待分类样本，都必须可以用向量的形式加以表征。向量的每一个维度，刻画样本的一个特征，必须是量化的，可比较的
		- 样本间距离的计算方法：
		  collapsed:: true
			- 欧氏距离、余弦距离、海明距离、曼哈顿距离等等
		- K值的选取：
		  collapsed:: true
			- K值的选取会影响待分类样本的分类结果
			- K值较小：K值的减小就意味着更复杂的决策边界，每个训练样本都会形成一个决策模型，容易发生过拟合
			- K值较大：K值的增大就意味着整体的模型变得简单，会导致偏差变大
		- KNN的优点
		  collapsed:: true
			- 简单，易于理解，易于实现，无需参数估计，无需训练
			- 对异常值不敏感（个别噪音数据对结果的影响不是很大）
			- 适合对稀有事件进行分类
			- 适合于多分类问题
		- KNN的缺点
		  collapsed:: true
			- 计算量大，内存开销大
			- 可解释性差。无法告诉你哪个样本更重要
			- K值的选择。当样本不平衡时会导致错误
			- KNN是一种消极学习方法、懒惰算法
	- 无监督学习K均值聚类
	  collapsed:: true
		- “类”指的是具有相似性的集合。聚类是指将数据集划分为若干类，使得类内之间的数据最为相似，各类之间的数据相似度差别尽可能大
		- k-means算法是一种简单的迭代型聚类算法，采用距离作为相似性指标，从而发现给定数据集中的K个类，且每个类的中心是根据类中所有值的均值得到，每个类用聚类中心来描述。
		- 聚类优化目标函数：
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041555125.png)
		- 步骤
		  collapsed:: true
			- 选取数据空间中的K个对象作为初始中心，每个对象代表一个聚类中心
			- 对于样本中的数据对象，根据它们与这些聚类中心的欧氏距离，按距离最近的准则将它们分到距离它们最近的聚类中心（最相似）所对应的类
			- 更新聚类中心：将每个类别中所有对象所对应的均值作为该类别的聚类中心，计算目标函数的值
			- 判断聚类中心和目标函数的值是否发生改变，若不变，则输出结果，若改变，则返回2）
	- 半监督学习：只有少量样本带标签，更符合实际需求
	- 强化学习：单步没有标签，流程走完时有标签，常用于游戏等人工智能应用中
	- 自监督学习：系统学会从输入的其他部分预测其输入的一部分。
-
- ![11-补充4-线性模型与非线性扩展.pdf](../assets/11-补充4-线性模型与非线性扩展_1717297628172_0.pdf)
- 十一，线性模型与非线性扩展
  collapsed:: true
	- 引言
	  collapsed:: true
		- 机器学习的基本流程：定义函数（带有未知参数）->定义损失（Loss)(基于训练数据）->优化（Optimization）
		- 回归与分类是两大类问题
		  collapsed:: true
			- 回归问题的输出是连续值
			- 分类问题的输出是离散值
			- 区别：模型最后一层的设计方式不同
	- 线性回归
	  collapsed:: true
		- f (x)= wTx＋b
		- 可用最小二乘法对w和b估计：基于预测值和真实值的均方差最小化，对w和b分别求偏导
		- 实际应用中一般不采用显式求解的方法，往往通过优化方法来进行迭代寻找最优参数。比如梯度下降法
	- 线性回归实例
	  collapsed:: true
		- 定义函数（带有未知参数）
		  collapsed:: true
			- y = b +wX1，w和b未知
		- 定义损失（Loss)(基于训练数据）
		  collapsed:: true
			- MAE绝对误差，MSE平方误差
		- 优化（Optimization）：梯度下降
		  collapsed:: true
			- w0，b0随机初始化
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041515806.png){:height 58, :width 200}
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041516645.png){:height 56, :width 211}
	- 线性模型的非线性拓展
	  collapsed:: true
		- Sigmoid
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041505182.png){:height 99, :width 109}
		- 叠加
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041507053.png){:height 62, :width 294}
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041521964.png){:height 328, :width 408}
		- 回到ML框架
		  collapsed:: true
			- 函数（含有未知参数）：未知的参数包括权重矩阵W，偏置矩阵b，缩放向量cT，外部的偏置b，统称为向量θ
			- 损失是参数θ的函数，损失指明θ取值有多好，L=1/N∑en
			- 优化：
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041527985.png){:height 227, :width 306}
				- 按照batch更新
				- Sigmoid和ReLU都是激活函数：传统倾向于Sigmoid，但是深度学习倾向ReLU
				- 神经元，神经网络，深度学习：2012：AlexNet(16.4%)8层，2014：VGG(7.3%)19层，GoogleNet(6.7%)22层，2015：Residual Net(3.57%)152层
				- 过拟合现象
	- 通用的多类分类器
	  collapsed:: true
		- one-hot 向量 y=[1 0 0]T   [0 1 0]T   [0 0 1]T
		- 回归是连续输出，分类要再加一个softmax归一化 e(x) / ∑e
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041540018.png){:height 67, :width 130}
		- 损失函数的形式可以影响模型优化的难度：MSE，交叉熵，分类问题中更多会用到交叉熵，因为MSE有时候可能会卡住，周围梯度都相同
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406041541219.png){:height 120, :width 314}
-
- ![12-第8章 人工神经网络及其应用（导论5）-修订3.pdf](../assets/12-第8章_人工神经网络及其应用（导论5）-修订3_1717487192192_0.pdf)
- 十二，人工神经网络及其应用
  collapsed:: true
	- 神经元与神经网络
	  collapsed:: true
		- 生物神经元的结构
		- 神经元的数学模型
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406042232860.png){:height 222, :width 345}
		- 神经网络的结构与工作方式
		  collapsed:: true
			- 决定人工神经网络性能的三大要素：神经元的特性，神经元之间相互连接的形式——拓扑结构，为适应环境而改善性能的学习规则
			- 感知机模型
			  collapsed:: true
				- 单层感知机只拥有一层M-P神经元，即只包含输入层和输出层，输入层接受外界输入信号后传递给输出层,输出层是M-P神经元，进行激活处理。
				- 学习规则：
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406042314447.png)
				- 单层感知机的学习能力：非常有限，要解决非线性可分问题要用多层
			- 神经网络的结构
			  collapsed:: true
				- 前馈型
				  collapsed:: true
					- 所有连接线的箭头方向相同
					- 一定包含输入层与输出层，隐含层数量不定
					- 神经元间的连接线不能跨层
					- 输入层之后的所有层中的神经元均与前面一层的所有神经元有连接，定义为全连接网络
					- 每层中神经元之间无连接
				- 反馈型
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406042317068.png){:height 205, :width 248}
			- 神经网络的工作方式
			  collapsed:: true
				- 同步（并行）方式：任一时刻神经网络中所有神经元同时调整状态
				- 异步（串行）方式：任一时刻只有一个神经元调整状态，而其它神经元的状态保持不变
			- 神经网络的工作过程
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406042318365.png)
		- 神经网络的学习
		  collapsed:: true
			- 神经网络方法是一种知识表示方法和推理方法
			- 神经网络知识表示是一种隐式的表示方法
			- 1944年赫布（Hebb）提出了改变神经元连接强度的 Hebb学习规则
			- Hebb学习规则：当某一突触两端的神经元同时处于兴奋状态，那么该连接的权值应该增强
	- BP神经网络及其学习算法
	  collapsed:: true
		- BP神经网络的结构
		  collapsed:: true
			- BP 网络结构
			- 输入输出变换关系
			- 工作过程
			  collapsed:: true
				- 第一阶段或网络训练阶段：N 组输入输出样本。对网络的连接权进行学习和调整，以使该网络实现给定样本的输入输出映射关系
				- 第二阶段或称工作阶段：把实验数据或实际数据输入到网络，网络在误差范围内预测计算出结果
		- BP学习算法
		  collapsed:: true
			- 两个问题：
			  collapsed:: true
				- 是否存在一个BP神经网络能够逼近给定的样本或者函数
				- 如何调整BP神经网络的连接权，使网络的输入与输出与给定的样本相同
		- ML Framework
		  collapsed:: true
			- 定义函数：前馈型多层神经网络，未知参数：连接权值w,偏置b
			- 定义损失
			- 优化：BP算法框架，参数优化方法
		- BP算法的实现
	- BP神经网络在模式识别中的应用
	- Hopfield神经网络及其改进
	  collapsed:: true
		- 离散Hopfield神经网络模型
		  collapsed:: true
			- 网络结构：
			-
	- Hopfield神经网络的应用
	- 卷积神经网络与深度学习
	  collapsed:: true
		- 卷积神经网络的结构
		  collapsed:: true
			- 总体
			  collapsed:: true
				- CNN是一个多层的神经网络，每层由多个二维平面组成，而每个平面由多个独立神经元组成
				- C层为特征提取层（卷积层），一个卷积模板（滤波器）产生一个特征图
				- S层是特征映射层（池化层（Pooling）/下采样层）
				- CNN中的每一个C层一般都紧跟着一个S层
			- 特征提取层（卷积层）——C层（Convolution layer）
			  collapsed:: true
				- 大部分的特征提取都依赖于卷积运算
				- 利用卷积算子对图像进行滤波，可以得到显著的边缘特征
			- 池化
			  collapsed:: true
				- 通过卷积获得了特征之后，如果直接利用这些特征训练分类器，计算量是非常大的
				- 对不同位置的特征进行聚合统计,称为池化
				- 池化常用方法：平均池化、最大池化
				- 卷积神经网络在池化层会丢失大的信息，从而降低了空间分辨率，导致了对于输入微小的变化，其输出几乎是不变的
		- 关键技术
		  collapsed:: true
			- 局部链接，权值数量大大减少
			- 权值共享，所有卷积模板均相同
			- 多卷积核，多特征学习
		- 3通道的2D CNN
		  collapsed:: true
			- 过滤器中的每个卷积核都应用到输入层的 3 个通道，执行 3 次卷积后得到了尺寸为 3 x 3 的 3 个通道
			- 融合3个通道的结果，即将3个通道的输出结果求和，变成单通道数据形式
		- 3D CNN：空间上，时间上的三维结构
		  collapsed:: true
			- 单个过滤器生成一个特征图，两个过滤器生成两个特征图
			- 基于3D CNN的视频流处理
			  collapsed:: true
				- 网络包含1个hardwired kernels层，3个卷积层，2个下采样层，1个全连接层组成
				- 直接对连续的帧做3D卷积，提取的信息过于简单，一般在3D卷积之前做一次hardwired kernels 卷积（含有灰度、x方向梯度、y方向梯度、x方向光流、y方向光流等5种特征信息）
		- 卷积神经网络的应用LeNet5
		  collapsed:: true
			- LeNet-5是一个数字手写系统，不包含输入层，共有7层，每层都包含可训练参数（连接权重）。输入图像为32*32大小
			- 卷积核大小：卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂
			- 步长：卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素
			- 填充：0填充或重复边界值填充等。由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。
		- 胶囊网络
		  collapsed:: true
			- 简介
			  collapsed:: true
				- Hinton教授 2017年，针对卷积神经网络训练数据需求大、环境适应能力弱、可解释性差、数据分享难等不足
				- 胶囊是一个包含多个神经元的载体，每个神经元表示了图像中出现的特定实体的各种属性
				- 胶囊不是传统神经网络中的一个神经元，而是一组神经元
				- 胶囊网络的核心思想：胶囊里封装的检测特征的相关信息是以向量的形式存在的，胶囊的输入是一个向量，是用一组神经元来表示多个特征
			- CNN缺陷
			  collapsed:: true
				- CNN中没有可用的空间信息：CNN不会识别特征之间的相互关系。没有学习到一种正确的特征间相位置对关系（特征的姿态信息）
				- 池化操作导致信息严重丢失如最大池化，只保留最为活跃的神经元，传递到下一层，导致有价值的空间信息丢失
			- 结构
			  collapsed:: true
				- 输入层：数字图片经过标准的卷积层，有256个通道，每个通道均用9×9的卷积核。用于抽取底层特征
				- PrimaryCaps层：卷积的胶囊层，包含32个胶囊。一个胶囊相当于多个神经元的组合。PrimaryCaps才是胶囊真正开始的地方。用于第二次卷积，初始化胶囊的输入
				- DigitCaps层：胶囊网络的全连接层。用于编码空间信息并进行最终分类。要识别的是10类数字（0~9），因此该层的胶囊个数共有10个，每个胶囊表示的向量中元素的个数为16，代表着不同状态下的同一个数字
			- 优点
			  collapsed:: true
				- 使用胶囊神经网络需要的训练数据量，远小于卷积神经网络，它采用动态路由协议算法，仅使用三层网络便可表现出很出色的性能，能够与深度卷积神经网络相当
				- 胶囊网络解决了卷积神经网络存在的信息丢失、视角变化等问题
				- 由于胶囊网络具有分别处理不同属性的能力，相比于CNN可以提高对图像变换的鲁棒性，在图像分割中也会有出色的表现
				- 胶囊网络相对于卷积网络的工作机理更接近人脑的工作方式
	- 生成对抗网络及其应用
	  collapsed:: true
		- Ian Goodfellow 2014年提出生成对抗网络GAN，使用对抗训练机制对两个神经网络进行训练，高斯噪声最终变为生成分布
		- 基本原理
		  collapsed:: true
			- 生成网络从隐空间（latent space）中随机采样作为输入，其输出结果需要尽量模仿训练集中的真实样本
			- 判别网络的输入则为真实样本或生成网络的输出，其目的是将生成网络的输出从真实样本中尽可能分辨出来
		- 结构与训练
		  collapsed:: true
			- 生成网络要尽可能地欺骗判别网络
			- 判别网络将生成网络生成的样本与真实样本尽可能区分出来
			- 两个网络相互对抗、不断调整参数，最终目的是使判别网络无法判断生成网络的输出结果是否真实
			- 固定判别网络，训练生成网络；固定生成网络，训练判别网络
			- 损失函数
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406050018151.png)
			- GAN在训练中容易出现一些问题，训练过程具有强烈的不稳定性，实验结果随机，具体表现：
			  collapsed:: true
				- 训练过程难以收敛，经常出现震荡；
				- 训练收敛，但是出现模式崩溃
				- 训练收敛，但是GAN还会生成一些没有意义或者现实中不可能出现的图片
		- 应用
-
- ### 第四章 不确定性推理方法
  collapsed:: true
	- 基本概念
	  collapsed:: true
		- 不确定性推理
		  collapsed:: true
			- 推理：是从已知事实出发，运用知识推出结论或者证明
			- 不确定性推理：从**不确定性的初始证据**出发，通过运用**不确定性的知识**，最终推出具有一定程度的不确定性但却是合理或者近乎合理的结论的思维过程
		- 表示与度量
		  collapsed:: true
			- 知识不确定性：一般由领域专家给出，数值，知识的静态强度
			- 证据不确定性：用户提供的初始证据，证据的动态强度
			- 不确定性度量：充分表达，便于估计，便于不确定性传递的计算，直观
		- 不确定性匹配算法，阈值选择
		  collapsed:: true
			- 不确定性匹配算法：用来计算匹配双方相似程度的算法
			- 阈值：用来指出相似的“限度”
		- 组合证据不确定性算法
		  collapsed:: true
			- 最大最小方法、Hamacher方法、概率方法、有界方法、Einstein方法等
		- 不确定性传递算法
		  collapsed:: true
			- 把证据和知识的不确定性经过每一步推理/多步推理传递到结论
		- 结论不确定性的合成
	- 可信度方法
	  collapsed:: true
		- 可信度：一个事物为真的相信程度，主观性，经验性
		- C-F模型：基于可信度表示不确定性推理
		- 知识的不确定性表示：IF E THEN H (CF(H, E))
		  collapsed:: true
			- E是知识的前提条件；H是知识的结论
			- CF(H, E)：可信度因子 certainty factor，前提条件与结论的联系强度
			  collapsed:: true
				- 取值范围：[-1, 1]，大于0则H为真且越大越真，小于0则H为假且越小越假，等于0则无关
		- 证据不确定性的表示：CF(E)
		  collapsed:: true
			- 同知识
			- 静态强度CF(H, E)：知识的强度，即当E所对应的证据为真时对H的影响程度
			- 动态强度CF(E)：证据E当前的不确定性程度
		- 组合证据不确定性的算法
		  collapsed:: true
			- 多个单一证据的合取
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403141132615.png)
			- 多个单一证据的析取
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403141133736.png)
		- 不确定性的传递算法
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403141135813.png){:height 50, :width 410}
		- 结论不确定性的合成算法
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403141136686.png){:height 269, :width 413}
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403141136291.png)
		- 计算的例子见ppt
	- 证据理论
	  collapsed:: true
		- 概率分配函数
			- 样本空间D：D是变量 x 所有可能取值的集合，且D中的元素是互斥的，在任一时刻x 都取且只能取D 中的某一个元素为值
			- 概率分配函数M(A)：领域内的命题A是D的子集
				- 函数M：2^D->[0, 1] ，对任何一个属于D的子集A，命它对应一个数M∈[0，1]，且满足 ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403141147528.png){:height 34, :width 93}![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202403141147112.png){:height 51, :width 111}
		- 信任函数
		- 似然函数
		- 概率分配函数的正交和（证据的组合）
		- 基于证据理论的不确定性推理
	- 模糊推理方法
-
- 去年考题
	- 名词概念类
	  collapsed:: true
		- LLM
		  collapsed:: true
			- （Large Language Model）
		- AIGC
		  collapsed:: true
			- （Artificial Intelligence Generated Content）
		- AGI
		  collapsed:: true
			- （Artificial General Intelligence）
		- GPT
		  collapsed:: true
			- (Generative Pre-Trained Transformer)
		- GPTs
		  collapsed:: true
			- (Generative Pre-Trained Transformers)
	- gpt类
	  collapsed:: true
		- GPT全称：Generative Pre-trained Transformer
		  collapsed:: true
			- GPT 是一种大型语言模型，能够生成各种不同的文本类型，由OpenAI开发
			- 而 ChatGPT 则是针对对话场景特别优化过的，它可以根据上下文自动生成跟人类一样的文本对话
		- chatgpt用到的技术
		  collapsed:: true
			- 第〇步：文字接龙—— GPT 大模型
			  collapsed:: true
				- 掌握基于前文内容生成后续文本的能力。这样的训练不需要人类标注数据，只需要给一段话的上文同时把下文遮住，将 AI 的回答与语料中下文的内容做对比，就可以训练 AI
			- 第一步：人类引导接龙方向——有监督训练初始模型
			  collapsed:: true
				- 研究人员让人类就一些问题写出人工答案，再把这些问题和答案丢给 GPT 学习
			- 第二步：给 GPT 请个“好老师”—— Reward 模型
			  collapsed:: true
				- 有个能辨别 GPT 回答好坏的「老师模型」（即 Reward 模型），以人类的评分标准对 GPT 所给出的答案进行评分
				- 于是研究人员让 GPT 对特定问题给出多个答案，由人类来对这些答案的好坏做排序（相比直接给出答案，让人类做排序要简单的多
			- 第三步：AI 指导 AI ——强化学习优化模型
			  collapsed:: true
				- 强化学习，我们随机问简易版 ChatGPT 一个问题并得到一个回答，让 Reward 模型（老师模型）给这个回答一个评分，AI 基于评分去调整参数以便在下次问答中获得更高分
		- 如果chatgpt api开放，你用它做什么
		  collapsed:: true
			- 语言理解与生成：利用ChatGPT API来提升对自然语言的理解，以及生成更加自然和准确的回答。
			- 聊天机器人：集成ChatGPT API来创建更加智能和有趣的聊天机器人，提供更高质量的对话体验。
			- 内容创作：使用API生成创意写作、故事、文章或其他类型的内容。
			- 语言翻译：结合API进行语言翻译，提供多语言支持。
			- 教育辅助：开发教育工具和应用，利用API提供个性化的学习体验和辅导
			- 信息检索：利用API增强信息检索系统，提供更准确和相关的搜索结果。
			- 情感分析：集成API进行情感分析，理解用户的情绪和反馈。
			- 问答系统：开发智能问答系统，提供快速准确的信息检索和问题解答
			- 辅助决策：结合API提供决策支持，分析数据和文本以提供建议
			- 社交媒体管理：使用API自动生成社交媒体内容或回复用户评论
			- 虚拟助手：集成API到虚拟助手中，提供更智能的个人助理服务
			- 游戏开发：在游戏中使用API为非玩家角色（NPC）提供更自然的语言交流
			- 自动化客户服务：利用API自动化客户服务流程，处理客户咨询和问题
			- 数据分析：结合API分析和解释大量文本数据，提取有用信息
			- 个性化推荐：使用API根据用户的语言和偏好提供个性化推荐
	- 历史类
	  collapsed:: true
		- 1936年，图灵：图灵机
		- 1943年，麦克洛奇和皮兹提出M-P模型，开创了人工神经网络研究
		- 1956 年 达特茅斯会议正式提出人工智能（artificial intelligence, AI），麦卡锡提议正式采用“人工智能”这一术语，标志着人工智能学科正式诞生。麦卡锡因而被称为人工智能之父
		- 1957年，罗森勃拉特提出感知器模型
		- 1986年，Rumelhart与Hinton提出BP算法
		- 1989年，Yann Lecun提出LeNet，数字识别
		- 2006年，Hinton在SCIENCE发表深度神经网络论文，可以认为是深度学习的起点
		- 2007年 李飞飞 ImageNet 一千四百万图片，两万分类，对模型的泛化能力要求高
		- 2012年，Hinton在ImageNet数据库竞赛中使用CNN获得极高精度，引爆了深度学习的浪潮AlexNet
		- 2012年5月16日：谷 歌首先发布了知识图谱（Knowledge Graph）
		- 2014年GAN(Generative Adversarial Nets)
		- 2015年，何恺明提出残差网络ResNet
		- 2016年，DeepMind发布AlphaGo，CNN+RL
		- 2017年，Google发表Transformer
		- 2017年10月：AlphaGo Zero，自学成才
		- 2018年 深度学习3巨头共同获得图灵奖
		  collapsed:: true
			- Yann Lecun 美国纽约大学Meta首席科学家CNN模型
			- Geoffrey Hinton加拿大多伦多大学深度学习理论
			- Yoshua Bengio加拿大蒙特利尔大学序列模型
		- 2020,大模型元年，GPT 3.0
		- 2022年，OpenAI发表GPT3，并发布ChatGPT
		- 2023.2，GPT4.0
		- GPTs (2023.11，智能体)
		- Sora（2024.2）
	- 卷积类
		- 卷积层和池化层的作用
		  collapsed:: true
			- 在卷积神经网络（CNN）中，卷积层和池化层都是用于提取图像特征的
			- 卷积层：通过使用卷积核来提取图像的局部特征
			- 池化层：通过对卷积层输出的特征图进行下采样来减少特征图的大小。池化层可以帮助减少过拟合，提高模型的泛化能力
		- CNN中的关键技术
		  collapsed:: true
			- 局部连接
			- 权值共享：所有卷积模板都相同
			- 多卷积核
		- CNN优于全连接神经网络的地方
		  collapsed:: true
			- 参数共享：CNN中的卷积核在整个输入数据上共享相同的权重，这意味着无论输入特征图有多大，卷积层的参数数量固定，这大大减少了全连接层中的参数数量
			- 局部连接：每个卷积神经元只与输入数据的一个局部区域相连接，这使得网络能够捕捉局部特征，同时减少了计算量
			- 自动特征提取：CNN能够自动学习到从输入数据中提取特征的最佳方式，而全连接网络通常需要手动设计特征提取器
			- 平移不变性：CNN通过使用滑动窗口的方式应用卷积核，能够在一定程度上保持对输入数据平移等变化的不变性，这对于图像识别等任务非常重要
			- 多尺度信息：通过使用不同大小的卷积核或池化层，CNN能够捕捉多尺度的特征，这有助于处理不同尺寸的特征
			- 更好的泛化能力：CNN通常能够在新数据上展现出更好的泛化能力，尤其是在大规模数据集上训练时
		- 卷积的原理，卷积层的特点和作用
		- 对于单通道，如果用不同大小的卷积核，那输出的特征图的大小相同吗
		  collapsed:: true
			- 如果步长和填充保持不变，仅改变卷积核的大小，输出特征图的大小将会改变。较大的卷积核通常会减少特征图的尺寸，而较小的卷积核则可能保持或稍微减小特征图的尺寸
			- (W-K+2P)/S + 1
		- 参数的计算
		  background-color:: blue
	- bp类
	  collapsed:: true
		- 简述bp神经网络的理论
		  collapsed:: true
			- 正向传播：输入信息由输入层传至隐层，最终在输出层输出
			- 反向传播：修改各层神经元的权值，使误差信号最小
		- bp的实现流程
		  collapsed:: true
			- 初始化：对所有连接权和阈值赋以随机任意小值
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406060943342.png)
			- 从 N 组输入输出样本中取一个样本：将样本特征向量及其对应的类别标签输入到BP网络中
			- 正向传播：计算各层节点的输出
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406060943162.png){:height 44, :width 264}
			- 计算网络的实际输出与期望输出的误差
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406060943329.png){:height 36, :width 289}
			- 反向传播：从输出层方向计算到第一个隐层，按连接权值修正公式向减小误差方向调整网络的各个连接权值
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406060944753.png){:height 118, :width 423}
			- 让t+1→t，取出另一个样本重复（2）－（5），直到 N个样本的训练误差达到要求时为止
			- 图示
			  collapsed:: true
				- ![](https://img-blog.csdnimg.cn/img_convert/7762be10f8cf3b5e2e11e5e2f8642d55.png)
		- bp特点：多层前向网络（输入层、隐层、输出层）
		  collapsed:: true
			- 连接权值：通过Delta学习算法进行修正
			- 神经元激活函数：S形函数等
			- 学习算法：正向传播、反向传播
			- 层与层的连接是单向的，信息的传播是双向的
		- bp优缺点
		  collapsed:: true
			- 优点：
			  collapsed:: true
				- 非线性映射能力，逼近特性：BP神经网络实质上实现了一个从输入到输出的映射功能,数学理论证明三层的神经网络就能够以任意精度逼近任何非线性连续函数。这使得其特别适合于求解内部机制复杂的问题,即BP神经网络具有较强的非线性映射能力
				- 自学习和自适应能力：BP神经网络在训练时,能够通过学习自动提取输出、输出数据间的“合理规则”,并自适应的将学习内容记忆于网络的权值中。即BP神经网络具有高度自学习和自适应的能力
				- 泛化能力：所谓泛化能力是指在设计模式分类器时,即要考虑网络在保证对所需分类对象进行正确分类,还要关心网络在经过训练后,能否对未见过的模式或有噪声污染的模式,进行正确的分类。也即BP神经网络具有将学习成果应用于新知识的能力
				- 容错能力：BP神经网络在其局部的或者部分的神经元受到破坏后对全局的训练结果不会造成很大的影响,也就是说即使系统在受到局部损伤时还是可以正常工作的。即BP神经网络具有一定的容错能力
			- 缺点：
			  collapsed:: true
				- 梯度消失或爆炸：在深层网络中，梯度可能会随着传播而迅速减小或增大，导致训练困难
				- 局部最小值和鞍点：BP算法可能会陷入局部最小值或鞍点，而不是全局最小值
				- 对初始化敏感：权重的初始化对网络的训练效果有很大影响，不当的初始化可能导致训练不收敛
				- 收敛速度慢，本质为梯度下降法，它所要优化的目标函数非常复杂，因此会出现“锯齿形现象”，使得BP算法低效
				- 难以确定隐层和隐层结点的数目
		- 如何解决bp神经网络噪声的梯度消散的问题
		  collapsed:: true
			- 原因：
			  collapsed:: true
				- sigmoid的导数是限制在[0, 1/4]，当它接近0时，有可能指数效应引起梯度消失
				- w参数初始化一般较小，但如果模值大于1了，很有可能引发梯度过大，爆炸，NaN
			- 方法：
			  collapsed:: true
				- 激活函数的改进：传统的激活函数如sigmoid和tanh在输入较大或较小的情况下梯度会趋近于零，可以尝试使用ReLU（Rectified Linear Unit）等激活函数，它在正区间上保持梯度为常数
				- 使用批归一化（Batch Normalization）：批归一化是一种在神经网络层之间插入归一化操作的技术。它有助于缓解梯度消失问题，提高网络的稳定性和训练速度。
				- 使用残差连接（Residual Connections）：残差连接是一种跳跃连接，将输入直接传递到网络的后续层。这样可以避免梯度在深层网络中消失，使得信息能够更容易地传递。
				- 使用梯度裁剪（Gradient Clipping）：梯度裁剪是一种限制梯度的大小的技术。通过设置梯度的阈值，可以避免梯度爆炸，并在一定程度上减轻梯度消失问题。
				- 使用正则化方法：正则化方法如L1正则化和L2正则化可以帮助减小权重的大小，从而缓解梯度消失问题。
				- 选择或设计适合特定任务的网络架构也能够缓解梯度消失问题。Transformer
	- 传统机器学习
	  collapsed:: true
		- knn
		  collapsed:: true
			- 介绍knn：分类算法
			  collapsed:: true
				- 如果一个待分类样本在特征空间中的k个最相似的样本中的大多数属于某一个类别，则该样本也属于这个类别
				- 需要样本标签，是一种有监督学习算法
			- k值的选取：影响分类样本的分类结果
			  collapsed:: true
				- K值较小：K值的减小就意味着更复杂的决策边界，每个训练样本都会形成一个决策模型，容易发生过拟合
				- K值较大：K值的增大就意味着整体的模型变得简单，会导致偏差变大。比如k为总的训练样本数量，那么每次投票肯定都是训练数据中多的类别获胜
			- knn的优缺点
			  collapsed:: true
				- 优点：
				  collapsed:: true
					- 简单，易于理解，易于实现，无需参数估计，无需训练
					- 对异常值不敏感（个别噪音数据对结果的影响不是很大）
					- 适合对稀有事件进行分类
					- 适合于多分类问题
				- 缺点：
				  collapsed:: true
					- 计算量大，内存开销大
					- 可解释性差。无法告诉你哪个样本更重要
					- K值的选择。当样本不平衡时会导致错误
					- KNN是一种消极学习方法、懒惰算法
		- k-means：
		  collapsed:: true
			- 聚类：将数据集划分为若干类，使得类内之间的数据最为相似，各类之间的数据相似度差别尽可能大
			- 介绍k-means：迭代型聚类算法
			  collapsed:: true
				- 采用距离作为相似性指标，从而发现给定数据集中的K个类
				- 每个类的中心是根据类中所有值的均值得到，每个类用聚类中心来描述
				- 聚类优化目标函数：
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406060955187.png)
			- 步骤
			  collapsed:: true
				- 选取数据空间中的K个对象作为初始中心，每个对象代表一个聚类中心
				- 对于样本中的数据对象，根据它们与这些聚类中心的欧氏距离，按距离最近的准则将它们分到距离它们最近的聚类中心（最相似）所对应的类
				- 更新聚类中心：将每个类别中所有对象所对应的均值作为该类别的聚类中心，计算目标函数的值
				- 判断聚类中心和目标函数的值是否发生改变，若不变，则输出结果，若改变，则返回2）
	- 遗传算法
	  collapsed:: true
		- 特点：全局优化概率算法
		  collapsed:: true
			- 遗传算法对所求解的优化问题没有太多的数学要求，由于进化特性，搜素过程中不需要问题的内在性质，可直接对结构对象进行操作
			- 利用随机技术指导对一个被编码的参数空间进行高效率搜索
			- 采用群体搜索策略，易于并行化
			- 仅用适应度函数值来评估个体，并在此基础上进行遗传操作，使种群中个体之间进行信息交换
		- 遗传算法的基本过程
		- 各个参数对算法的影响
		  background-color:: blue
	- gan
	  collapsed:: true
		- gan简介：
		  collapsed:: true
			- 生成对抗网络（GAN）是一种深度学习模型，由两个神经网络组成：生成器和判别器
			- 生成器的作用是生成与训练数据相似的新数据
			- 判别器的作用是区分真实数据和生成器生成的数据
			- GAN 的训练过程是交替训练这两个网络，直到生成器生成的数据能够以假乱真，并与判别器的能力达到一定均衡
		- gan工作流程：
		  collapsed:: true
			- 初始化生成器和判别器的参数
			- 从训练集中抽取 n 个样本，以及从噪声分布中采样得到 n 个噪声样本。
			  固定生成器，训练判别器，使其尽可能区分真假
			- 固定判别器，训练生成器，使其尽可能欺骗判别器
			- 多次更新迭代后，最终辨别器无法区分图片到底是来自真实的训练样本集合，还是来自生成器 G 生成的样本即可，此时辨别的概率为0.5，完成训练
		- gan的问题：训练过程具有强烈的不稳定性，实验结果随机
		  collapsed:: true
			- 训练过程难以收敛，经常出现震荡
			- 训练收敛，但是出现模式崩溃(model collapse)
			- 训练收敛，但是GAN还会生成一些没有意义或者现实中不可能出现的图片
	- 即使使用可val，但是仍出现过拟合
	  collapsed:: true
		- val的大小
		- 模型有多复杂，Hval通常不会太大（待选择的模型太多也会造成overfitting）
	- 图灵测试
	  collapsed:: true
		- 测试涉及三个参与者：一个人类提问者（C），一个人类回答者（A），以及一个机器回答者（B）。提问者与回答者之间存在隔离，他们不能见面，交流只能通过键盘输入（或最初设想的打字机）进行。提问者提出问题，并通过回答来判断回答者是人类还是机器。如果提问者在对话过程中不能一致地区分出哪个是机器，或者认为机器与人类一样智能，那么机器就可以说通过了图灵测试
		- 即使通过图灵测试也不能说明计算机能思维
	- 模型的评估
	  collapsed:: true
		- 训练集和验证集的划分
		- 评估模型的三大原则
		  collapsed:: true
			- 奥卡姆剃刀：在性能得到满足的情况下，模型越简单越好
			- 数据集划分时的样本采样原则：训练集、测试集和验证集的分布应尽量一致
			- 测试集使用原则：训练阶段不要以任何理由偷看测试集，对测试集的反复评估也是一种隐蔽地偷看行为
		- 模型性能度量
			- 准确率：分类结果正确的样本数量占总样本数量的比例
			- 错误率：分类结果错误的样本数量占总样本数量的比例
			- 准确率和错误率的特点：准确率 + 错误率 = 1，每个样本在统计时的权重相同，适用于类样本平衡的数据集
			- 查准率recall：分类结果为正类的样本中，实际结果为正类的比例，P=TP / (TP + FP)
			- 查全率precision：实际结果为正类的样本中，分类结果为正类的比例，R=TP / (TP +FN)
			- 查准率与查全率：适合类样本不平衡的数据集，稀有样本类别为正类，其余样本为负类
			- F1度量：是P和R的调和平均，F1=2*P*R / (P + R)
-
- 上课业余知识点
	- 第二章
	  collapsed:: true
		- 大模型怎么走，MoonShot 月之暗面 10亿美元，90后，80人；Minimax，Meta，Facebook开源模型or研发
		- 八，九十年代专家系统很火，大模型的核心是Transformer由Google提出，陆奇all in ai，Claude多模态
		- 知识图谱不一定只是面向互联网，eg医疗
	- 第三章
	  collapsed:: true
		- 单调模型类似于瀑布模型，不能回退修改
	- 第四章
	  collapsed:: true
		- 微软投了OpenAI 10亿，Musk2019年起诉OpenAI，Altman和Grek和Llya宫斗，Llya最终被踢出局
		- 深度学习从一个浅层的网络变成几千几万层的时候会出现梯度消失和梯度爆炸
		- Yann Lecun发明CNN
		- AGI的核心：next token prediction，token可以理解为样本，信号。前面吐出的在后一个被算入已知
		- AI Agent，OpenAI Store
	- 第五章
	  collapsed:: true
		- Robot Programmer：Devin，只做编程
		- 深度学习和机器学习的本质是搜索->找到最优函数
		- 搜索找到的不一定是最优解，暴力搜索能找到，理论上能找到，时间上代价太大，需要终止
		- 宽度优先找到的是最短的，但是深度优先并不能保证是最短路径。不过宽度的效率太低，不适合实际应用
		- ML和DL中是基于深度的，找到的不一定是最优的，找到基本合理的方向即可
		- 为什么接受局部最优？①全局最优很难找②找到的全局最优未必真的好，可能不连续，也可能由噪声引起
		- Softbank孙正义 2000年->2000万$ 阿里，2004年->4000万 $，28% stock
		- AI 2023年6月，十年之后AGI实现，十年之后碾压
		  collapsed:: true
			- AGI(Artificial General Intelligence)，通用人工智能，强人工智能：具备与人类同等智能、或超越人类的人工智能
			- 相对的，只解决特定的某一类问题的叫做弱人工智能
		- 求解问题强调相关性但并不是严格的理论推理
		- 25个AI重要模型，其中22个出自公司，3个出自学校，因为LLM Scaling Law，公司有更多算力
		- h*(n)实际上与启发式函数没关系，指的是实际上的最优路径
	- 第六章
	  collapsed:: true
		- Altman说5-10年会用自然语言取代编程，自由，范式
		- Yann Lecun(CNN发明者)批判Transformer，但是目前Transformer是主流，没看到瓶颈
		- 深度学习是end-to-end端到端，中间是个黑盒，可解释性不强
		- 周志华提出deep forest，不是基于梯度的，决策树的可解释性最强
		- 交叉概率一定要比变异概率大得多
		- 孪生网络，Transformer中encoder和decoder，层间信息交换
	- 第七章
	  collapsed:: true
		- 专家系统的推理能力是局限的
	- 第八章
	  collapsed:: true
		- GPT Agent：基于ChatGPT建立程序调教
		- 模式识别添加后处理可以大大提高准确率，同时后处理和应用场景相关联
		- 分类识别分为统计模式识别和句法模式识别
		  collapsed:: true
			- 统计模式识别的抗干扰能力好
			- 句法识别，eg比划，形成符号序列，用于70-80-90年代
		- 模型越复杂泛化能力可能越差
		- 通过画网格发现有效特征，最复杂的是分类模型，但是性能上特征提取的作用最大
		- 强化学习：总体上训练是无监督的，但最后有标签又类似于有监督
		- 最近邻是惰性算大，没有训练过程
		- 分类器模型有两种：
		  collapsed:: true
			- 一种是基于生成的Generative model，不关心那个类，学习样本的分布，更为本质
			- 另一种是基于判别的Discriminative model，不管分布，只关心分类，好建立，黑盒
	- 第九章
	  collapsed:: true
		- 检验特征的学习：把这些特征用一定的方式表示出来，eg权重，热力图
		- 和欠拟合相比，过拟合是主要的问题
		- Yann Lecun提出World Model，远大于LLM，图灵测试，语言模型，多模态，批判Sora(transformer+diffusion)
		- LVM，大型视觉语言模型，Vision，AI for Science，李飞飞支持
	- 第十章
	  collapsed:: true
		- knn没有显式的学习过程，是一种惰性算法，k是超参数，现在的k近邻一般用作baseline
		- 聚类一开始采用的是随机，对k值和聚类中心敏感
		- 自监督本质上是无监督，但是在中间过程中又存在监督
	- 第十一章
	  collapsed:: true
		- 定义损失时会包含所有参数，否则没法优化某个未包含参数
		- 损失计算的时候为什么求和之后还要平均：从优化角度不起作用，但是从直观物理理解角度有意义，平均看1个损失到什么程度
	-
- ImageNet，2.2万类，2000万 -> 2006 李飞飞 众包 2010年发布，2012年AlexNet，Llya，Hinton 85%
- 一个偏置+多个sigmoid之和
- 所有参数统称θ
- 损失函数L(θ)
- 做成batch的问题
	- 10000个样本，100个样本，梯度迈进的方向不同，但是不会有问题。学习率很重要，学习率需要很小，如果学习率很大会震荡很大
- ReLU：Rectified Linear Unit(ReLU) 负数为0，正数不变
- 两个特定的ReLU相加是Sigmoid
- 传统倾向于Sigmoid，但是深度学习倾向ReLU
- 2012：AlexNet(16.4%)，2014：VGG(7.3%)，GoogleNet(6.7%)，2015：Residual Net(3.57%)
- Fat 没有Deep效果好
- 通用的多类分类器
	- one-hot 向量 y=[1 0 0]T   [0 1 0]T   [0 0 1]T
	- 回归是连续输出，分类要再加一个softmax归一化 e(x) / ∑e
	- 损失：MSE，交叉熵，分类问题中更多会用到交叉熵，因为MSE有时候可能会卡住，周围梯度都相同
- ### 第八章 人工神经网络
- 生物神经网络
- 人工神经网络
- 神经元与神经网络
	- 工作状态：兴奋，抑制
	- 记忆与遗忘
- BP神经网络及其学习方法
	- θ阈值
	- ReLU，Leaky ReLU
- 人工神经网络性能三大要素
- 感知机模型：针对单个神经元模型
- 分类
	- 异或分类不是纯线性的，没有办法用一条直线做到
- 全连接
	- 两层or三层
	- 连接是不跨层的
- 偏置量，哑结点
-
-
- 2024.04.17
	- Embodied AI 具身智能 ，现在chatGPT是离身的，但具身是根据从环境交互获得更多信息
	- AI两大阵营
		- Hinton，Benjo 危险
		- Yann Lecun，吴恩达 不危险
	- 是激活函数实现了非线性的逼近
	- 损失函数中一定要包含所有的未知参数w和b，因为优化的时候都要进行优化
-
- 2024.04.22
	- Water loo在美国认可高
	- Ian GoodFellow GAN的发明者
	- science美国，nature英国
	- 层数码多之后会出现梯度消失和梯度爆炸
	- SVM：结构风险最小化，升维(二维不可分的可能在三维上可分)，对XXT去建模，隐式而非显式
	- Hinton提出逐层预训练，但是现在已经不使用，Hinton+Alex+Llya->Google，Microsoft，deepmind，Baidu
	- 视频分析：获取上下文信息
-
- 2024.04.24
	- 卷积的好处：
		- 局部感受野，局部聚焦
		- 权值共享：模板在滑动时权值是相同的
	- 卷积的缺点：
		- 容易过训练，参数空间越大越容易在训练集拟合
		- 池化，不可逆，丢失信息
		- 局部会导致长距离，Attention，动态连接
-
-