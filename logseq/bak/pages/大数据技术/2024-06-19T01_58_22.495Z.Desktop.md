- 1，大数据概述
  collapsed:: true
	- 概述
	  collapsed:: true
		- 数据的概念
		  collapsed:: true
			- 数据
			- 数据对象
			- 不同类型的数据：记录数据（关系表，事物），多媒体数据，时空数据（时间，空间），关系图数据
		- 数据的使用：数据清洗，数据管理，数据分析
		- 数据的组织形式：文件，数据库（层次，网状，关系，NoSQL）
		- 数据的价值：会因为不断重组而产生更大的价值
		- 数据爆炸
		  collapsed:: true
			- 存储效率：需要实现低成本的大规模分布式存储
			- 网络效率：需要实现及时响应的用户体验
			- 数据中心：开发绿色节能新一代数据中心，在有效响应大数据处理需求的同时，实现最大化资源利用率、最小化系统能耗的目标
	- 大数据时代
	  collapsed:: true
		- 数据产生方式：运营式系统阶段->用户原创内容阶段->感知式系统阶段
		- 技术支撑：存储设备容量不断增加，CPU计算能力大幅提升，网络带宽不断增加
		- 大数据的发展历程：
			- 萌芽期：数据仓库、专家系统、知识管理系统等
			- 成熟期：并行计算，分布式系统，GFS，MapReduce，Hadoop
			- 大规模应用期：各行各业使用
	- 世界各国的大数据发展战略
	- 大数据的概念
	  collapsed:: true
		- 4V：数据量大，数据类型繁多，处理速度快，价值密度低（商业价值高）
		- 大数据核心挑战：具有多源、异构、信息碎片化的特征
	- 大数据的应用领域
	- 大数据的技术概况
	  collapsed:: true
		- 核心问题：“关联”，发现多源、异构的碎片化信息之间的关联关系
		- 大数据技术体系：
			- 数据采集、数据存储、数据处理、统计分析、智能挖掘、可视化
				- ![](https://api2.mubu.com/v3/document_image/5f4faf78-42e4-44d0-a9ba-ca9c68b22b4b-27042929.jpg)
			- 两大核心技术：分布式存储（GFS\HDFS，BigTable\HBase，NoSQL，NewSQL），分布式处理（MapReduce）
			- 大数据计算模式及其代表产品
				- ![](https://api2.mubu.com/v3/document_image/f839082b-7900-4b9c-9392-002573d35fe4-27042929.jpg)
		- 大数据统计分析技术：条件查询，聚合统计，复杂报表，多维度、多层次统计分析：联机分析处理（OLAP）
		- 实际智能挖掘项目的过程模型：业务理解，数据获取，数据理解，数据预处理，模型建立，测试和评估，部署
			- ![](https://api2.mubu.com/v3/document_image/75a3dadc-b45f-43a0-ba37-ce5d6badb732-27042929.jpg)
		- 数据挖掘
			- 多学科融合
			- 数据挖掘任务：多维概念描述（特征化和区分），关联规则挖掘，分类和回归预测，聚类分析，离群点检测，推荐系统（协同过滤），趋势和演变分析
	- 大数据产业
	  collapsed:: true
		- IT基础设施层，数据源层，数据管理层，数据分析层，数据平台层，数据应用层
		- ![](https://api2.mubu.com/v3/document_image/f50a5f2d-535c-4cb6-8765-d849d5c31c6f-27042929.jpg)
	- 大数据的学习资源
- 2，大数据与其他技术之间的关系
  collapsed:: true
	- 云计算
	  collapsed:: true
		- 云计算概念：通过网络、以服务的方式，为千家万户提供非常廉价的IT资源
			- 初期零成本，瞬时可获得
			- 后期免维护，使用成本低
			- 在供水量方面“予取予求”
		- 云计算特点：超大规模计算、虚拟化、高可靠性和安全性、通用性、动态扩展性、按需服务、降低成本
		- 云计算服务模式和类型
			- 最终用户，SaaS(Software as a Service)：从一个集中的系统部署软件，使之在一台本地计算机上(或从云中远程地)运行的一个模型。由于是计量服务，SaaS 允许出租一个应用程序，并计时收费
			- 应用开发者，PaaS(Platform as a Service)：包括操作系统和围绕特定应用的必需的服务
			- 运维人员，IaaS(Infrastructure as a Service)：将基础设施(计算资源和存储)作为服务出租
		- 云计算数据中心
			- 数据中心建设在地质结构稳定，气候凉爽，电力资源丰富的地方
			- 数据中心能耗非常大
		- 云计算的应用：政务云，教育云，中小企业云，医疗云
		- 云计算产业
			- ![](https://api2.mubu.com/v3/document_image/d9bd0bf5-ab9a-448d-91cb-7305ac5d9a8a-27042929.jpg)
	- 物联网
	  collapsed:: true
		- 物联网概念IoT(The Internet of Things)：物物相连的互联网，是互联网的延伸
		- 物联网层次架构
			- ![](https://api2.mubu.com/v3/document_image/b902306c-d997-40e8-a415-65e036d445d0-27042929.jpg)
		- 物联网的关键技术：识别和感知技术（二维码、RFID、传感器等）、网络与通信技术、数据挖掘与融合技术等
		- 物联网的应用：智能交通，智慧医疗，智能家居，环保监测，智能安防，智能物流，智能电网，智慧农业，智能工业，智慧城市
		- 物联网的应用与产业
			- 核心感应器件提供商，感知层末端设备提供商，网络提供商，软件与行业解决方案提供商，系统集成商，运营及服务提供商![](https://api2.mubu.com/v3/document_image/ef90c988-c4ab-4c9e-9bf6-c4b6834c2599-27042929.jpg)
	- 大数据与云计算、物联网的关系
	  collapsed:: true
		- 云计算与大数据：云计算为大数据提供了技术基础，大数据为云计算提供用武之地
		- 大数据与物联网：物联网是大数据的重要来源，大数据技术为物联网数据分析提供支撑
		- 物联网与云计算：云计算为物联网提供海量数据存储能力，物联网为云计算技术提供了广阔的应用空间
	- 大数据与人工智能
	  collapsed:: true
		- 人工智能概念：Artificial Intelligence，英文缩写为AI，是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学
		- 人工智能学科：机器人，语言识别，图像识别，自然语言处理，专家系统
		- 人工智能关键技术：机器学习，知识图谱，自然语言处理，人机交互，计算机视觉，生物特征识别，VR/AR
		- 人工智能的应用：智能制造，智能家居，智能金融，智能交通，智能安防，智能医疗，智能物流，智能零售
		- 人工智能产业：智能基础设施建设，智能信息及数据，智能技术服务，智能产品
		- 大数据与人工智能的关系
			- 联系
				- 人工智能需要数据来建立其智能，特别是机器学习
				- 大数据技术为人工智能提供了强大的存储能力和计算能力
			- 区别
				- 人工智能与大数据也存在着明显的区别，人工智能是一种计算形式，而大数据是一种传统计算，它不会根据结果采取行动，只是寻找结果
				- 二者要达成的目标和实现目标的手段不同
	- 大数据与区块链
	  collapsed:: true
		- 区块链原理
			- 初衷：在数字世界中如何创建一个无须中介或者说去中心化的数字现金
			- 原理
				- 将总账本拆分成区块
				- 在每个区块上，增加区块头，其中记录了父区块的哈希值
				- 通过每个区块存储父区块的哈希值，把所有区块按照顺序组织起来，形成区块链
				- 比特币和区块链的本质：就是一个人人可见的大账本，只记录交易
				- 核心技术：通过密码学+数据结构，保证账本记录不被篡改
				- 核心功能：创造信任。法币依靠政府公信力，比特币依靠技术
			- 交易安全
				- 进行交易需要账号和密码，对应公钥和私钥
					- 私钥：一串256位二进制数字。
					- 地址：由私钥转化而成，地址不能反推私钥
					- 地址即身份，代表比特币世界的ID
					- 一个地址（私钥）产生以后，只有进入区块链账本，才被大家知道
				- 数字签名技术
					- 张三通过签名函数Sign()，使用自己的私钥对本次交易进行签名
					- 任何人都可以通过验证函数Verify()，来验证此次签名是否由持有张三私钥的张三本人发出的，是就返回True，反之返回False![](https://api2.mubu.com/v3/document_image/df0b4f07-05a5-413f-94e5-835b8e626ebb-27042929.jpg)
			- 去中心化：P2P网络
				- 中心化记账的缺点：拒绝服务攻击，厌倦后停止服务，中心机构易被攻击
				- 人人都可以记账，每个人都可以保留完整账本.任何人都可以下载开源程序,参与P2P网络，监听全世界发送的交易，成为记账节点，参与记账
				- 去中心化记账特点
					- 每隔10分钟产生一个区块，但是，并不是所有在这10分钟内的交易都会被记录
					- 获得记账权的记账节点会获得50个比特币的奖励每隔21万个区块（大约4年）以后，奖励减半。总量约2100万枚，预计到2040年可以开采完
					- 记录一个区块的奖励,也是比特币的唯一发行方式
		- 区块链技术全貌
			- 数据结构+哈希函数：保证账本不能被篡改
			- 数字签名技术：保证只有自己才能动自己的账户
			- P2P网络和POW共识：保证去中心化运作方式
		- 区块链定义与应用
			- 区块链区组成三要素
				- 交易：一次操作，导致账本状态的一次改变，如添加一条记录
				- 区块：记录一段时间内发生的交易和状态结果，是对当前账本状态的一次共识
				- 链：由一个个区块按照发生顺序串联而成，是整个状态变化的日志记录
			- 定义与应用：从科技层面来看，区块链涉及数学、密码学、互联网和计算机编程等很多科学技术问题；从应用视角来看，区块链是一个分布式的共享账本和数据库，具有去中心化、不可篡改、全程留痕、可追溯、集体维护、公开透明等特点
		- 大数据与区块链的关系
			- 数据量
				- 区块链技术是分布式数据存储、点对点传输、共识机制、加密算法等计算机技术的新型应用模式，区块链处理的数据量更小，是细致的处理方式
				- 大数据管理的是海量数据，要求广度和数量，处理方式上也会更粗糙
			- 结构化和非结构化
				- 区块链是结构定义严谨的块，通过指针组成的链，是典型的结构化数据
				- 大数据需要处理的更多是非结构化数据
			- 独立和整合
				- 区块链系统为保证安全性，信息是相对独立的
				- 大数据的重点是信息的整合分析
			- 直接和间接
				- 区块链是一个分布式账本，本质上是一个数据库
				- 大数据指对数据的深度分析和挖掘，是一种间接的数据
			- 联系
				- 区块链使大数据极大降低信用成本
				- 区块链是构建大数据时代信任基石
				- 区块链是促进大数据价值流通管道
			- 区别
				- CAP(Consistency一致性, Availability可用性, Tolerance of Network Partition)理论，大数据通常选择实现AP，区块链则选择实现CP
				- 网络：大数据基于基础网络，区块链底层基础设施则是基于P2P网络
				- 价值来源：对于大数据而言数据是信息,需要从数据中提炼得到价值。对于区块链而言数据是资产，是价值的传承
				- 计算模式：在大数据的场景中是把一件事情分给多人做,如在MapReduce计算框架中，一个大型任务会被分解成很多子任务。在区块链的场景中是让多个人重复做一件事情，比如，P2P网络中的很多个节点同时记录一笔交易
- 3.1，分布式文件处理系统HDFS
  collapsed:: true
	- 分布式文件系统
	  collapsed:: true
		- 计算机集群结构：文件分布存储到多个计算节点上，降低了硬件上的开销
		- 分布式文件系统的结构
		  collapsed:: true
			- 主节点(Master Node)/名称节点(NameNode)
			- 从节点(Slave Node)/数据节点(DataNode)
			- ![](https://api2.mubu.com/v3/document_image/41443f9e-20cc-458b-bfdf-54fd2bc5fb67-27042929.jpg){:height 230, :width 398}
	- HDFS简介
	  collapsed:: true
		- 目标：兼容廉价的硬件设备，流数据读写，大数据集，简单的文件模型，强大的跨平台兼容性
		- 局限性：不适合低延迟数据访问，无法高效存储大量小文件，不支持多用户写入及任意修改文件
		- 块：以块为单位存储，一个块64MB
			- 支持大规模文件存储，文件的大小不会受到单个节点的存储容量的限制
			- 简化系统设计
				- 简化存储管理：文件块大小是固定的，这样就可以很容易计算出一个节点可以存储多少文件块
				- 方便元数据的管理：元数据不需要和文件块一起存储，可以由其他系统负责管理元数据
			- 适合数据备份
				- 每个文件块都可以冗余存储到多个节点上，大大提高了系统的容错性和可用性
		- 名称节点和数据节点
			- NameNode：存储元数据，元数据保存在内存中，保存文件、block、 datanode之间的映射关系
			- DataNode：存储文件内容，文件内容保存在磁盘，维护了block id到datanode本地文件的映射关系
	- HDFS相关概念
	  collapsed:: true
		- 名称节点的数据结构
			- FsImage用于维护文件系统树以及文件树中所有的文件和文件夹的元数据
			- 操作日志文件EditLog中记录了所有针对文件的创建、删除、重命名等操作
			- 名称节点记录了每个文件中各个块所在的数据节点的位置信息![](https://api2.mubu.com/v3/document_image/dcf085b6-c5e7-4971-acf7-7b25eb6ebaa9-27042929.jpg)
		- FsImage文件
			- FsImage文件包含文件系统中所有目录和文件inode的序列化形式
			- 每个inode是一个文件或目录的元数据的内部表示，并包含此类信息：文件的复制等级、修改和访问时间、访问权限、块大小以及组成文件的块。对于目录，则存储修改时间、权限和配额元数据
			- FsImage文件没有记录块存储在哪个数据节点。而是由名称节点把这些映射保留在内存中，当数据节点加入HDFS集群时，数据节点会把自己所包含的块列表告知给名称节点，此后会定期执行这种告知操作，以确保名称节点的块映射是最新的
		- 名称节点的启动
			- 先将FsImage加载到内存中，再执行EditLog中的各种操作，使内存中的元数据和实际同步，元数据支持客户端读操作
			- 一旦在内存中成功建立文件系统元数据的映射，则创建一个新的FsImage文件和一个空的EditLog文件
			- 名称节点起来之后，HDFS中的更新操作会重新写到EditLog文件中，因为FsImage文件一般都很大，导致系统运行很慢
			- 每次执行写操作之后，且在向客户端发送成功代码之前，edits文件都需要同步更新
		- 名称节点运行期间EditLog不断变大
			- 当名称节点重启的时候，名称节点需要先将FsImage里面的所有内容映像到内存中，然后再一条一条地执行EditLog中的记录，当EditLog文件非常大的时候，会导致名称节点启动操作非常慢，这段时间内HDFS系统处于安全模式，一直无法对外提供写操作，影响了用户的使用
			- SecondaryNameNode第二名称节点，是HDFS架构中的一个组成部分，它是用来保存名称节点中对HDFS元数据信息的备份，并减少名称节点重启的时间。SecondaryNameNode一般是单独运行在一台机器上
			- SecondaryNameNode的工作![](https://api2.mubu.com/v3/document_image/ce9582bf-cc06-45f3-9a1b-70754c6ca672-27042929.jpg)
		- 数据节点
			- 数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，会根据客户端或者是名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块的列表
			- 每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中
	- HDFS体系结构
	  collapsed:: true
		- HDFS体系结构概述
			- 主从模型结构：一个名称节点，若干个数据节点
				- 名称节点作为中心服务器，负责管理文件系统的命名空间及客户端对文件的访问
				- 数据节点一般是一个节点运行一个数据节点进程，负责处理文件系统客户端的读/写请求，在名称节点的统一调度下进行数据块的创建、删除和复制等操作。每个数据节点的数据实际上是保存在本地Linux文件系统中的![](https://api2.mubu.com/v3/document_image/a67cda1a-6a6f-4794-9b5a-251675a1b03b-27042929.jpg)
		- HDFS命名空间管理
			- HDFS的命名空间包含目录、文件和块
			- 在HDFS1.0体系结构中，在整个HDFS集群中只有一个命名空间，并且只有唯一一个名称节点，该节点负责对这个命名空间进行管理
			- HDFS使用的是传统的分级文件体系，因此，用户可以像使用普通文件系统一样，创建、删除目录和文件，在目录间转移文件，重命名文件等
		- 通信协议
			- 所有的HDFS通信协议都是构建在TCP/IP协议基础之上的
			- 客户端通过一个可配置的端口向名称节点主动发起TCP连接，并使用客户端协议与名称节点进行交互
			- 名称节点和数据节点之间则使用数据节点协议进行交互
			- 客户端与数据节点的交互是通过RPC（Remote Procedure Call）来实现的。在设计上，名称节点不会主动发起RPC，而是响应来自客户端和数据节点的RPC请求
		- 客户端
			- 客户端是用户操作HDFS最常用的方式，HDFS在部署时都提供了客户端
			- HDFS客户端是一个库，暴露了HDFS文件系统接口，这些接口隐藏了HDFS实现中的大部分复杂性
			- 严格来说，客户端并不算是HDFS的一部分
			- 客户端可以支持打开、读取、写入等常见的操作，并且提供了类似Shell的命令行方式来访问HDFS中的数据
			- 此外，HDFS也提供了Java API，作为应用程序访问文件系统的客户端编程接口
		- 局限性：只设置唯一一个名称节点
			- 命名空间的限制：名称节点是保存在内存中的，因此，名称节点能够容纳的对象（文件、块）的个数会受到内存空间大小的限制
			- 性能的瓶颈：整个分布式文件系统的吞吐量，受限于单个名称节点的吞吐量
			- 隔离问题：由于集群中只有一个名称节点，只有一个命名空间，因此，无法对不同应用程序进行隔离
			- 集群的可用性：一旦这个唯一的名称节点发生故障，会导致整个集群变得不可用
	- HDFS存储原理
	  collapsed:: true
		- 冗余数据保存：多副本方式
		  collapsed:: true
			- 通常一个数据块的多个副本会被分布到不同的数据节点上
			- 优点：加快数据传输速度，容易检查数据错误，保证数据可靠性
		- 数据存取策略
		  collapsed:: true
			- 数据存放
				- 第一个副本：放置在上传文件的数据节点；如果是集群外提交，则随机挑选一台磁盘不太满、CPU不太忙的节点
				- 第二个副本：放置在与第一个副本不同的机架的节点上
				- 第三个副本：与第一个副本相同机架的其他节点上
				- 更多副本：随机节点
			- 数据读取
				- HDFS提供了一个API可以确定一个数据节点所属的机架ID，客户端也可以调用API获取自己所属的机架ID
				- 当客户端读取数据时，从名称节点获得数据块不同副本的存放位置列表，列表中包含了副本所在的数据节点，可以调用API来确定客户端和这些数据节点所属的机架ID，当发现某个数据块副本对应的机架ID和客户端对应的机架ID相同时，就优先选择该副本读取数据，如果没有发现，就随机选择一个副本读取数据
		- 数据错误与恢复：（硬件出错看作一种常态，而不是异常）
		  collapsed:: true
			- 名称节点出错
				- HDFS设置了备份机制，把这些核心文件同步复制到备份服务器SecondaryNameNode上。当名称节点出错时，就可以根据备份服务器SecondaryNameNode中的FsImage和Editlog数据进行恢复。
			- 数据节点出错
				- 每个数据节点会定期向名称节点发送“心跳”信息，向名称节点报告自己的状态
				- 当数据节点发生故障，或者网络发生断网时，名称节点就无法收到来自一些数据节点的心跳信息，这时，这些数据节点就会被标记为“宕机”，节点上面的所有数据都会被标记为“不可读”，名称节点不会再给它们发送任何I/O请求
				- 这时，有可能出现一种情形，即由于一些数据节点的不可用，会导致一些数据块的副本数量小于冗余因子，名称节点会定期检查这种情况，一旦发现某个数据块的副本数量小于冗余因子，就会启动数据冗余复制，为它生成新的副本
				- HDFS和其它分布式文件系统的最大区别就是可以调整冗余数据的位置
		- 数据出错
		  collapsed:: true
			- 网络传输和磁盘错误等因素，都会造成数据错误
			- 客户端在读取到数据后，会采用md5和sha1对数据块进行校验，以确定读取到正确的数据
			- 在文件被创建时，客户端就会对每一个文件块进行信息摘录，并把这些信息写入到同一个路径的隐藏文件里面
			- 当客户端读取文件的时候，会先读取该信息文件，然后，利用该信息文件对每个读取的数据块进行校验，如果校验出错，客户端就会请求到另外一个数据节点读取该文件块，并且向名称节点报告这个文件块有错误，名称节点会定期检查并且重新复制这个块
	- HDFS数据读写过程
	  collapsed:: true
		- FileSystem：通用文件系统的抽象基类
		  collapsed:: true
			- 被分布式文件系统继承，所有可能使用Hadoop文件系统的代码，都要使用这个类
			- DistributedFileSystem就是FileSystem在HDFS文件系统中的具体实现
			- FileSystem的open()方法返回的是一个输入流FSDataInputStream对象，在HDFS文件系统中，具体的输入流就是DFSInputStream
			- FileSystem中的create()方法返回的是一个输出流FSDataOutputStream对象，在HDFS文件系统中，具体的输出流就是DFSOutputStream![](https://api2.mubu.com/v3/document_image/e7cb7b08-5595-4398-a3fa-d8c78225c354-27042929.jpg)
		- 读数据的过程
		  collapsed:: true
			- ![](https://api2.mubu.com/v3/document_image/be5155e4-433b-4707-be59-c5cc2c413032-27042929.jpg)
			- ![](https://api2.mubu.com/v3/document_image/d0f1b81d-5d0b-4753-adfa-a3b9ae9b8923-27042929.jpg)
		- 写数据的过程
		  collapsed:: true
			- ![](https://api2.mubu.com/v3/document_image/26871660-7332-405b-90d2-1286e93b2959-27042929.jpg)
			- ![](https://api2.mubu.com/v3/document_image/8486f0ec-33e2-41ad-a0b7-c8b5c930510e-27042929.jpg)
- 3.2，分布式数据库HBase
	- 概述
	  collapsed:: true
		- BigTable
		  collapsed:: true
			- BigTable是一个分布式存储系统，MapReduce处理数据，GFS作为底层数据存储，Chubby提供协同服务管理，可扩展性、高性能和高可用性
			- BigTable起初用于解决典型的互联网搜索问题
		- HBase简介
		  collapsed:: true
			- HBase是一个高可靠、高性能、面向列、可伸缩的分布式数据库，是谷歌BigTable的开源实现，主要用来存储非结构化和半结构化的松散数据
			- 目标：
			  collapsed:: true
				- 处理非常庞大的表，可以通过水平扩展的方式
				- ![](https://api2.mubu.com/v3/document_image/f8dde4c1-a820-4298-b7c9-7410a4308451-27042929.jpg){:height 253, :width 406}
			- HBase和BigTable
			  collapsed:: true
				- ![](https://api2.mubu.com/v3/document_image/38fb5628-bcea-4c1c-a8b1-e4926af2cc7b-27042929.jpg){:height 133, :width 461}
			- 产生原因
			  collapsed:: true
				- Hadoop无法满足大规模数据实时处理应用的需求
				- HDFS面向批量访问模式，不是随机访问模式
				- 传统的通用关系型数据库无法应对在数据规模剧增时导致的系统扩展性和性能问题
				- 传统关系数据库在数据结构变化时一般需要停机维护；空列浪费存储空间
				- 业界出现了一类面向半结构化数据存储和处理的高可扩展、低写入/查询延迟的系统，例如，键值数据库、文档数据库和列族数据库（如BigTable和HBase等）
		- HBase与传统关系数据库的对比分析
		  collapsed:: true
			- 数据类型：关系数据库采用关系模型，具有丰富的数据类型和存储方式，HBase则采用了更加简单的数据模型，它把数据存储为未经解释的字符串
			- 数据操作：关系数据库中包含了丰富的操作，其中会涉及复杂的多表连接。HBase操作则不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空等，因为HBase在设计上就避免了复杂的表和表之间的关系
			- 存储模式：关系数据库是基于行模式存储的。HBase是基于列存储的，每个列族都由几个文件保存，不同列族的文件是分离的
			- 数据索引：关系数据库通常可以针对不同列构建复杂的多个索引，以提高数据访问性能。HBase只有一个索引——行键，通过巧妙的设计，HBase中的所有访问方法，或者通过行键访问，或者通过行键扫描，从而使得整个系统不会慢下来
			- 数据维护：在关系数据库中，更新操作会用最新的当前值去替换记录中原来的旧值，旧值被覆盖后就不会存在。而在HBase中执行更新操作时不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留
			- 可伸缩性：关系数据库很难实现横向扩展，纵向扩展的空间也比较有限。相反，HBase和BigTable这些分布式数据库就是为了实现灵活的水平扩展而开发的，能够轻易地通过在集群中增加或者减少硬件数量来实现性能的伸缩
	- HBase访问接口
	  collapsed:: true
		- ![](https://api2.mubu.com/v3/document_image/9ef3a6b6-6fb2-4192-8aff-302be142ef43-27042929.jpg){:height 279, :width 471}
	- HBase数据模型
	  collapsed:: true
		- 数据模型概述
		  collapsed:: true
			- HBase是一个稀疏、多维度、排序的映射表，这张表的索引是行键、列族、列限定符和时间戳
			- 每个值是一个未经解释的字符串，没有数据类型
			- 用户在表中存储数据，每一行都有一个可排序的行键和任意多的列
			- 表在水平方向由一个或者多个列族组成，一个列族中可以包含任意多个列，同一个列族里面的数据存储在一起
			- 列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行数据类型转换
			- HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留（这是和HDFS只允许追加不允许修改的特性相关的）
		- 数据模型相关概念
		  collapsed:: true
			- 表：HBase采用表来组织数据，表由行和列组成，列划分为若干个列族
			- 行：每个HBase表都由若干行组成，每个行由行键（row key）来标识
			- 列族：一个HBase表被分组成许多“列族”（Column Family）的集合，它是基本的访问控制单元
			- 列限定符：列族里的数据通过列限定符（或列）来定位
			- 单元格：在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，总被视为字节数组byte[]
			- 时间戳：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引
			- ![](https://api2.mubu.com/v3/document_image/ae34e6b9-3030-46bd-a0e9-23a130808b5c-27042929.jpg)
		- 数据坐标
		  collapsed:: true
			- HBase中需要根据行键、列族、列限定符和时间戳来确定一个单元格，因此，可以视为一个“四维坐标”，即[行键, 列族, 列限定符, 时间戳]
		- 概念视图
		- 物理视图
		- 面向列的存储
	- HBase的实现原理
	  collapsed:: true
		- HBase功能组件
		  collapsed:: true
			- 三个主要的功能组件：
			  collapsed:: true
				- 库函数：链接到每个客户端
				- 一个Master主服务器：负责管理和维护HBase表的分区信息，维护Region服务器列表，分配Region，负载均衡
				- 许多个Region服务器：负责存储和维护分配给自己的Region，处理来自客户端的读写请求
			- 客户端
			  collapsed:: true
				- 客户端并不是直接从Master主服务器上读取数据，而是在获得Region的存储位置信息后，直接从Region服务器上读取数据
				- 客户端并不依赖Master，而是通过Zookeeper来获得Region位置信息，大多数客户端甚至从来不和Master通信，这种设计方式使得Master负载很小
		- 表和Region
		  collapsed:: true
			- 拆分
			  collapsed:: true
				- 一个HBase表被划分成多个Region
				- 一个Region会分裂成多个新的Region
				- 开始只有一个Region，后来不断分裂
				- Region拆分操作非常快，接近瞬间，因为拆分之后的Region读取的仍然是原存储文件，直到“合并”过程把存储文件异步地写到独立的文件之后，才会读取新文件
			- Region与Region服务器的分配
			  collapsed:: true
				- 每个Region默认大小是100MB到200MB：每个Region的最佳大小取决于单台服务器的有效处理能力，目前每个Region最佳大小建议1GB-2GB
				- 同一个Region**不会**被分拆到多个Region服务器
				- 每个Region服务器存储10-1000个Region
		- Region的定位
		  collapsed:: true
			- 元数据表，又名.META.表，存储了Region和Region服务器的映射关系
			- 当HBase表很大时， .META.表也会被分裂成多个Region
			- 根数据表，又名-ROOT-表，记录所有元数据的具体位置，-ROOT-表只有唯一一个Region，名字是在程序中被写死的
			- Zookeeper文件记录了-ROOT-表的位置
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406172240303.png){:height 178, :width 408}
			- 为了加快访问速度，.META.表的全部Region都会被保存在内存中
			- 客户端访问数据时的“三级寻址”
			  collapsed:: true
				- 为了加速寻址，客户端会缓存位置信息，同时，需要解决缓存失效问题
				- 寻址过程客户端只需要询问Zookeeper服务器，不需要连接Master服务器
	- HBase运行机制
		- HBase系统架构
		  collapsed:: true
			- 客户端
			  collapsed:: true
				- 客户端包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程
			- Zookeeper服务器
			  collapsed:: true
				- Zookeeper可以帮助选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题 Zookeeper是一个很好的集群管理工具，被大量用于分布式计算，提供配置维护、域名服务、分布式同步、组服务等。
			- Master：主服务器Master主要负责表和Region的管理工作
			  collapsed:: true
				- 管理用户对表的增加、删除、修改、查询等操作
				- 实现不同Region服务器之间的负载均衡
				- 在Region分裂或合并后，负责重新调整Region的分布
				- 对发生故障失效的Region服务器上的Region进行迁移
			- Region服务器
			  collapsed:: true
				- Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求
		- Region服务器工作原理
		  collapsed:: true
			- 图示
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406172301473.png){:height 279, :width 295}
			- 用户读写数据过程
			  collapsed:: true
				- 用户写入数据时，被分配到相应Region服务器去执行
				- 用户数据首先被写入到MemStore和Hlog中
				- 只有当操作写入Hlog之后，commit()调用才会将其返回给客户端
				- 当用户读取数据时，Region服务器会首先访问MemStore缓存，如果找不到，再去磁盘上面的StoreFile中寻找
			- 缓存的刷新
			  collapsed:: true
				- 系统会周期性地把MemStore缓存里的内容刷写到磁盘的StoreFile文件中，清空缓存，并在Hlog里面写入一个标记
				- 每次刷写都生成一个新的StoreFile文件，因此，每个Store包含多个 StoreFile文件
				- 每个Region服务器都有一个自己的HLog 文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入MemStore，再刷写到StoreFile，最后删除旧的Hlog文件，开始为用户提供服务
			- StoreFile的合并
			  collapsed:: true
				- 每次刷写都生成一个新的StoreFile，数量太多，影响查找速度
				- 调用Store.compact()把多个合并成一个
				- 合并操作比较耗费资源，只有数量达到一个阈值才启动合并
		- Store工作原理
		  collapsed:: true
			- Store是Region服务器的核心，多个StoreFile合并成一个，单个StoreFile过大时，又触发分裂操作，1个父Region被分裂成两个子Region
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406172303822.png){:height 126, :width 423}
		- HLog工作原理
			- 分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复
			- HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log）
			- 用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘
			- Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master
			- Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录
			- 系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region服务器中，并把与该Region对象相关的HLog日志记录也发送给相应的Region服务器
			- Region服务器领取到分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复
			- 共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志
	- HBase应用方案
-
- ![1.1-大数据技术概述.pdf](../assets/1.1-大数据技术概述_1717942657677_0.pdf)
-
-
- ![2-大数据与其他新技术之间的关系.pdf](../assets/2-大数据与其他新技术之间的关系_1717942679657_0.pdf)
-
- ![3大数据的获取、存储与并行计算.pdf](../assets/3大数据的获取、存储与并行计算_1717942736931_0.pdf)
-
- ![3.1-大数据分布式文件处理系统HDFS.pdf](../assets/3.1-大数据分布式文件处理系统HDFS_1717942646743_0.pdf)
-
-
- ![3.2 分布式数据库HBase.pdf](../assets/3.2_分布式数据库HBase_1715760379160_0.pdf)
-
- ![3.3-MapReduce.pdf](../assets/3.3-MapReduce_1717942706182_0.pdf)
-
-
- ![3.4-Hive.pdf](../assets/3.4-Hive_1716797474656_0.pdf)
- Hive
  collapsed:: true
	- 概述
	  collapsed:: true
		- Hive是一个构建于Hadoop顶层的数据仓库工具（分钟级别）
		- 数据库vs数据仓库
		  collapsed:: true
			- 数据库：存数据，检索数据，但是数据太离散
			  数据仓库：静态的，专门面向主题的，eg关注某个地区成本经营分析，基于数据库
		- Hive vs 传统数据仓库
		  collapsed:: true
			- 传统的数据仓库能同时支持数据的存储和处理分析，Hive本身并不支持数据存储和处理，提供了一种编程的语言
			- Hive依赖HDFS存储数据，MapReduce处理数据
		- Hive功能
		  collapsed:: true
			- 定义了简单的类似SQL的查询语言HiveQL，HQL可以运行MapReduce任务
			- 支持类似SQL的接口，容易移植
			- 采用批处理方式处理海量数据，因为数据仓库存储的是静态数据
			- Hive是一个可以提供有效合理直观组织和使用数据的分析工具
		- Hive与Hadoop生态系统中其他组件的关系
		  collapsed:: true
			- 图示
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151140057.png){:height 222, :width 425}
			- Hive依赖于HDFS 存储数据
			  collapsed:: true
				- HDFS作为高可靠性的底层存储，用来存储海量数据
			- Hive依赖于MapReduce 处理数据
			  collapsed:: true
				- MapReduce对这些海量数据进行处理，实现高性能计算，用HiveQL语句编写的处理逻辑最终均要转化为MapReduce任务来运行
			- Pig可以作为Hive的替代工具
			  collapsed:: true
				- pig是一种数据流语言和运行环境，适合用于Hadoop和MapReduce平台上查询半结构化数据集。常用于ETL过程的一部分，即将外部数据装载到 Hadoop集群中，然后转换为用户期待的数据格式
			- HBase 提供数据的实时访问
			  collapsed:: true
				- HBase一个面向列的、分布式的、可伸缩的数据库，它可以提供数据的实时访问功能，而Hive只能处理静态数据，主要是BI报表数据，所以HBase与 Hive的功能是互补的，它实现了Hive不能提供功能
		- Hive与传统数据库的对比分析
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151232779.png)
		- Hive在企业中的部署案例
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151233671.png){:height 235, :width 414}
	- Hive系统架构
	  collapsed:: true
		- Hive对外访问接口概述
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151235930.png){:height 251, :width 345}
		- Hive HA基本原理
		  collapsed:: true
			- Hive High Availability(高可用性Hive解决方案)
			- Hive很多时候会表现出不稳定性
	- Hive工作原理
	  collapsed:: true
		- SQL语句转换成MapReduce的基本原理
		- Hive中SQL查询转换成MapReduce作业的过程
		  collapsed:: true
			- 查询的转换
			  collapsed:: true
				- 驱动模块接收该命令或查询编译器
				- 对该命令或查询进行解析编译
				- 由优化器对欧查询进行优化计算
				- 该命令或查询通过执行器进行执行
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151247387.png){:height 322, :width 232}
			- 过程说明
			  collapsed:: true
				- 当启动MapReduce程序时，Hive本身是不会生成MapReduce算法程序的
				- 需要通过一个表示“job执行计划”的XML文件驱动执行内置的、原生的 Mapper和Reducer模块
				- Hive通过和JobTracker通信来初始化MapReduce任务，不必直接部署在 JobTracker所在的管理节点上执行
				- 通常在大型集群上，会有专门的网关机来部署Hive工具。网关机的作用主要是远程操作和管理节点上的JobTracker通信，来执行任务
				- 数据文件通常存储在HDFS上，HDFS由NameNode节点管理
	- Impala：用于即时交互性查询
	  collapsed:: true
		- Impala简介
		  collapsed:: true
			- Cloudera公司开发的新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase上的PB级大数据
			- Impala最开始是参照 Dremel系统进行设计的，Impala的目的不在于替换现有的MapReduce工具，而是提供一个统一的平台用于实时查询
			- Impala的运行需要依赖于Hive的元数据
			- Impala和Hive采用相同的SQL语法ODBC驱动程序和用户接口
			- Impala采用了与商用并行关系数据库类似的分布式查询引擎，可以直接与HDFS和HBase进行交互查询
			- Impala和Hive采用相同的SQL语法ODBC驱动程序和用户接口
		- Impala系统架构
		  collapsed:: true
			- 整体架构
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151351416.png){:height 285, :width 514}
				- 使用Hive的Metastore，自己实现了与传统的分布式并行数据库类似的组件，最终执行类似SQL的语句
				- 没有所谓的master和slave，所有的计算节点都是等价的
				- impalad的三个组件：Query Coordinator查询协调器分解任务，和namenode交换信息，解析树；规划片段；Query Planner协调其他impalad协调其他任务；queryExcutor将最终结果汇总
			- 三部分
			  collapsed:: true
				- Impalad
					- Impala查询过程
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151357204.png){:height 210, :width 342}
					- Impalad：计算进程，核心，和hdfs的namenode或者hbase交换，region上找到想要的数据，执行任务，结果返回其他impalad做其他任务，既是任务发布者也是任务执行者
				- State Store：资源调度器，创建进程，跟踪所有执行的进程，控制impalad
				- CLI：CLI实现命令行和用户接口
		- Impala与Hive的比较
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151358813.png){:height 259, :width 316}
			- 直接与HDFS和Hbase交互，底层并不是mapreduce，不处理长时间处理任务，而hive底层是MapReduce
			- 不同点：
			  collapsed:: true
				- Hive批处理，Impala实时交互
				- 执行的底层逻辑：Hive依赖于底层MapReduce计算框架；Impala将所有任务变成一个执行树
				- Hive内存放不下找外存放；Impala不会利用外存，只使用当前内存
			- 相同点：
			  collapsed:: true
				- 都是用相同的原数据
				- 底层架构hdfs，hbase管理，hadoop
				- 执行：类sql，源操作，先根遍历排序
	- Hive编程实践
	  collapsed:: true
		- Hive的数据类型
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151400194.png)
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151400626.png)
		- Hive基本操作
		  collapsed:: true
			- hive> create database hive;
			- hive> create database if not exists hive;
			- hive> use hive;
			- hive>create table if not exists hive.usr(id bigint,name string,age int)>location ‘/usr/local/hive/warehouse/hive/usr’;
			- hive>create external table if not exists hive.usr(id bigint,name string,age int)>row format delimited fields terminated by ',' location ‘/usr/local/data’;
			- hive>create table hive.usr(id bigint,name string,age int) partition by(sex boolean);
			- hive> use hive;
			- hive>create table if not exists usr1 like usr;
			- hive>create view little_usr as select id,age from usr;
-
-
- ![3.5-Spark.pdf](../assets/3.5-Spark_1716799066384_0.pdf)
- Spark
  collapsed:: true
	- Spark概述：基于内存计算的大数据分布架构
	  collapsed:: true
		- 简介：
		  collapsed:: true
			- 加州伯克利AMP人机物计算协同
			- apache三大分布式计算系统：Hadoop，Spark，Storm
			- Spark用了十分之一的计算资源比Hadoop快3倍
		- 特点
		  collapsed:: true
			- 速度快：自己的DAG执行引擎
			- 容易使用：Scala，Java，Python，R都支持
			- 通用性：强大技术栈，SQL查询，流式计算框(MapReduce的Shuffle太慢，希望用这套框架代替所有的，
			- 模式多样：单机和集群都可以，和hadoop整合也可以，有自己的文件系统)，机器学习，图算法组件
		- Scala简介
		  collapsed:: true
			- 多范式编程语言，兼容Java
			- 并发性，语法简洁
		- Spark和Hadoop对比
		  collapsed:: true
			- Spark放入内存中，不用再取
			- 怎么保证电脑即使崩溃数据也不会丢失：自己的DAG
	- Spark生态系统
	  collapsed:: true
		- 批量数据处理：hadoop
		- 基于历史数据的交互式查询：Cloudera Impala
		- 基于实时数据流的数据处理：Storm
		- 问题：
			- 不同场景之间输入输出数据无法做到无缝共享，通常需要进行数据格式的转换
			- 不同的软件需要不同的开发和维护团队，带来了较高的使用成本
			- 比较难以对同一个集群中的各个系统进行统一的资源协调和分配
		- Spark提供一套完整的生态系统，基于YARN
		- 组件
			- Spark Core 提供内存计算
			- Spark SQL 提供交互式查询分析
			- Spark Streaming 提供流计算功能
			- MLLib 提供机器学习算法库的组件
			- GraphX 提供图计算
	- Spark运行架构
	  collapsed:: true
		- 基本概念
		  collapsed:: true
			- RDD：弹性分布式数据集，提供了一种高度受限的共享内存模型
			- DAG：反映RDD之间的依赖关系
			- Executor：是运行在工作节点（WorkerNode）的一个进程，负责运行Task
			- Application：用户编写的Spark应用程序，相当于MapReduce中的Job
			- Task：运行在Executor上的工作单元
			- Job：一个Job包含多个RDD及作用于相应RDD上的各种操作
			- Stage：是Job的基本调度单位，一个Job会分为多组Task，每组Task被称为Stage，或者也被称为TaskSet，代表了一组关联的、相互之间没有 Shuffle依赖关系的任务组成的任务集
		- 架构设计
			- Program驱动程序，产生Spark Context管理所有运行的资源，负责资源显示但是不负责资源调度，yarn负责
			- Cluster Manager，相当于Master
			- Worker Node就是Slave，有task和缓存
			- 一个Application就是一个用户程序，由一个Driver和若干个Job构成，一个Job由多个Stage构成，同一个stage里的每条线可以并行执行，MapReduce中所有map都能并行，但是stage之间会有依赖关系，对任务的表达能力更强，能做一些map不能做的东西，spark可以表达顺序
		- 运行基本流程
		  collapsed:: true
			- 构建起基本的运行环境，即由Driver创建一个 SparkContext，可以运行到yarn上去
			- 分配资源，执行Excutor
			- 根据RDD构建DAG图，Scheduler将其解析成不同的Stage，然后把一个个TaskSet提交给底层调度器 TaskScheduler处理；Executor向 SparkContext申请Task，Task Scheduler将Task发放给Executor运行，并提供应用程序代码
			- Task在Executor上运行，把执行结果反馈给TaskScheduler，然后反馈给DAGScheduler，运行完毕后写入数据并释放所有资源
		- RDD运行原理
			- 设计背景
				- 为了机器学习和人工智能设计，中间结果，hdfs会存储复制，这样会有时间损耗，希望
				- 只读，每个RDD有若干分区
			- 运行原理
				- 两类：动作Action和转换Transformation，所有的中间结果都叫转换，只有最后一个叫做Action
				- DAG图：只记住转换的操作，得到DAG拓扑排序，即Lineage血缘关系
			- 特性
				- 高效的容错性：现有容错机制+RDD
				- 中间结果持久化到内存
				- 可以存放Java对象，避免了不必要的对象序列化和反序列化
			- RDD并行化
				- 窄依赖：一个或多个父RDD的分区对应于一个子RDD的分区
				- 宽依赖：一个父RDD的一个分区对应一个子 RDD的多个分区
			- Stage划分
			  collapsed:: true
				- 在DAG中进行反向解析，遇到宽依赖就断开
				- 遇到窄依赖就把当前的RDD加入到Stage中
				- 将窄依赖尽量划分在同一个Stage中，可以实现流水线计算
				- eg
					- A->B宽依赖，不能并行操作，把A单独变为Stage
					- C->D,E->F窄依赖，加进去
					- F->G一个新的分区
				- 两种Stage
					- ShuffleMapStage：不是最终的Stage，在它之后还有其他Stage
					- ResultStage：最终的Stage，没有输出，而是直接产生结果或存储
			- RDD在Spark架构中的运行过程
			  collapsed:: true
				- 创建RDD对象
				- SparkContext负责计算RDD之间的依赖关系，构建DAG
				- DAGScheduler负责把DAG图分解成多个Stage，每个Stage中包含了多个Task，每个Task会被TaskScheduler分发给各个WorkerNode上的Executor去执行
-
- ![3.6-Flink.pdf](../assets/3.6-Flink_1717942721259_0.pdf)
- Flink
  collapsed:: true
	- Flink简介
	  collapsed:: true
		- 主要特性：批流一体化，精密的状态管理，事件时间支持，精确一次的状态一致性保障
		- Flink起源于Stratosphere 项目，Apache软件基金会顶级项目
		- 阿里巴巴、美团、滴滴等使用Flink
	- 为什么选择Flink
	  collapsed:: true
		- 传统数据处理架构
		  collapsed:: true
			- 采用一个中心化的数据库系统，用于存储事务性数据
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151432470.png){:height 176, :width 292}
		- 大数据Lambda架构
		  collapsed:: true
			- 主要包含两层，即批处理层和实时处理层
			- 批处理层中，采用MapReduce、Spark等技术进行批量数据处理
			- 在实时处理层中，则采用Storm、Spark Streaming等技术进行数据的实时处理
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151434365.png)
		- 流处理架构
		  collapsed:: true
			- 为了高效地实现流处理架构，一般需要设置消息传输层和流处理层（如图所示）
			- 消息传输层从各种数据源采集连续事件产生的数据，并传输给订阅了这些数据的应用程序
			- 流处理层会持续地将数据在应用程序和系统间移动，聚合并处理事件，并在本地维持应用程序的状态
			- 流处理架构的核心是使各种应用程序互连在一起的消息队列，消息队列连接应用程序，并作为新的共享数据源，这些消息队列取代了从前的大型集中式数据库
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151449751.png){:height 215, :width 393}
			- 希望
				- 不使用大型集中式数据库，避免数据库不堪重负
				- 将批处理看成流处理的子集，避免多个框架难管理
		- Flink是理想的流计算框架
		  collapsed:: true
			- Flink实现了Google Dataflow流计算模型，是一种兼具高吞吐、低延迟和高性能的实时流计算框架，并且同时支持批处理和流处理。此外，Flink支持高度容错的状态管理，防止状态在计算过程中因为系统异常而出现丢失。因此，Flink就成为了能够满足流处理架构要求的理想的流计算框架。
		- Flink的优势
		  collapsed:: true
			- 同时支持高吞吐、低延迟、高性能
			- 同时支持流处理和批处理
			- 高度灵活的流式窗口
			- 支持有状态计算
			- 具有良好的容错性
			- 具有独立的内存管理
			- 支持迭代和增量迭代
	- Flink应用场景
	  collapsed:: true
		- 事件驱动型应用
		  collapsed:: true
			- 什么是事件驱动型应用
			  collapsed:: true
				- 一类具有状态的应用，建立在有状态流处理应用的基础之上
				- 从一个或多个事件数据流中读取事件，并根据到来的事件做出反应，包括触发计算、状态更新或其他外部动作等
				- 在传统的应用设计基础上进化而来的
				- 数据和计算不是相互独立的层，而是放在一起的，应用只需访问本地（内存或磁盘）即可获取数据
				- 系统容错性是通过定期向远程持久化存储写入检查点来实现的
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151456743.png)
				- 典型的事件驱动型应用包括反欺诈、异常检测、基于规则的报警、业务流程监控、Web 应用（社交网络）
			- 事件驱动型应用的优势
			  collapsed:: true
				- 访问本地数据，而无需查询远程的数据库->吞吐量，延迟可以获得更好的性能
				- 向一个远程的持久化存储周期性地写入检查点，采用异步和增量的方式，检查点对于常规的事件处理的影响是很小的
				- 每个事件驱动型应用都只需要考虑自身的数据，对数据表示方式的改变或者应用的升级，都只需要很少的协调工作
			- Flink是如何支持事件驱动型应用的
			  collapsed:: true
				- Flink提供了丰富的状态操作原语，它可以管理大量的数据，确保精确的一致性
				- 还支持事件时间、高度可定制的窗口逻辑和细粒度的时间控制，实现高级的商业逻辑
				- 复杂事件处理（CEP）类库，用来检测数据流中的模式
				- 保存点是一个一致性的状态镜像，可以作为许多相互兼容的应用的一个初始化点，升级或扩容
		- 数据分析应用
		  collapsed:: true
			- 什么是数据分析应用
			  collapsed:: true
				- 批量：分析作业会从原始数据中提取信息，并得到富有洞见的观察。传统的数据分析通常先对事件进行记录，然后在这个有界的数据集上执行批量查询。为了把最新的数据融入到查询结果中，就必须把这些最新的数据添加到被分析的数据集中，然后重新运行查询。查询的结果会被写入到一个存储系统中，或者形成报表。
				- 流式：一个高级的流处理引擎，可以支持实时的数据分析。这些流处理引擎并非读取有限的数据集，而是获取实时事件流，并连续产生和更新查询结果。这些结果或者被保存到一个外部数据库中，或者作为内部状态被维护。仪表盘应用可以从这个外部的数据库中读取最新的结果，或者直接查询应用的内部状态。
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151506541.png)
				- 典型的数据分析应用包括电信网络质量监控、移动应用中的产品更新及实验评估分析、消费者技术中的实时数据即席分析、大规模图分析等
			- 流式分析应用的优势
			  collapsed:: true
				- 消除了周期性的导入和查询，因而从事件中获取洞察结果的延迟更低，不需要处理输入数据中的人为产生的边界
				- 更加简单的应用架构，把从数据提取到连续结果计算的所有步骤都整合起来，可以依赖底层引擎提供的故障恢复机制
			- Flink是如何支持数据分析应用的
			  collapsed:: true
				- Flink可以同时支持批处理和流处理。Flink提供了一个符合ANSI规范的 SQL接口，它可以为批处理和流处理提供一致的语义
				- 提供了丰富的用户自定义函数，使得用户可以在SQL查询中执行自定义代码，如果需要进一步定制处理逻辑，Flink的 DataStream API和DataSet API提供了更加底层的控制
				- Flink 的 Gelly 库为基于批量数据集的大规模高性能图分析提供了算法和构建模块支持
		- 数据流水线应用
		  collapsed:: true
			- 什么是数据流水线
			  collapsed:: true
				- Extract-transform-load（ETL）是一个在存储系统之间转换和移动数据的常见方法，ETL作业会被周期性地触发，从而把事务型数据库系统中的数据复制到一个分析型数据库或数据仓库中
				- 数据流水线可以转换、清洗数据，或者把数据从一个存储系统转移到另一个存储系统中，但是它们是以一种连续的流模式来执行的，而不是周期性地触发
				- 典型的数据流水线应用包括电子商务中的实时查询索引构建、电子商务中的持续ETL等
			- 数据流水线的优势
			  collapsed:: true
				- 减少了数据转移过程的延迟。此外，由于它能够持续消费和发送数据，因此用途更广，支持用例更多
			- Flink如何支持数据流水线应用
			  collapsed:: true
				- Flink的SQL接口（或者Table API）以及丰富的用户自定义函数，可以解决许多常见的数据转换问题
				- 通过使用更具通用性的DataStream API，还可以实现具有更加强大功能的数据流水线
				- Flink提供了大量的连接器，可以连接到各种不同类型的数据存储系统，比如Kafka、Kinesis、 Elasticsearch和JDBC数据库系统
				- Flink供了面向文件系统的连续型数据源，可用来监控目录变化，并提供了数据槽（sink），支持以时间分区的方式写入文件
	- Flink技术栈
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151532373.png){:height 301, :width 309}
	- Flink体系架构
	  collapsed:: true
		- Flink系统主要由两个组件组成，分别为JobManager和TaskManager
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151533672.png){:height 241, :width 337}
	- Flink编程模型
	  collapsed:: true
		- Flink 提供了不同级别的抽象（如图所示），以开发流或批处理作业
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406151533559.png){:height 181, :width 306}
	- Flink编程实践
-
- ![4 理解数据.pdf](../assets/4_理解数据_1717574881493_0.pdf)
- 理解数据
  collapsed:: true
	- 主要任务：EDA(exploratory data analysis)
	  collapsed:: true
		- 任务一：每个属性取值分布统计
		  collapsed:: true
			- 对给定数据集的各个属性的取值分布情况进行统计概括，方差，平均值
			- 宽表：把所有数据放到一个表中
		- 任务二：多个属性取值分布统计
		  collapsed:: true
			- 对给定数据集的多个属性的取值分布情况进行统计概括
		- 任务三：数据的总体质量评估
		  collapsed:: true
			- 数据存在误差属性值缺失噪声和不一致性等潜在的数据质量问题
	- 基于统计描述的数据理解方法
		- 集中趋势度量
			- 数值型数据
				- 均值
					- 简单平均数
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406051629784.png){:height 48, :width 212}
					- 截尾均值
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406051630474.png){:height 49, :width 259}
				- 分位数
				- 几何均值
		- 类别型数据
			- 众数
-
- ![5 大数据技术综合应用.pdf](../assets/5_大数据技术综合应用_1717942747208_0.pdf)
-
- 回顾
  collapsed:: true
	- 什么是大数据：数据量大，种类多（不同的传感器观察手段拿来的数据是不同的），4v
	- 大数据和云计算：云计算的优点，急卖急用，计算资源，技术基础，大数据的挖掘产生价值
	- 大数据和物联网：大数据的重要来源，数据整理成一个宽表
	- HDFS：spark和flink的底层，是整个的基础，存储，大数据的存储容易发生异常，冗余存储的保证，hdfs的put和get等操作
	- hbase：数据结构是也是二维表，列族的概念，需要五元组确定一个位置，稀疏，storefile是最小的检索单元，route存到zukeeper，上面有meta表，每个region有hlog，region的分裂和结合该清楚
	- mapreduce，map映射key和value值，map阶段需要shuffle
	- hive数据仓库，底层没有分布式架构，用户接口和cli和hwi和jdbc-connector，odbc是自己的，借助mysql存储，把hiveql执行成hive操作单元
	- impladad 为了解决hive的缺陷，既能管理任务又能执行任务
	- spark：uc伯克利，有一套并行操作分解为若干stage，计算模式从mapreduce进一步扩展，支持受限内存访问单元，自己的mllibrary，dag，scheler，可并行的变成taskset，交给每个excutor执行，但是spark在流处理不太好，只能在秒级，而不能毫秒级
	- flink，所有数据源放在消息缓冲队列，eg kafka，计算，计算结果再存到消息队列中，取代了集中式的大型数据库，内存中运行的很快，存到结果数据库中，完成批流一体化
	-
-
- b站自学
- 传统数据处理架构
  collapsed:: true
	- 结构化数据（数据表等）：数据库、数据仓库
	- 非结构化（图片，音频等）、半结构化数据（json日志文件等）：NoSQL数据库、并发程序
	- 中小规模不建议使用大数据技术
- 大数据背景下存在的问题
  collapsed:: true
	- 结构化数据：单机可能存储不下，即使可以单机处理速度慢，MPP架构（从单机演变过来的集群）存在扩展性、热点问题（比较热的数据存在某个节点上，这个节点有更多的压力，容易挂掉）
	- 非结构化、半结构化数据：NoSQL数据库只负责存储不负责计算（需要用程序编写计算任务），程序处理时涉及到数据移动，速度慢（把数据转移到计算节点）
- 大数据：专门为了数据达到海量规模以后对他进行存储和计算的技术架构
- 大数据的特征4v
  collapsed:: true
	- 数据量Volume：数据规模巨大
	- 速度Velocity：生成和处理速度极快
	- 多样性Variety：数据类型多样，包括结构化，非结构化，半结构化数据
	- 价值Value：价值巨大但密度较低
- 大数据的应用场景
  collapsed:: true
	- 离线和实时的区分：处理的数据究竟是有界数据还是无界数据
	  collapsed:: true
		- 离线
		  collapsed:: true
			- 有界数据就是离线，处理过程中数据不会增加或者减少，离线最适合批处理
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406072256199.png){:height 140, :width 580}
			- 另一层含义是断网也能处理
			- 批处理：所有数据在同一个时间会处于同一个阶段
		- 在线
		  collapsed:: true
			- 数据是无界的，实时产生的，数据从数据源产生以后立马交给流处理任务，然后实时显示出来
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406072258605.png){:height 131, :width 556}
			- 流处理：像流水线一样做处理，在同一时间，数据不在同一阶段
	- 离线处理场景：数据仓库，搜索与检索，图计算，数据分析
	  collapsed:: true
		- 数据仓库
		  collapsed:: true
			- 传统数据仓库
			  collapsed:: true
				- 只能解决中小规模数据问题
				- 数据产生之后会存储在单机数据库中，定期集中到数据仓库中进行汇总和管理，数仓在早期是单节点的（大型机，mpp等），从数仓中做一个数据查询，将数据抽取到计算程序所在的节点运算得到结果输出
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406080017135.png){:height 129, :width 574}
			- 大数据分析
			  collapsed:: true
				- 首先数据源丰富，数据以完全分布式的方式存储，扩展性能很好
				- 如果容量达到上限可以新增一些节点上来
				- 如果单个文件比较大会将其拆分成128M的小块，再均匀打散到各个节点存储
				- 移动计算而非移动数据：不要移动数据，计算任务分发到节点进行运算，每个节点得到部分结果，只需要最终汇总结果输出
				- 调度时间远远小于计算时间时才使用大数据技术
		- 搜索与检索
		  collapsed:: true
			- 传统：数据存起来，对数据做一些检索：模糊匹配，正则匹配，语义匹配
			- 大数据：任务和传统差不多，但是要在短时间内，还有更复杂的匹配，压力更重，性能更高
		- 图计算
		  collapsed:: true
			- 展现数据之间的关系，eg公司关系，担保链，企业之间互相投资
		- 数据挖局
		  collapsed:: true
			- eg人流密集情况
	- 实时处理场景：实时流处理
	  collapsed:: true
		- 产生的数据先进入到分布式的消息队列里面做缓冲，而不是直接到分布式流集群，主要是为了抗压，避免风险，因为有些无法预测的峰值，再从分布式队列中拿消息
- 大数据的发展
  collapsed:: true
	- 初期
	  collapsed:: true
		- 2002.10 Doug Cutting，Mike Cafarella创建开源网页爬虫项目Nutch
		- 2003.10 Google发布GFS(Google File System)，即HDFS前身
		- 2004.10 Google MapReduce，移动计算而非移动数据，Map就是把计算任务分发到各个节点，Reduce就是进行部分结果汇总
		- 2005.02Mike Cafarella在Nutch中实现MapReduce功能
		- 2006.01 Doug Cutting加入Yahoo，将Hadoop发展为一个可在网上运行的系统，此时已经包含了HDFS和MapReduce
		- 2006.02 Apache Hadoop项目正式启动，支持MapReduce和HDFS独立发展
		- 2006.02 Yahoo使用Hadoop，2006.03Yahoo建立了第一个用于开发的Hadoop集群
		- 2006.04 第一个Apache Hadoop版本发布
		- 2006.11Google发布Bigtable，是个NoSQL数据库，基于HDFS这个文件系统搭建
	- 发展期
	  collapsed:: true
		- 2007.04 YahooHadoop集群发展成两个1000个节点的集群
		- 2008.01 Hadoop成为Apache顶级项目
		- 2008.02 Yahoo运行了世界最大的Hadoop应用，搜索引擎部署在了拥有1万个内核的Hadoop集群上
		- 2008.06Hadoop的第一个SQL框架Hive成为Hadoop子项目
		- 2008.08第一个Hadoop商业化公司Cloudera成立
		- 2008.11Apache Pig的第一个版本发布
		- 2009.03 Cloudera推出世界上首个Hadoop发行版——CDH，并完全开放源码
		- 2009.07MapReduce和HDFS成为Hadoop子项目
		- 2010.05 HBase脱离Hadoop项目，成为Apache顶级项目
		- 2010.09Hive脱离Hadoop项目，成为Apache顶级项目
		- 2010.09Pig脱离Hadoop项目，成为Apache顶级项目
		- 2011.01ZooKeeper脱离Hadoop项目，成为Apache顶级项目
		- 2012.03HDFS NameNode HA加入Hadoop主版本
		- 2012.08YARN成为Hadoop子项目
	- 成熟期
	  collapsed:: true
		- 2014.02 Spark代替MapReduce成为Hadoop的缺省计算引擎（MapReduce的问题是计算很慢，因为04年内存昂贵，所以节约内存大量与磁盘交互，但是14年硬件成本廉价，Spark先全部在内存中计算），并成为Apache顶级项目
		- 2015.10 Cloudera公布继HBase以后的第一个Hadoop原生存储替代方案——Kudu
-
- 大数据的生态架构
  collapsed:: true
	- 图示
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406081102085.png){:height 413, :width 596}
	- 数据源
	  collapsed:: true
		- 结构化数据一般通过Sqoop抽取到数据存储平台，主流选型是HDFS，Sqoop通过jdbc的方式连接到数据库，Sqoop抽取数据一般是T+1(今天的数据明天才能导到平台，时效性比较低，数据仓库一般都是零点廷议导入)
		- 非结构化，半结构化数据一般通过Flume或者Logstach实时抽取，这个实时数据要推到消息队列即Kafka中
		- 非结构化，半结构化数据当然可以定时抽取
		- 如果结构化数据也像进行一个实时抽取，可以利用CDC或者OGG(OGG是Oracle特有的，CDC是开源的)，可以监控数据库中的结构化数据，数据一旦发生变化，把变动的数据抽到Kafka中。他们监控的是数据库的日志
	- 数据存储
	  collapsed:: true
		- 数据经过上一步会存储到HDFS中，但HDFS本质上是一个文件系统，生产中不太好用，一般把数据存在数据库中，hbase是一个分布式的nosql数据库，基于hdfs建立。最终的数据其实可以存到hdfs中，也可以存到hbase中
	- 通用计算
	  collapsed:: true
		- 运算可以用MapReduce(速度慢)，也可以用Spark(速度快)，这两种都是把计算任务分发到节点，如何分发需要使用资源管理层
	- 资源管理
	  collapsed:: true
		- 分布式资源调度框架Yarn，部署时是和数据存储框架部署到一起的，运算之后会收回计算资源
	- 数据分析：提高易用性
	  collapsed:: true
		- 单纯的MapReduc和Spark没有那么好用，对于结构化数据我们习惯用SQL，对于非结构化和半结构化数据我们习惯用API，这层提供了一些易用框架
		- Hive把SQL转换成MapReduc，当然也可以转换为Spark，pig也是但是之前停止维护了
		- Malhot做机器学习
		- 如果默认转换为MapReduce的就是属于Hadoop这个生态圈（因为Hadoop默认包含hdfs，mapreduce和yarn），后期有一些框架也是兼容spark的
		- spark的生态：帮助把spark sql转换成spark任务，mllib是做机器学习的，GraphX是做图计算的，Spark stream是做流计算的，实时计算结果一般存储在hbase中（如果存储在hdfs中会产生一些小文件问题，hdfs对于小文件很敏感，很容易把管理节点内存占满，也会导致后续计算的效率下降）
		- 独立开发：elastic search，用于搜索与检索，可以进行一些模糊查询，精确匹配，语义匹配。说他是独立的，因为他有自己的通用计算，资源计算，包括数据存储，所以不依赖于hadoop和spark
		- ZooKeeper：分布式协调服务，告诉其他节点哪个节点新增，哪个节点挂了，选举主节点和备份节点，对于这些大数据产品很重要，比如kafka一定要zookeeper的安装
		- 任务调度节点：Oozie和azkaban，用于调度计算任务，如果有计算顺序，和定时完成任务可以用这两者
	- 实际上是在软件开发层面把底层OS（分布式操作系统）构建出来
-
- HDFS
  collapsed:: true
	- 简介
	  collapsed:: true
		- Hadoop分布式文件系统 Hadoop Distributed File System
		- 2003年10月Google发表了GFS (Google File System）论文
		- HDFS是GFS的开源实现
		- HDFS是Apache Hadoop的核心子项目（有3个，hdfs，yarn，mapreduce）
		- 在开源大数据技术体系中，地位无可替代
	- 设计目标
	  collapsed:: true
		- 运行在大量廉价商用机器上:硬件错误是常态，提供容错机制
		- 简单一致性模型:一次写入多次读取，支持追加，不允许修改，保证数据一致性
		- 流式数据访问:批量读而非随机读，关注吞吐量而非时间
		- 存储大规模数据集:典型文件大小GB~TB，关注横向线性扩展
	- 优缺点
	  collapsed:: true
		- 优点
		  collapsed:: true
			- 高容错（允许节点出现错误）、高可用（一般针对管理节点，管理节点也进行备份）、高扩展（大数据通用特征，通过增加节点增加数据存储总量）
			  collapsed:: true
				- 数据冗余多副本，副本丢失后自动恢复
				- NameNode HA、安全模式
				- 10K节点规模
			- 海量数据存储
			  collapsed:: true
				- 典型文件大小GB~TB，百万以上文件数量，PB以上数据规模
			- 构建成本低、安全可靠
			  collapsed:: true
				- 构建在廉价的商用服务器上（普通的服务器就行，可能挂掉）
				- 提供了容错和恢复机制适合（软件层面，默认备份3个节点）
			- 大规模离线批处理（所有数据在同一个位置）
			  collapsed:: true
				- 流式数据访问
				- 数据位置暴露给计算框架，计算任务分发到数据节点
		- 缺点
		  collapsed:: true
			- 不适合低延迟数据访问
			  collapsed:: true
				- HDFS延迟很高，注重怎样把海量数据存储起来，像hbase的延迟就很低
			- 不支持并发写入
			  collapsed:: true
				- 一个文件同时只能有一个写入者，把文件拆分成很多份，文件是一份一份去写的，串行写入保持数据可靠性
			- 不适合大量小文件存储
			  collapsed:: true
				- 元数据占用NameNode大量内存空间，元数据记录了文件名，文件被拆分成了多少份，存储到了哪些节点，管理节点被占满就意味着整个集群不可用了，计算任务也多
				- 磁盘寻道时间超过读取时间
			- 不支持文件随机修改
			  collapsed:: true
				- 仅支持追加写入，负载大，而且改一次则三次备份都要改，开销大
				- 修改可以采用把之前的删掉，后来的再追加进来的思路
	- HDFS原理
	  collapsed:: true
		- 系统架构
		  collapsed:: true
			- 图示
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406082226976.png)
			- 典型的主从架构
			  collapsed:: true
				- 主节点
				  collapsed:: true
					- 主节点是NameNode，从节点是DataNode
					- 主节点(Active)一般做管理，接受客户端发过来的读写请求
					- 和高可用HA(High Aviliable)的NameNode相连，备用节点(Standby)
				- 从节点
				  collapsed:: true
					- 用于存储数据
					- 文件一般以128M为一个Block拆分成多个块，如果最后切到不足128M就是不足128M
					- 为了保证数据的安全性需要有备份，默认情况备3个副本
			- 数据写入流程示例
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406082232235.png)
				- 客户端发起文件上传，文件拆分，放到DataNode存储，备份
				- 在NameNode记录元数据，重要的是文件被分成多少块，下面的每个块存储在哪些节点是可以通过心跳机制每隔3s上报的，还上报当前节点的空闲状态(空闲or忙碌)，如果一个节点长时间没有上报，那么NameNode有理由认为这个节点挂掉了
		- 存储机制
		  collapsed:: true
			- Block数据块
			  collapsed:: true
				- HDFS最小存储单元
				- 多Block多副本
				  collapsed:: true
					- 文件被切分成若干个Block，每个Block有多个副本（默认3副本)
					- Block以DataNode为存储单元，即一个DataNode上只能存放Block的一个副本
					- 机架感知:尽量将副本存储到不同的机架上，以提升数据的容错能力
					- 副本均匀分布:DataNode的Block副本数和访问负荷要比较接近，以实现负载均衡
				- Block大小
				  collapsed:: true
					- 默认128M，可设置（若Block中数据的实际大小<设定值，则Block大小=实际数据大小)
					- 如何调整Block大小
					  collapsed:: true
						- 目标:①最小化寻址开销，降到1%以下;②任务并发度和集群负载比较适中，作业运行速度较快
						- 块太小:①寻址时间占比过高;②Map任务太多，并发度太高，导致集群负载过高，作业变慢
						- 块太大: Map任务太少，并发度太低，导致集群负载过低，作业变慢
			- Block副本放置策略(Hadoop 3.x)
			  collapsed:: true
				- 副本1:放在Client所在节点上
				  collapsed:: true
					- 对于远程Client，系统会随机选择机架和节点
					- 存储在最近的节点中，客户端首先找到机房是一跳，机房到不同机架的交换机又是一跳，从交换机到各个节点又是一跳。所以客户端与各个节点的距离都相同，都是3跳。于是从所有节点中选择一个最空闲的
				- 副本2:放在与副本1不同的机架上，因为存到当前机架会有些风险，因为当前机架会接同一个交换机，同一个电源
				- 副本3:放在与副本2同一机架的不同节点上，在同一机架是因为数据已经保证足够安全了，不用再费力找第三个
				- 副本N:在遵循相关原则的前提下，随机选择
				- 节点选择原则
				  collapsed:: true
					- 避免选择访问负荷太重的节点
					- 避免选择存储太满的节点
					- 避免将Block的所有副本都放在同一机架上
			- Block文件
			  collapsed:: true
				- Block文件是DataNode本地磁盘中名为“blk_blockId”的Linux文件
				  collapsed:: true
					- DataNode在启动时自动创建存储目录，无需格式化
					- DataNode的current目录下的文件名都以“blk_”为前缀
					- Block元数据文件(*.meta）由一个包含版本、类型信息的头文件和一系列校验值组成
				- 图示
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406082307426.png){:height 218, :width 278}
				- 注意
				  collapsed:: true
					- dfs.datanode.data.dir是可以通过参数配置的
					- BP-开头的是文件名
					- 这个meta文件和元数据没有关系，仅仅是这个数据文件的属性信息
			- 元数据
			  collapsed:: true
				- 目录文件的基本属性（如名称、所有者等）、Block相关信息（如件包含哪些Block、Block放在哪些节点上等)、DataNode相关信息
			- 内存元数据
			  collapsed:: true
				- Active NameNode:最新的元数据(= fsimage + edits，实时更新)
				  collapsed:: true
					- 因为文件被拆分成了几个block这个信息比较重要，需要保证断电时不丢失，写到fsimage里面，但因为实时变动，但是磁盘的随机修改压力很大，于是以日志形式追加到edits中，顺序追加，顺序读写比随机读写压力更小
					- fsimage恢复比较早的，用edits恢复到最新
					- 但是edits也不能特别大，这样会导致恢复时间很长
				- Standby NameNode:通过QJM定期（默认60s）同步AN的元数据
			- 文件元数据
			  collapsed:: true
				- 内存元数据持久化后形成的文件
				- edits（编辑日志文件)
				  collapsed:: true
					- 保存了最近一个Checkpoint检查点之后的所有变更操作（需定期瘦身)，把edits定期合并到fsimage里面，而这是通过StandbyNamenode来完成的
					- 变更操作应先写edits，再写内存
					- edits文件名通过“Transaction ld前后缀”标记所包含更新操作的范围
				- fsimage （元数据检查点镜像文件)
				  collapsed:: true
					- Standby NameNode在Checkpoint检查点定期对内存中的元数据进行持久化，生成fsimage镜像文件
					- fsimage的写入速度比较慢，所以不可能对变更操作进行实时持久化
					- fsimage文件名标记出最后一个变更操作的Transaction ld，以下图为例，只要在内存中载入fsimage_* 19，然后在内存中执行edits _inprogress_ *20就可以还原出最新的元数据
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092044190.png){:height 213, :width 356}
				- 数据合并，edits与fsimage持久化
				  collapsed:: true
					- 如果集群没有做高可用HA，那么合并是由SecondaryNameNode来完成的，并不是用于备份的节点，唯一的作用是合并元数据（Hadoop1.x）
					  collapsed:: true
						- 基于远程合并的持久化
						  collapsed:: true
							- ①在Checkpoint检查点，SN请求PN停用edits，后续变更操作写入edits.new
							- ②将fsimage和edits下载到SN（第一次需下载fsimage)
							- ③在内存中载入fsimage，与edits进行合并，然后生成新的fsimage，并将其上传给PN
							- ④用新文件替换edits和fsimage (edits实现瘦身)
						- 缺点
						  collapsed:: true
							- 合并前要先将fsimage载入内存，速度慢
							- 未实现edits高可用(SN上的edits不是最新的），若PN上的edits损毁，将导致元数据丢失
							- SN无法承担热备职能
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092048784.png){:height 327, :width 332}
					- 如果做了HA，有多台NameNode，如果一台NameNode挂掉了之后另外一台如何接替原来NameNode的功能？一定要同步元数据，fsimage可以放在本地，因为原来都是空的，但是edits就不能放到本地了，否则挂掉之后edits没办法获取，所以HA中把edits放到三方集群中Journal Node，一般是奇数台（好区分半数以上），保证edits可靠性（Hadoop2.x）
					  collapsed:: true
						- QJM ( Quorum Journal Manager）共享存储系统
						  collapsed:: true
							- 基于Paxos算法实现的JournalNode集群，实现了edits的高可用存储和共享访问
							- 最好部署奇数（2n+1）个节点，最多容忍n个节点宕机
							- 过半(≥n+1)节点写入成功，即代表写探
						- 基于QJM的edits持久化
						  collapsed:: true
							- ①AN将变更操作同步写入本地和QJM的edits
							- ②在内存中执行该操作，并将结果反馈给Client
						- 基于QJM的fsimage持久化
						  collapsed:: true
							- ①在Checkpoint检查点，SN先将内存元数据变为只读来暂停QJM edits的定期同步，再将元数据镜像到fsimage中
							- ②SN将fsimage上传到AN，同时恢复QJM定期同步
							- ③AN根据fsimage的事务id，删除旧edits，实现瘦身
		- 读写操作
		  collapsed:: true
			- 写操作
			  collapsed:: true
				- 客户端发起写请求，发送到NameNode节点
				- NameNode目录检查并鉴权
				- 如果有权限返回允许上传
				- 在客户端将文件拆分成128M大小的Block，减小hdfs压力
				- 这3个Block一个一个上传，客户端发起上传Block请求
				- 服务器分配DataNode，寻找3个副本
				- 返回dn列表
				- 客户端与dn建立连接，这三个dn以pipeline建立通道
				- 数据发送，以数据包packet发送
				- 第一个dn内存接收，写入磁盘的同时，发给第二个dn，第三个也同理，写入成功之后返回成功，第二个接收到第三个的成功信息并且自己也写入成功之后给第一个返回成功信息，第一个同理
				- 第一个Block写完之后以此类推写第二个和第三个
				- 全部写完之后向NameNode汇报
				- 最后NameNode在内存中生成元数据
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092109274.png)
			- 读操作
			  collapsed:: true
				- 客户端发起读文件请求
				- NameNode检察目录并鉴权
				- 如果有权限就把要读的元数据返回
				- 而且返回的时候对3个dn根据与客户端距离进行排序
				- 与最近的节点连接，把各个Block读过来
				- 在客户端合并
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092119190.png)
		- 安全模式
		  collapsed:: true
			- 什么是安全模式
			  collapsed:: true
				- 安全模式是HDFS的一种特殊状态，在这种状态下，HDFS只接收读数据请求，而不接收写入、删除、修改等变更请求，“只读”
				- 安全模式是HDFS确保Block数据安全的一种保护机制，当它觉得集群不安全时
				- Active NameNode启动时，HDFS会进入安全模式，DataNode主动向NameNode汇报可用Block列表等信息，在系统达到安全标准前，HDFS一直处于“只读”状态
			- 何时正常离开安全模式
			  collapsed:: true
				- Block上报率:DataNode上报的可用Block个数/NameNode元数据记录的Block个数
				- 当Block上报率>=阈值时，HDFS才能离开安全模式，默认阈值为0.999
				- 不建议手动强制退出安全模式
			- 触发安全模式的原因
			  collapsed:: true
				- NameNode重启，因为元数据存储在内存中，上报率被清为0
				- NameNode磁盘空间不足
				- Block上报率低于阈值
				- DataNode无法正常启动
				- 日志中出现严重异常
				- 用户操作不当，如:强制关机（特别注意!)
			- 故障排查
			  collapsed:: true
				- 找到DataNode不能正常启动的原因，重启DataNode
				- 清理NameNode磁盘
		- 高可用
		  collapsed:: true
			- HDFS在设计上:
			  collapsed:: true
				- 优势
				  collapsed:: true
					- 高容错性
					- 扩展性
					- 海量数据的高效读、写
				- 劣势
				  collapsed:: true
					- NameNode内存受限问题
						- Federation机制，多台NameNode共同管理
					- NameNode单点故障问题
					- 对NameNode做高可用(High Availability)
			- 实现细节，思考:
			  collapsed:: true
				- 集群中，提供两台NameNode做热备：Active，Standby
				- 一台NameNode宕机，另外一台如何接管集群?
				  collapsed:: true
					- 元数据信息保持一致，JournalNode负责元数据同步：edits，fsimage
					- 如何做到自动的故障迁移：Zookeeper集群
					  collapsed:: true
						- 分别启动FailoverController进程，通过心跳机制定期与ZooKeeper汇报，选举和状态切换
						- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092131060.png)
	- HDFS操作
	  collapsed:: true
		- 文件系统命令Shell
		  collapsed:: true
			- 语法
			  collapsed:: true
				- hadoop fs <args>(使用面最广，可以操作任何文件系统)
				- hadoop dfs <args>(只能操作HDFS文件系统，已Deprecated)
				- hdfs dfs <args>(推荐使用命令，hadoop fs执行时会转换为此命令)
				- 大部分用法和Linux Shell类似，可通过help查看帮助
			- HDFS URI
			  collapsed:: true
				- 格式: scheme://authority/path
				- 示例:HDFS上的一个文件/parent/child
					- URI全写: hdfs:/nameservice/parent/child (用nameservice替代namenodehost），操作多个的时候用全写区分
					- URI简写:/parent/child
					- 需在配置文件中定义hdfs://namenodehost
			- 操作
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092137951.png)
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092138429.png)
		- REST API：把对HDFS的操作转换为了http形式
		  collapsed:: true
			- 兼容多种编程语言，都可以网络请求
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092141016.png)
			- Step1会返回DataNode
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092143067.png)
	- HDFS运维管理
	  collapsed:: true
		- 系统配置
		  collapsed:: true
			- 核心配置文件
			  collapsed:: true
				- core-site.xml:Hadoop全局配置，对HDFS，Yarn，MapReduce全局配置用
				- hdfs-site.xml:HDFS局部配置
				  collapsed:: true
					- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092147448.png)
				- 示例: NameNode URI配置(core-site.xml)
				- ```xml
				  <configuration>
				  	<property>
				  		<name>fs.defaultFS</name>
				  		<value>hdfs://nameservice:9000</value>
				    	</property>
				  </configuration>
				  ```
			- 环境变量文件
			  collapsed:: true
				- Hadoop-env.sh:设置了HDFS运行所需的环境变量
		- HDFS管理命令
		  collapsed:: true
			- NameNode格式化或恢复：“认主”
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092147254.png)
			- Report查看报告文件系统信息
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092149517.png)
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092149687.png){:height 310, :width 341}
		- 系统监控
		  collapsed:: true
			- Fsck检查文件系统健康状况
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092150925.png)
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092150344.png)
			- Safemode安全模式
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092151122.png)
			- NameNode HA主备切换
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092152367.png)
			- Decommission or Recommission(DataNode退役和服役)：用于扩容和缩容
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092152981.png)
			- Balancer (数据重分布)
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092153490.png)
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092154878.png)
			- Distcp分布式拷贝，集群之间，内部也可以
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092155883.png)
			- Quota配额限制
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092156053.png)
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092156786.png)
			- 快照Snapshot
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092157086.png)
			- 网页
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406092158983.png)
-
- MapReduce
  collapsed:: true
	- 简介
	  collapsed:: true
		- 起源
		  collapsed:: true
			- 2004年10月Google发表了MapReduce论文
			- 设计初衷:解决搜索引擎中大规模网页数据的并行处理
			- Hadoop MapReduce是Google MapReduce的开源实现
			- MapReduce是Apache Hadoop的核心子项目
		- 概念
		  collapsed:: true
			- 面向批处理的分布式计算框架
			- 一种编程模型:MapReduce程序被分为Map（映射）阶段和Reduce（化简）阶段
		- 核心思想
		  collapsed:: true
			- 分而治之，并行计算
			- 移动计算，而非移动数据
		- 特点
		  collapsed:: true
			- 计算跟着数据走
			- 良好的扩展性:计算能力随着节点数增加，近似线性递增
			- 高容错
			- 状态监控
			- 适合海量数据的离线批处理
			- 降低了分布式编程的门槛
		- 适用场景
		  collapsed:: true
			- 数据统计，如:网站的PV、UV统计
			- 搜索引擎构建索引
			- 海量数据查询
			- 复杂数据分析算法实现
		- 不适用场景
		  collapsed:: true
			- OLAP
			  collapsed:: true
				- 要求毫秒或秒级返回结果
			- 流计算
			  collapsed:: true
				- 流计算的输入数据集是动态的，而MapReduce是静态的
		- DAG计算
		  collapsed:: true
			- 多个作业存在依赖关系，后一个的输入是前一个的输出，构成有向无环图D
			  AG
			- 每个MapReduce作业的输出结果都会落盘，造成大量磁盘IO，导致性能非常低下
			- DAG一般用Spark做
	- 词频统计原理
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406101339030.png)
		- 从Map到Reduce中间经过shuffle阶段，利用hash之后对4取模，得到余数
		- reduce节点个数可以手动指定
	- 运行原理
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406101344352.png)
		- shuffle实际上在map task中有一部分，在reduce task中也有一部分
		- shuffle过程
		  collapsed:: true
			- 这个MapReduce最慢的阶段就是shuffle阶段
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406101347053.png)
			- 文件首先被拆分为Block块
			- 每个Block在运算过程中被称作map，map得到的结果是key-value
			- k-v被存放到内存缓冲区中，有100M，达到80M的时候会溢写到磁盘中，溢写的同时会对文件进行一个分组的排序
			- 所有磁盘中的小文件合并
			- reduce task去map task中拉取
			- 首先也是缓存，超过后溢写
			- 之后合并成大文件，分组排序，按照k分组
			- 省内存，但是大量与磁盘交互，从map到reduce还需要大量数据在网络中传输
	- 作业运行模式
	  collapsed:: true
		- JobTracker/TaskTracker模式 Hadoop1.X
		  collapsed:: true
			- JobTracker节点（Master）
			  collapsed:: true
				- 调度任务在TaskTracker上运行
				- 若任务失败，指定新TaskTracker重新运行
			- TaskTracker节点（Slave)
			  collapsed:: true
				- 执行任务，发送进度报告
			- 存在的问题
			  collapsed:: true
				- JobTracker存在单点故障
				- JobTracker负载太重（上限4000节点)
				- JobTracker缺少对资源的全面管理
				- TaskTracker对资源的描述过于简单
				- 源码很难理解
			- 图示
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406101432322.png){:height 358, :width 387}
		- YARN模式 Hadoop2.X
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406101435566.png)
	- 作业提交与监督
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406101437448.png)
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406101438752.png)
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406101440336.png)
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406101440402.png)
		-
-
- Spark
  collapsed:: true
	- 产生背景
	  collapsed:: true
		- MapReduce有较大的局限性
		  collapsed:: true
			- 仅支持Map、Reduce两种语义操作
			- 执行效率低，时间开销大
			- 主要用于大规模离线批处理
			- 不适合迭代计算、交互式计算、实时流处理等场景
		- 计算框架种类多，选型难，学习成本高
		  collapsed:: true
			- 批处理:MapReduce
			- 流处理:Storm、Flink
			- 交互式计算: Impala、Presto
			- 机器学习:Mahout
		- 统一计算框架，简化技术选型
		  collapsed:: true
			- 在一个统一框架下，实现批处理、流处理、交互式计算、机器学习
	- 简介
	  collapsed:: true
		- 由加州大学伯克利分校的AMP实验室开源
		- 大规模分布式通用计算引擎
			- Spark Core:核心计算框架
			- Spark SQL:结构化数据查询
			- Spark Streaming:实时流处理 *微批处理，底层还是在跑批处理，但是时间间隔很短，这样近乎在做实时流处理*
			- Spark MLib:机器学习
			- Spark GraphX:图计算
		- 具有高吞吐、低延时、通用易扩展、高容错等特点采用Scala语言开发 *速度快*，*All in one*
		- 提供多种运行模式，*之前的hadoop，通过on Yarn模式分发到hadoop*
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406122132079.png)
	- RDD
	  collapsed:: true
		- 弹性分布式数据集(Resilient Distributed Datasets)
		  collapsed:: true
			- 分布在集群中的只读对象集合，每次处理后的数据需要用一个新的RDD保存
			- 由多个Partition组成，每个block是一个partition
			- 通过转换操作构造
			- 失效后自动重构（弹性)
			- 存储在内存或磁盘中。如果某个节点做错了只需要返回上一个节点，但是走了shuffle之后返回就比较麻烦
		- Spark基于RDD进行计算
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406122140715.png)
			- sc：SparkContext，spark上下文对象
			- 文件读进来就变成了RDD
			- flatMap，每一行的数据切分
			- map给每一个单词标了个1
			- reduceByKey：把key相同的数据分发到同一个节点中，key相同的做求和，前面的_是占位符
			-
		- RDD操作
		  collapsed:: true
			- Transformation
			  collapsed:: true
				- 将Scala集合或Hadoop输入数据构造成一个新RDD
				- 通过已有的RDD产生新RDD
				- 惰性执行:只记录转换关系，不触发计算
				- 例如: map、filter、flatmap、union、distinct、sortbykey
			- Action
			  collapsed:: true
				- 通过RDD计算得到一个值或一组值
				- 真正触发计算
				- 例如:first、count、collect、foreach、saveAsTextFile
				  collapsed:: true
					- first：输出结果的第一条
					- count：统计RDD行数输出
					- collect：RDD结果输出
					- foreach：遍历
		- RDD依赖
		  collapsed:: true
			- 窄依赖(Narrow Dependency)
			  collapsed:: true
				- 父RDD中的分区最多只能被一个子RDD的一个分区使用
				- 子RDD如果有部分分区数据丢失或损坏，只需从对应的父RDD重新计算恢复
				- 例如: map、filter、union
			- 宽依赖 ( Shuffle/Wide Dependency )
			  collapsed:: true
				- 子RDD分区依赖父RDD的所有分区
				- 子RDD如果部分或全部分区数据丢失或损坏，必须从所有父RDD分区重新计算
				- 相对于窄依赖，宽依赖付出的代价要高很多，尽量避免使用
				- 例如:groupByKey、reduceByKey、sortByKey
			- 图示
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406122210701.png)
	- 程序架构
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406122214495.png)
		- 客户端向主节点发起一个任务请求
		- master会在worker中分配运行资源
		- 运行作业管理进程driver，解析任务，再向master申请资源
		- master把资源封装成excutor返回
		- driver把自己解析出来的task分发到excutor中执行
		- 执行过程中task会向driver汇报
		- 所有task完成后，driver向master申请资源释放
		- 类似于yarn中的container，但是yarn中的container只能运行一个task，但是这个可以运行多个task
	- 作业提交模式
	  collapsed:: true
		- Local模式
		  collapsed:: true
			- 单机运衍，通常用于测试
			- Spark程序以多线程方式直接运行在本地
		- Standalone模式
		  collapsed:: true
			- Spark集群独立运行，不依赖于第三方资源管理系统，如:YARN、Mesos
			- 采用Master/Slave架构
			- Driver在Worker中运行，Master只负责集群管理
			- ZooKeeper负责Master HA，避免单点故障
			- 适用于集群规模不大，数据量不大的情况
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406131735319.png)
		- Yarn模式
		  collapsed:: true
			- YARN-Client模式:适用于交互和调试
			  collapsed:: true
				- 客户端也能看到情况
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406131739886.png)
			- YARN-Cluster模式:适用于生产环境
			  collapsed:: true
				- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406131736386.png){:height 404, :width 514}
				- 只需要在Driver外面套一个Application Master的盒子，负责解析的还是Driver，由AM申请资源
	- Spark作业解析与监控
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406131741596.png)
		- 生成逻辑查询计划
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406131741974.png)
			- 链式编程
			- 逻辑查询计划仅仅关注RDD的状态
		- 生成物理查询计划
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406131933759.png)
			- 关注地层数据的变化
			- 需要根据宽依赖切分stage
		- 任务执行与调度
		  collapsed:: true
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406131936268.png)
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406131938410.png)
			- stage越多，shuffle越多，性能越差
		- 监控
			- ![Replaced by Image Uploader](https://raw.githubusercontent.com/qugushihua/blog-images/master/202406131939403.png)
-