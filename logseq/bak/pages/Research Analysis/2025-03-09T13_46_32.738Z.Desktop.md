- ActiveSplat论文comment
  collapsed:: true
	- 1，感觉Overview of ActiveSplat下面那个描述信息基本上和Abstract是一样的，没有提及图中的一些信息或者数据流或者流程之类的。就是感觉一般其他文章直接看Overview的图再结合下面文字是能大概看明白流程的，但是我们之前投稿那个图感觉看不太出来，新的这个信息量还有些多
	- 2，Fig2那个全景图要不要用虚线隔一下体现出来是多张图片拼起来的全景图，或者在B. Active Viewpoint Selection这个部分加，因为感觉这个纯opacity的图看不太出来边界
	- 3，Methodology的前面的Overview可以简要提一下下面每个部分对应讲的是什么
	- 4，A. Hybrid Map Updating这个标题叫updating，但是大部分篇幅说了GS Map怎么更新，然后是GS Map怎么抽维诺图，GS Map和维诺图结合的好处，要不要提一句维诺图也是实时更新
	- 5，B. Active Viewpoint Selection这里小标题前面那段要不先说我们选用维诺图。因为感觉之前的逻辑是先说active mapping的目的，迭代选择target view，然后就直接说了维诺图的性质，最后说所以用维诺图，感觉有一点不连贯。
	- 6，B. Active Viewpoint Selection最好要不要加一下展示那种选择顺序的图示，因为这个部分纯文字描述确实不太直观。
	- 7，B. Active Viewpoint Selection的1）和3）感觉有点相似，尤其是position选择那里。然后1）那里感觉我们这个维诺图不是增量更新的是不是不能直接叫incremental augmented。2）要不要分个段就是Note that后面，然后公式9里面的变量要不再简要提一下
	- 8，hierarchical planning的motivation感觉或许可以在前面多说一点，感觉多次提到了但是每次都是为了减少重复访问同一件事情，但是前面或许可以具体说一下我们发现近处没有看完整但远处也有不确定性的时候会导致往返，这样不efficient
	- 9，子区域划分那里，原文说的是动态划分图为一个本地子区域 (Rl) 和剩余的 (k) 个子区域 (Rk)，但是实际上是整个Voronoi图被划分成多个子区域，其中一个被标记为当前的局部子区域 (Rl)
	- 10，local-global那里，感觉一上来就提到下一个最佳子区域(global规划)逻辑上有点跳跃，就是上面说了子区域划分，这个上来应该先说在local子区域Rl内部进行探索，达到条件之后再进行下一个最佳子区域的选择
	- 11，local-global那里的Rk后续也没有用到，而且这个k和前面那些Ck，Dk，Ok的下角标k是不是冲突了，要不换一个变量，而且前面那个k也没有解释，应该是像素索引，要不要在前面提一下
	- 12，Implementation Details这里感觉这4个部分是有一点割裂的，就是没有说关系，只是把里面需要的具体参数摆上了，或许是这里可以串一下。但是我也不太确定有没有这个必要
	- 13，感觉可能为什么有的评论说实验比较少，就是实验部分C. Comparison to Other Methods这里，感觉可能只是说了一句the  proposed system outperforms all relevant methods within the  limited steps就是有点少。还有就是Ablation那里1）different exploration strategies+hierarchical planning和3）后处理，感觉篇幅差不多，然后感觉1）hierarchical planning作为主要贡献点或许可以更多一些分析
	- 上面完全是个人建议，可能有不对的，仅供参考
-
- GiftedNav corner case -- my writing
  collapsed:: true
	- v0
	  collapsed:: true
		- 在三维重建中，遮挡和局部观测是从二维观察三维时天然存在的问题。多个视角的观测能够帮助我们更好地确定物体的语义。目前一个流行的解决范式是通过分割模型得到每一帧类别无关的分割区域再用Foundation Model获取对应区域的特征，再将帧间特征进行融合。但是这种方法论存在问题，依赖于分割结果的粒度和获取到的特征的准确性。
		- 首先，这种方法论缺乏有效的局部与全局上下文信息整合机制，导致模型无法充分利用不同尺度下的信息，即使是在理想情况下当分割模型粒度恰好，刚刚好能够分出一个物体时。有些方法通过根据局部特征(基于mask内像素提取)和全局特征(基于整张图像提取)的相似度加权local和global信息来缓解这个问题。但是由于这两种特征是通过独立路径生成的，而不是在同一空间中协同学习的，仅仅通过加权机制进行特征拼接会导致局部和全局之间的联系较为松散，无法实现真正的上下文感知，依然无法处理特定类别。尤其是当local预测错误时global的权重很低而无法拉回错误的local。比如cabinet会因为外面那层黑色条状条纹而被识别为air vent根据局部信息无法判断。这就导致后续将每一帧上的2D图像域特征融合到3D时关联了很多错的特征。
		- 其次，目前的分割模型存在强烈的过分割现象，而这是因为图像域类别无关的分割受到很多因素的影响，比如复杂的纹理，几何形状，光照等等。而过分割导致的问题就是被识别成多个非常不相关的其他物体而不是一个完整的正确的物体，损害了一个物体的语义一致性。比如在图1中的这个黑色sofa被分割成很多bag，jacket，blanket，pillow。再比如图2中的sofa被分割为tissue，chair和column。比如图3中的wardrobe被识别成blinds，air vent，door。而分割错误会直接影响后续特征提取和融合的质量。而且，过分割导致的局部信息碎片化直接加剧了上下文信息缺失的问题。过多的错误分割区域使得在整合多视角信息时，系统难以形成对物体整体结构和位置的准确理解。比如当看到mirror时，会识别成mirror里面的door frame。
		- 而我们的方法中使用YOLO-World，其方法论在设计上直接避免局部和全局信息割裂的问题，也不会出现过分割现象。YOLOWorld通过多尺度特征提取，将局部和全局信息作为一个整体来建模。在这种设计中，局部特征不再是独立提取的，而是从不同尺度的特征中共享上下文信息。特征图的低分辨率部分（大感受野）提供了全局上下文信息，如场景的大致布局和物体的整体位置；高分辨率部分（小感受野）保留了局部的细节信息，如边缘、纹理等。多尺度特征的联合建模使得局部细节在被建模时已经考虑到了全局语义约束，局部和全局在同一特征空间中具有语义一致性。
	- v1
	  collapsed:: true
		- 在语义导航任务中，系统需要具备准确的空间感知能力，以识别场景中的物体及其相对位置，从而生成高质量的语义地图。然而，由于单视角观测受到遮挡和局部视野限制，系统常难以完整捕获物体的语义属性或结构，因此多视角观测成为一种解决方案。一个流行的范式是：首先通过类别无关的分割模型将每一帧图像划分为多个区域，再利用Foundation Model提取这些分割区域的特征，最后通过帧间特征融合生成三维的语义表示。然而，这种方法论仍存在问题，它的效果高度依赖于分割模型的粒度提取特征的准确性。
		- 首先，这种方法论缺乏有效的局部与全局上下文信息整合机制，导致模型无法充分利用不同尺度下的信息，即使是在理想情况下当分割模型粒度恰好，刚刚好能够分出一个物体时。有些方法通过根据局部特征(基于mask内像素提取)和全局特征(基于整张图像提取)的相似度加权local和global信息来缓解这个问题。但是由于这两种特征是通过独立路径生成的，而不是在同一空间中协同学习的，仅仅通过加权机制进行特征拼接会导致局部和全局之间的联系较为松散，无法实现真正的上下文感知，依然无法处理特定类别。尤其是当局部特征预测错误时全局特征的权重很低而无法纠正错误的特征。比如cabinet会因为外面那层黑色条状条纹而被识别为air vent根据局部信息无法判断。进一步地，这种误判在多帧2D图像域特征融合到3D时会导致特征关联错误，从而影响地图语义的最终结果。
		- 其次，目前的分割模型存在显著的过分割现象。由于图像域类别无关的分割模型受到纹理复杂性、几何形状和光照等多种因素的影响，往往会将单一物体划分为多个碎片化区域。这种现象会破坏物体的语义一致性，使其被误识别为多个不相关的其他物体。例如，图1中黑色sofa可能被分割为bag、jacket、blanket和pillow等碎片区域。图2中的sofa被分割为tissue，chair和column。比如图3中的wardrobe被识别成blinds，air vent，door。而分割错误会直接影响后续特征提取和融合的质量。而且，过分割导致的局部信息碎片化直接加剧了上下文信息缺失的问题。过多的错误分割区域使得在整合多视角信息时，系统难以形成对物体整体结构和位置的准确理解。比如当看到mirror时，会识别成mirror里面的door frame。损害了语义特征的准确性。
		- 相反，我们的方法使用YOLO-World，从设计上直接避免了局部和全局信息割裂的问题，并且有效解决了过分割现象。YOLOWorld通过多尺度特征提取，将局部和全局信息在同一框架中进行建模。特征图的低分辨率部分（大感受野）捕捉全局上下文信息，例如场景的大致布局和物体的整体位置；高分辨率部分（小感受野）保留局部的细节信息，例如边缘、纹理等。通过多尺度特征的联合建模，局部特征生成时已经受到全局语义约束，确保局部和全局特征在同一特征空间中保持语义一致性。此外，YOLOWorld在直接从整张图像生成目标特征时，通过整体语义建模避免了依赖碎片化的分割区域，从而从根本上杜绝了过分割问题。
	- v1-en
	  collapsed:: true
		- In semantic navigation tasks, systems require accurate spatial perception to identify objects in the scene and their relative positions, enabling the generation of high-quality semantic maps. However, single-view observations are inherently ==limited by occlusion and partial observation==, making it difficult for the system to fully capture the semantic attributes or geometry of objects. To address this, multi-view observations have become a common solution. One popular paradigm involves using a ==class-agnostic segmentation model== to divide each image frame into multiple regions, extracting semantic features of these segmented regions with a visual language foundation model, and subsequently fusing the inter-frame features to produce 3D semantic representations. However, this paradigm still faces challenges, as its effectiveness heavily depends on the ==granularity of the segmentation model and the accuracy of the extracted features.== [[#red]]==这里表述有问题，the accuracy of the extracted features相当于什么都没说==
		- First and foremost, this methodology lacks an effective mechanism to ==integrate local and global contextual information==. This limitation persists even in ideal cases where the segmentation model achieves perfect granularity, just enough to isolate an entire object. Some methods attempt to address this by ==weighting== local (extracted from mask-based pixels) and global (extracted from the entire image) features ~~based on their semantic similarity~~. However, since these two features are generated through independent pathways rather than [[#red]]==being learned in a shared space==, simply fusing them via weighting results in loose connections between them, failing to [[#red]]==achieve genuine context awareness==.  Especially when local predictions are incorrect, global feature weights tend to remain low due to low similarity to correct false local predictions. For example, a cabinet in Figure 5 might be misclassified as an air vent due to the black striped patterns on its surface, and a mirror in Figure 4 might be misinterpreted as the door frame reflected within it, as this way of combining global information cannot resolve this ambiguity. Furthermore, such misclassifications can propagate during the fusion of multi-frame 2D features into 3D, resulting in incorrect feature associations and degrading the semantic quality of the map.
		- Additionally, current segmentation models often suffer from ==significant over-segmentation==. Class-agnostic segmentation in the image domain is highly influenced by factors such as ==complex textures, geometric shapes, and lighting conditions,== which frequently cause a single object to be fragmented into multiple unrelated regions, undermining the semantic consistency of the object. For instance, as illustrated in Figure 1, a black sofa might be segmented into fragments ==labeled as bag, jacket, blanket, and pillow==. Similarly, in Figure 2, ==a sofa might be segmented into tissue, chair, and column==, while in Figure 3, a wardrobe could be misidentified as blinds, air vent, and door. Such segmentation errors directly affect the quality of subsequent feature extraction and fusion. Moreover, the fragmentation of local information caused by over-segmentation exacerbates the loss of contextual information. The abundance of erroneous segmented regions makes it challenging for the system to form an accurate understanding of the overall structure and position of objects when integrating multi-view information.
		- In contrast, our method leverages YOLO-World, which inherently avoids the issue of weak connection between local and global information while effectively addressing over-segmentation. YOLO-World ==integrates local and global information== within a unified framework through [[#blue]]==multi-scale feature extraction 这里描述可以再清楚具体一些==. The low-resolution parts of the feature map (with a large receptive field) capture global contextual information, such as the general layout of the scene and the overall position of objects, while the high-resolution parts (with a small receptive field) preserve local details, such as edges and textures. Through the joint modeling of multi-scale features, local features are generated with global semantic constraints already in place, ensuring semantic consistency between local and global features within the same feature space. Additionally, YOLO-World directly generates target features from the entire image while avoiding reliance on fragmented segmentation regions through holistic semantic modeling, thereby fundamentally eliminating over-segmentation issues.
	- mirror Fig4
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/Laura-Ting/blog-images/master/202501101629938.png){:height 210, :width 458}
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/Laura-Ting/blog-images/master/202501101629894.png){:height 264, :width 362}
	- air vent, caninet Fig 5
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/Laura-Ting/blog-images/master/202501101630103.png){:height 197, :width 386}
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/Laura-Ting/blog-images/master/202501101631540.png){:height 265, :width 384}
	- black sofa Fig1
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/Laura-Ting/blog-images/master/202501101631735.png){:height 240, :width 498}
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/Laura-Ting/blog-images/master/202501101632660.png){:height 341, :width 488}
	- wardobe Fig3
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/Laura-Ting/blog-images/master/202501101632003.png){:height 263, :width 566}
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/Laura-Ting/blog-images/master/202501101633795.png){:height 366, :width 548}
	- sofa Fig2
	  collapsed:: true
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/Laura-Ting/blog-images/master/202501101633778.png){:height 308, :width 585}
		- ![Replaced by Image Uploader](https://raw.githubusercontent.com/Laura-Ting/blog-images/master/202501101633195.png){:height 444, :width 557}
	- v2-en
	  collapsed:: true
		- In semantic navigation tasks, systems require accurate spatial perception to identify objects in the scene and their relative positions, enabling the generation of high-quality semantic maps. However, the ability to infer semantic attributes  of objects from a single view is inherently limited by occlusion and partial observation. To address this, multi-view observations have become a common solution. One popular paradigm involves using a class-agnostic segmentation model to divide each image frame into multiple regions, extracting semantic features of these segmented regions with a visual language foundation model, and subsequently fusing the inter-frame features to produce 3D semantic representations. However, this paradigm still faces challenges, as its effectiveness heavily depends on the granularity of the segmentation model and the inherent lack of contextual information within segmented regions.
		- First and foremost, this methodology lacks an effective mechanism to ==integrate local and global contextual information==. This limitation persists even in ideal cases where the segmentation model achieves perfect granularity, just enough to isolate an entire object. Some methods attempt to address this by ==weighting== local (extracted from mask-based pixels) and global (extracted from the entire image) features, but they fail to achieve [[#blue]]==context awareness==.  Especially when local predictions are incorrect, global feature weights tend to remain too low to correct false local predictions. For example, the mirror in Figure 4 might be misinterpreted as the door frame reflected within it, as this way of combining global information cannot integrate contextual information effectively. Furthermore, such misclassifications can propagate during the fusion of multi-frame 2D features into 3D, resulting in incorrect feature associations and degrading the semantic quality of the map.
		- Additionally, current segmentation models often suffer from ==significant over-segmentation==. Class-agnostic segmentation in the image domain is highly influenced by factors such as ==complex textures and lighting conditions,== which frequently cause a single object to be fragmented into multiple regions. This over-segmentation exacerbates the loss of contextual information, as these fragments are frequently misidentified as unrelated regions, undermining the semantic consistency of the object. For instance, as illustrated in Figure 1, the black sofa might be segmented into regions with features corresponding to a bag, jacket, blanket, and pillow. The abundance of erroneous segmented regions makes it challenging for the system to form an accurate understanding of the overall structure and position of objects when integrating multi-view information.
		- In contrast, our method leverages YOLO-World, which inherently provides contextual information while effectively addressing over-segmentation. YOLO-World ==integrates local and global information== within a unified framework through [[#blue]]==multi-scale feature extraction==. The low-resolution parts of the feature map (with a large receptive field) capture global contextual information, such as the general layout of the scene and the overall position of objects, while the high-resolution parts (with a small receptive field) preserve local details, such as edges and textures. Through the joint modeling of multi-scale features, local features are generated with global semantic constraints already in place, ensuring semantic consistency between local and global features within the same feature space. Additionally, YOLO-World directly generates target features from the entire image while avoiding reliance on fragmented segmentation regions through holistic semantic modeling, thereby fundamentally eliminating over-segmentation issues.
	- local-global问题：mirror，cabinet(air vent)
	- 过分割问题：sofa被分割成很多bag，sofa过分割成tissue，wardrobe被分割成很多blinds
	- 多视角不一致性
-
- GiftedNav当前文章结构
  collapsed:: true
	- Intro
		- focus在构建queryable地图上，但是问题是closed-set
		- open-vocabulary进展，dense-feature field-based(基于backprojection，differentiable rendering)和scene graph-based(在内存中保留所有点云)
		- 这些方法应用于离线构建，而不是在线。离线方法不能处理丢帧(还有什么)？。task-oriented。->提出open-vocabulary, online, and task-oriented map representation
		- key challenges：
			- perceptual inconsistency
			- Ensure memory efficiency
			- Achieve fast convergence
			- Require active exploration
-
-
-
-
- Architecture不同设定 -- 希望继续研究residual结构作用
  collapsed:: true
	- ![Replaced by Image Uploader](https://raw.githubusercontent.com/Laura-Ting/blog-images/master/202501140029020.png)
	- embed_full: 1 + 2
	- embed_pos: 2
	- geo_pos: 3 + 2
	- color_pos: 2 + 4
-
- Gaussian GPU并行处理
  collapsed:: true
	- ![55d50747662b8cf335aac73909eddb9.png](../assets/55d50747662b8cf335aac73909eddb9_1737097490255_0.png){:height 827, :width 633}
	- 希望通过images和poses给整个场景的高斯上语义特征
	- 1，images和poses分组，分组个数和第一组GPU数量N相同，多组可以实现并行，每一组内串行处理SAM分割mask，计算每个mask的CLIP embedding，每一组输入整个高斯场景用于做特定poses下的rasterization得到二维渲染图片，根据SAM分割结果可以知道是哪些区域被分割，也就是可以知道三维高斯场景的哪些区域被分割的parameter
	- 2，将上面步骤得到的不同GPU上的parameter信息和自己对应的embedding信息写入同一共享内存后重新分组，分组数量与第二组GPU数量M相同。只需要聚合这些高斯的parameter和它对应特征进行整合，归一化，可以得到整合后的segment-wise embedding。(相当于data association吧，只不过全程都是全局Gaussian parameter)
	- 3，可以知道这些parameter对应的高斯子区域，并赋给它对应的embedding
	- 4，这些高斯在NoSQL DB中按照空间位置坐标进行索引，就是使用三维空间中的坐标（如世界坐标）作为键，对数据进行排序，按区域存储数据，可以快速找到自己对应的语义向量所对应的分区
	-
	- 感觉比较有优势的点就是，
	- 1，可以并行来搞的原因就是不同区域的高斯是局部的，互不影响的，比如一个房间和另一个房间，一个房间左侧的物体和右侧的物体这种。
	- 2，最后存储embedding的粒度不是在每个高斯上，而是segment或者region(类似于node那种稀疏的)，efficient，或许还可调？
	- 3，一直是对一个全局Gaussian scene进行局部操作，而不是把局部通过复杂的关联整合起来，或者最近邻再去找。这样整合的时候可以处理边界的划分的问题。就是感觉用到了高斯这个渲染和可以逆着找回去的过程。
	- 4，如果按照物理顺序存储感觉可能也有优点，感觉和局部性原理有点关系。在一开始的images/poses subset划分这一块，如果不故意打乱顺序，就使用拍摄顺序的话，在第一步完成之后顺序写入第二步的storage bucket or shared memory如果也按顺序的话，实际上得到的就是这个局部的embedding，空间上邻近的点通常具有语义相关性（例如，属于同一个物体或区域），按顺序存储可以简化聚合操作。而且按顺序存储也可以减少GPU在处理共享存储时的随机访问开销。
	-
	- 如果物理位置临近的点存储在一起，后续的处理（如聚合、查询或三维建图）可以更高效地访问这些数据。
	- 在三维语义建图任务中，空间上邻近的点通常具有语义相关性（例如，属于同一个物体或区域），按顺序存储可以简化聚合操作。
	- 按顺序存储可以减少GPU在处理共享存储时的随机访问开销，提高数据访问的带宽利用率。
-
- 毕业设计：主动建图系统
-
-
- online Gaussian+Scene Graph静态场景，主要强调在线高保真开集语义建图
- Problem setting。切入角度：从二维环境的感知->三维环境->如何应用于task(task，sensing，mapping，planning，execution)
	- 对于智能系统来说，对整个场景的精准的环境感知能力是核心，支持其在复杂场景中实现更智能的决策与任务执行。
	- task: hierarchy open-vocabulary online high-fidelity panoptic scene reconstruction
	- 二维感知能力(sensing)：这首先需要具备强大的感知能力尤其是开集语义的理解，未见过物体。
	- 实时mapping到三维地图(mapping)：其次，需要实时高保真建模能力，可用于动态和交互场景。
	- 如何应用于task(指导planning, execution)：再者，层次化语义指导，解耦的物体表示，结构化地图。可通过文本查询(文本和视觉的结合)
- 应用：
	- 在线语义导航，找东西。
	- 仿真器：数字资产重建，解耦的物体表示。
	- 实时holistic 理解，场景查询，用户query这个场景的信息。
- 难点：
	- memory storage: 高维feature的存储问题。众多高斯对应一个语义，语义是高维特征，开集特征如何轻量化语义存储，因为在线有资源限制->scene graph(目前就是字典)
	- accuracy: 由遮挡和局部观测(感知不一致性)和分割模型的不稳定性引起的多视不一致性，导致错误特征(noise, outlier)会作为监督信号。
	- accuracy: 高斯一旦绑定到scene graph上就不可逆，高斯的feature可以做全局优化但是高斯所属scene graph id不能经过后续优化改进. 可优化scene graph。
	- data association。
- Paradigm
	- 显式表征，语义分布
- Methodology
- Implementation
	- 字典存储高维特征
	-
-
- **Gaussian Splatting：** 利用高斯分布对多视图观测进行概率融合，减少遮挡和部分观测带来的不确定性。
- **Scene Graph：** 通过图结构整合多视图语义信息，利用节点（物体）和边（关系）的一致性约束，纠正分割模型的误差。
- **语义推理：** 基于 Scene Graph 进行语义推理，补全缺失的语义信息（如推断被遮挡物体的类别）。
- 引入反馈机制优化关联结果。
-
- online Gaussian+Scene Graph 动态场景
	- Problem setting
		- 智能系统的核心是智能感知和交互
		- 视觉和文本。
		- 在当今世界，智能系统（如机器人、自动驾驶车辆、增强现实设备等）正在越来越多地融入我们的生活和工作环境。这些系统的核心能力是 **智能感知与交互**，即能够理解周围环境并与之进行有效的交互。然而，这种能力依赖于对环境的 **精准建模**，而环境本身是 **动态的、未知的**，这给智能感知与交互带来了巨大挑战。
		  collapsed:: true
			- **机器人导航：** 机器人需要在复杂的环境中自主移动，避开障碍物，并完成任务。
			- **自动驾驶：** 自动驾驶车辆需要理解交通场景，识别行人、车辆和交通标志，并做出安全决策。
			- **增强现实（AR）：** AR设备需要在真实场景中叠加虚拟内容，提供沉浸式体验。
			- **智能家居：** 智能家居系统需要理解家庭环境，自动调节灯光、温度等，提供个性化服务。
			- **工业自动化：** 工业机器人需要在工厂环境中执行搬运、装配等任务，提高生产效率。
		- 要解决的问题是如何精准建模动态和未知->在线实时构建一个开集的，可查询的层次化地图。
			- 通过层次化解耦的(适合动态场景，场景编辑，局部更新)，层次化语义指导。解耦的物体表示，结构化地图->scene graph
			- 高保真可编辑精准建模->高斯：实时，高保真，离散表征。结合scene graph
			- 可查询的开集语义地图构建来解决：开集能够应对之前没见到过的物体
		- 目前的问题：
		- 提供逼真的三维模型，支持智能系统的视觉感知与交互。支持动态环境中的快速更新和显示。
		-
		- 动态建模能力是智能系统在实际场景中稳定运行的基础
	- Paradigm
	- Methodology
	- Implementation
-
- Stage1：
	- 达到和OpenFusion同样的性能：实时，精度。不进行步骤简化。
- Stage2：
	- 更低的计算复杂度
	- 解耦表示，物体一致性约束
-
- 要做的事情：
	- 用颜色去判断是否有高斯，获得render segment mask，改成按照opacity直接渲染高斯id
	- 把每一帧的res拿到关键帧里面
	- 能否两帧直接匹配，降低计算复杂度
	- confidence map干什么用
	- 高斯语义优化
	- 接Habitat数据集，用大分辨率图像实验segmentation效果
	- OpenFusion性能统计
	-
-
- 问题：一个高斯可能代表多个object，所以需要更明确的边界，而且每个object都有自己的表面->需要基于现成codebase
- 总共有T时刻，即总共T帧，每一帧有M个mask
- 二维是否需要具体到每一个像素：如果使用存储id，而且feature本身是local-global的，那么就不需要每个像素上的feature。如果使用
- scene graph优化
  collapsed:: true
	- 每个高斯
- Projection路线:
  collapsed:: true
	- 每个高斯上存id
		- 记录帧ID，maskID，就能通过这两个信息找到feature。比如说一个数据库存储。
		- 高斯表征：xyz，rgb，o，r，帧id+maskid
		- 因为是sequential的，有深度的就代表是之前已经被splat上去的。先不根据opacity，而是直接t时刻所有的点去t-1时刻地图中找最近邻，如果找到了最近邻切。。
		- 优势：特征不是存储在高斯上，节省显存
		- 缺点：无法直接优化特征，特征的语义信息无法作用于高斯属性的调整(比如透明度，颜色)
	- 每个高斯上存特征
		- 高维特征
		- 低维特征：PCA或自编码器
-
- Render路线：
  collapsed:: true
	- 比如说现在是时刻t
	- 存储ID方式：
		- 数据库存储两个dict：
			- 1，2D语义信息用于优化过程指导。key：帧ID+maskID，value：高维feature(比如说通过SEEM得到)
			- 2，高斯语义信息。key：semantic gaussian ID，value：高维feature(每新来一个时刻都进行更新)
			- confidence
		- 高斯表征：xyz，rgb，o，r，帧id+maskid，semantic gaussian ID
		- 已有数据：
			- t-1时刻高斯地图
			- t时刻gt rgb，depth，pseudo semantic(以mask为划分单位)
			- t-1时刻地图的高斯按照时刻t下的pose做渲染得到render rgb, render depth，render opacity。
			- 根据rasterization信息找到渲染的每个pixel对应的多个高斯，获取每个高斯的帧ID和maskID，得到当前2D这个pixel下对应的多个高维特征，进行特征加权平均(根据距离和opacity)，得到估计的t时刻下的语义图。
			- 这个语义和每个高斯对应存到数据库中，可以通过每个高斯有一个semantic gaussian ID找到，这个feature是每个时刻都进行更新的。和t时刻pseudo semantic求loss。如何进行feature的优化？这样和直接是否真正减少了存储高斯的显存（动态加载？）。
			- 根据opacity将新高斯添加到场景中。
			- 根据pseudo semantic的mask内深度和opacity的高斯进行聚类，再经过几何检查和语义检查提取scene graph节点。
	- 存储特征方式：
		- 每个高斯：id(需要维护全局id)，xyz，rgb，o，r，feature(512)
		- scene graph每个节点：id，gaussian id list，feature(512)，质心xyz 。
			- 对于高置信度的节点（几何和语义高度一致的节点），可以即时更新。
			- 对于低置信度的节点，采用延迟更新的策略，通过多帧观测来积累信息。
			- **活跃节点**：当前滑动窗口内的节点，优先更新。
			- **延迟节点**：滑动窗口外的节点，等待剪枝或确认。
			- **稳定节点**：已确认的节点，不再频繁更新，除非新的观测明显与其匹配。
			- 能不能不即时更新，构造一个树结构，之后再剪枝，就是说当发现这个节点经过多长时间之后不再有变化？但是如果这时候序列又再次经过了这个地方怎么办？
			- 而且有一篇论文说如何能够利用上过去的信息？
		- 已有数据：
			- 第一帧全部高斯用来初始化，2D有多少个mask的话scene graph就有多少个节点(假设mask不重叠)，
			- t-1时刻Gaussian Map，t时刻gt rgb，depth，pseudo semantic，t-1时刻scene graph
			- t-1时刻的map在t时刻的pose下渲染得到render rgb，render depth，render opacity，render semantic
			- 利用t时刻的gt和t时刻render的rgb，depth，semantic分别求loss梯度回传给高斯参数进行优化(semantic loss的设计需要注意)
			- t时刻rgb和depth投影得到的三维点去全局gaussian map中找几何的重叠，如果几何重叠大于一定阈值的就检查语义相关性，语义相关性大于一定阈值的就把这些高斯和赋予和之前地图里的高斯一样的scene graph的id。如果匹配上现有节点，则更新节点信息，更新质心，gaussian id list，feature
			- opt
				- 如果几何上有重叠但语义上没有相关性？希望通过多帧观测来检查一致性？还是说我合并的时候给一个权重。但是如果直接和进去就是混入了错误的特征。
				- 如果几何上没有重叠但是语义上有相关性？这种有可能出现嘛？局部观测和遮挡。
					- 三维也有可能出现局部观测，受有限视角影响。
					- 利用场景上下文信息来辅助判断。例如，如果一个语义标签在场景中出现的概率很高，那么即使几何不重叠，也可以考虑其潜在的相关性。
				- 这两种情况考虑使用多帧验证：使用滑动窗口进行不一致性检测，加权合并/投票机制，上下文验证
			- 如果几何上没有重叠且语义上没有相关性，则初始化新的scene graph node
			- 但是spalt的时候还是根据opacity去给没有深度的地方上高斯。
			-
			- key frame如何利用？
			- 几何和语义的动态权重？环境复杂度，历史数据，不确定性估计
			- 能不能不基于阈值的方法，能不能学习高斯场在空间上的几何分布？或许根据平面？
			- 节点的合并和分裂？
				- 合并：如果新帧中的某些mask与多个现有节点有较高的几何和语义重叠，可以考虑将这些节点合并为一个节点，以简化场景图。
				- 分裂：如果一个现有节点在新帧中被多个mask分开覆盖，且这些mask之间的几何和语义差异较大，则可以考虑将该节点分裂为多个节点，分别表示不同的对象或区域。
			-
			- 同时，t时刻render的semantic和t时刻pseudo semantic做匹配，t-1时刻的都在scene graph中了
	-
	- 只要有semantic id的高斯就不需要再有帧id和mask id，可以实现feature的更新。但是feature的优化？
	-
	- 还是感觉如果用rendering特性的话没有丢失了深度信息，就是说这个特征匹配能准嘛？按照几何还是语义？
	-
	- 高斯表征：xyz，rgb，o，r，feature
	- t-1 时刻地图的高斯按照时刻 t 下的 pose 做渲染彩色图和深度图和语义图。
	-
	-
	-
	- render的color和render的depth是否能够利用上。不对，直接用gt color和gt rgb。不对，render才能真正表示地图里有什么。
	- 这个语义图是什么样的格式？按照语义颜色？按照语义id？
	- semantic
-
-
- 找论文具体的分类：codebase，baseline，轮子。
- project-associate-assign
	- 需要解决的问题和解决方法需要明确定义，就目前3个解决方案，和当前的领域内的方法去比较。
		- memory storage: 高维feature的存储问题
			- solution: 采用字典形式，键值对查找解决
		- accuracy: 由遮挡和局部观测和分割模型的不稳定性引起的多视不一致性，导致错误特征(noise, outlier)会作为监督信号
			- solution: 添加滑动窗口，进行多视一致性的check
		- accuracy: 高斯一旦绑定到scene graph上就不可逆，高斯的feature可以做全局优化但是高斯所属scene graph id不能经过后续优化改进
			- solution: subgraph延迟更新
		- accuracy: 动态阈值权重。几何是由于局部观测引起的重叠部分过小(相机移动距离过大时)，而语义有可能是一致的错误，而且背景会特征突变很多。
	- memory storage角度，semantic feature
	- 1，采用scene graph和字典键值对查找解决高维feature存储问题
	  collapsed:: true
		- Motivation(memory storage): 高斯的语义特征优化需要在每个高斯上存储高维feature需要大量显存，如何解决高维feature存储问题
		- Paradigm: Dictionary
		- Methodology: 高斯只存对应帧ID和maskID，使用字典形式键值对进行feature的查找和更新(参考OpenFusion)。高斯和scene graph的双向关联，因为高斯特征需要做更新。
	- 2，采用滑动窗口检查多视不一致性问题
	  collapsed:: true
		- Motivation(accuracy): 遮挡和局部观测和分割模型的不稳定性导致多视不一致性，如何解决错误特征(noise, outlier)作为监督信号问题。(由perception和segmentation引起的多视角instance-level semantic feature不一致)
		- Paradigm: 滑动窗口
		- Methodology: 添加滑动窗口进行多视一致性的check，用efficiency和storage换取accuracy。（自适应步长的策略，根据局部特征的密度动态调整）这个check可以更新对应索引关系？
	- 3，scene graph延迟更新解决错误关联不可逆问题
	  collapsed:: true
		- Motivation(accuracy): 给每个高斯即时分配的scene graph node id一经分配就无法修改，导致错误关联
		- Paradigm: Wait and Update
		- Methodology: 划分subgraph，对scene graph node和高斯的所属关系延迟更新。（or按照优先级更新，置信度高的可以即时更新，置信度低的先保留一段时间？）
	- 4，动态权重&阈值解决data association中阈值不适用情况
	  collapsed:: true
		- Motivation(accuracy): association对硬性阈值敏感
		- Paradigm: adaptive thresholding and weighting
		- Methodology: 分别针对导致几何和语义出现threshold不work的原因进行相应动态阈值调整，比如当相机位移过大时几何可能由于部分观测引起重叠部分过小，此时需要增加语义的weight和降低几何threshold
		- 反馈机制？
-
- 每个高斯先自己检测，自己vote出一个id(通过观测次数和观测置信度共同决定)
- 然后当前语义一致的，在附近相同区域的所有高斯再合并起来check
-
- render-match-update
- 单帧的mask进行匹配
-
-
-
- weighting？？？
- 近年来，智能系统在复杂环境中导航和理解的需求迅速增长。从自动驾驶汽车到机器人助手，准确构建和维护周围世界的表示对于有效的决策和交互至关重要。传统的地图构建方法通常依赖于封闭集环境，其中所有对象和特征都是预先已知的。然而，现实世界场景本质上是动态和不可预测的，因此需要开发能够适应新出现的、未见过的对象和环境的开集地图构建系统。
-
- 高斯开集语义SLAM和Scene Graph结合的问题
- 动态场景
- OpenFusion性能评测
-
-
- 高斯能达到实时高质量渲染的效果，本身是显式离散的表征，适合局部更新和编辑，但是却能进行连续的稠密预测，可微优化。梯度直接反传到高斯身上，直接进行参数优化。能够同时优化颜色，深度，语义(通过添加一个属性)。
-
- 但是构建这样一个系统面临challenge：
- 首先是memory storage上，我们希望能够有开集语义的能力，那么就需要使用高维特征，我们还希望使用高斯优化的特性对语义进行优化
-
- 高效存储和检索。
- 高维feature有什么好处？
-
- 发issue
- 对硬性阈值敏感
-
- SEEM
  collapsed:: true
	- ad
		- 相比于SAM+CLIP或者YOLOWorld+FastSAM，语义一致性更强一些也感觉更准确一些
		- 不用自己依靠2-stage分割模型
	- dis
		- 边缘模糊
		- 有些前景被分为背景类(由背景类的添加和卡阈值共同导致)
		- 存在欠分割问题（这两点导致有些物体检不出来）
	- potential
		- 也卡了阈值0.4，所以我们此处也可以weighting
		- 可以加task aware，替换COCO
		- 是否可以考虑语义和颜色联合优化？
		- 边缘模糊能否用(节点内一致性，节点间区分性进行约束)
-
- 高斯能不能动起来，不受splatam限制，恢复ADC，但是此时语义怎么处理？
- 感觉关键帧也比较重要，决定用何种信息优化
-
- 一种方法是存id
- 一种方法是解码‘
-
- 三维匹配：
- 二维匹配：
- render好处，利用高斯可微渲染特性
	- 特征优化
	- 就先按照点云来说，3D通常基于几何匹配(较少使用语义，因为语义可能由于多视不一致而不准)，在相机位移过大的情况下，如果上一帧看到了物体的左边，下一帧看到了物体的右边，几何匹配没有达到阈值导致两边没有合并。但是如果是render方法的话，我就固定视角，我从这个视角一定能看到这个东西是这种形态，相当于几何是准的，再加上我一直利用语义去match，就保证语义一致性？